{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "plXAwgRZt-bB"
   },
   "source": [
    "'''Sequence to sequence example in Keras (character-level).\n",
    "\n",
    "This script demonstrates how to implement a basic character-level\n",
    "sequence-to-sequence model. We apply it to translating\n",
    "short English sentences into short French sentences,\n",
    "character-by-character. Note that it is fairly unusual to\n",
    "do character-level machine translation, as word-level\n",
    "models are more common in this domain.\n",
    "\n",
    "# Summary of the algorithm:\n",
    "\n",
    "- We start with input sequences from a domain (e.g. English sentences)\n",
    "    and correspding target sequences from another domain\n",
    "    (e.g. French sentences).\n",
    "- An encoder LSTM turns input sequences to 2 state vectors\n",
    "    (we keep the last LSTM state and discard the outputs).\n",
    "- A decoder LSTM is trained to turn the target sequences into\n",
    "    the same sequence but offset by one timestep in the future,\n",
    "    a training process called \"teacher forcing\" in this context.\n",
    "    Is uses as initial state the state vectors from the encoder.\n",
    "    Effectively, the decoder learns to generate `targets[t+1...]`\n",
    "    given `targets[...t]`, conditioned on the input sequence.\n",
    "- In inference mode, when we want to decode unknown input sequences, we:\n",
    "    - Encode the input sequence into state vectors\n",
    "    - Start with a target sequence of size 1\n",
    "        (just the start-of-sequence character)\n",
    "    - Feed the state vectors and 1-char target sequence\n",
    "        to the decoder to produce predictions for the next character\n",
    "    - Sample the next character using these predictions\n",
    "        (we simply use argmax).\n",
    "    - Append the sampled character to the target sequence\n",
    "    - Repeat until we generate the end-of-sequence character or we\n",
    "        hit the character limit.\n",
    "\n",
    "# Data download:\n",
    "\n",
    "English to French sentence pairs.\n",
    "https://www.manythings.org/anki/fra-eng.zip\n",
    "\n",
    "Lots of neat sentence pairs datasets can be found at:\n",
    "https://www.manythings.org/anki/\n",
    "\n",
    "# References:\n",
    "\n",
    "- Sequence to Sequence Learning with Neural Networks\n",
    "    https://arxiv.org/abs/1409.3215\n",
    "- Learning Phrase Representations using\n",
    "    RNN Encoder-Decoder for Statistical Machine Translation\n",
    "    https://arxiv.org/abs/1406.1078"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hb0LAYiot-bN"
   },
   "source": [
    "# Problem 1 Machine translation execution and code reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BWebvAmCt-bO"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Embedding\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KiLvYnZMt-bS"
   },
   "source": [
    "- line 1-4: Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qs1GCCSyt-bS"
   },
   "outputs": [],
   "source": [
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 100  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "num_samples = 10000  # Number of samples to train on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jz_Xz_j1t-bU"
   },
   "source": [
    "- Line 1-4: hyperparameter settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YUtRnwTfIRy4",
    "outputId": "b01de1b5-37e7-4184-a66f-f97f42739540"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 10000\n",
      "Number of unique input tokens: 70\n",
      "Number of unique output tokens: 90\n",
      "Max sequence length for inputs: 16\n",
      "Max sequence length for outputs: 53\n"
     ]
    }
   ],
   "source": [
    "data_path = 'fra2.txt'\n",
    "lines = open(data_path, encoding='utf8').read().split('\\n')\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "  input_text, target_text = line.split('\\t')\n",
    "  target_text = '\\t' + target_text + '\\n'\n",
    "  input_texts.append(input_text)\n",
    "  target_texts.append(target_text)\n",
    "  for char in input_text:\n",
    "    if char not in input_characters:\n",
    "      input_characters.add(char)\n",
    "  for char in target_text:\n",
    "    if char not in target_characters:\n",
    "      target_characters.add(char)\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "frAacreXt-bW"
   },
   "source": [
    "- Line 1: get the data path\n",
    "- Line 2: opening file\n",
    "- Line 3 to 6: define variable input and target test and characters\n",
    "- Line 8: loop along the length of the file string\n",
    "- Line 9: Divide input text and target text\n",
    "- Line 10 to 11: save to list\n",
    "- Line 12 to 17: loop along the lenght of input text and add in input,target characters\n",
    "- Line 18 to 19: sorted the characters\n",
    "- Line 20-21: get lenght characters\n",
    "- Line 22: get lenght max sequence\n",
    "- Line 23: return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CMcvP5DRt-by"
   },
   "outputs": [],
   "source": [
    "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AIl8UGBEt-bz"
   },
   "source": [
    "- Line 1 to 3: assign an index to each charectar. But the characters do not match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7jA1Fm8tt-b0"
   },
   "outputs": [],
   "source": [
    "encoder_input_data = np.zeros((len(input_texts), max_encoder_seq_length, num_encoder_tokens),dtype='float32')\n",
    "decoder_input_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens),dtype='float32')\n",
    "decoder_target_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens),dtype='float32')\n",
    "# print(\"endoder input data:\\n{}\".format(encoder_input_data[:][1]))\n",
    "# print(\"decoder input data:\\n{}\".format(decoder_input_data[:][1]))\n",
    "# print(\"decoder target data:\\n{}\".format(decoder_target_data[:][1]))\n",
    "#Zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCWUD0uRt-b1"
   },
   "source": [
    "- line 1 to 3: definition encoder data and decoder data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kRFwjIHmt-b1",
    "outputId": "1f47a1ef-0c82-43b1-9704-e9dcf597dba6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder target data shape: (10000, 53, 90)\n",
      "encoder input data shape: (10000, 16, 70)\n"
     ]
    }
   ],
   "source": [
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
    "\n",
    "\n",
    "print(f\"decoder target data shape: {decoder_target_data.shape}\")\n",
    "print(f\"encoder input data shape: {encoder_input_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_w3gHRo4t-b2"
   },
   "source": [
    "- Line 1 to 8: give value in encoder input data and decoder target data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f7V5itcLt-b3"
   },
   "outputs": [],
   "source": [
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vC48NRhZt-b4"
   },
   "source": [
    "- Line 1 to 4: Define an input sequence and process it.\n",
    "- Line 5 to 6: discard `encoder_outputs` and only keep the states.\n",
    "- Line 7 to 8: Set up the decoder, using `encoder_states` as initial state.\n",
    "- Line 9 to 15: We set up our decoder to return full output sequences, and to return internal states as well. We don't use the return states in the training model, but we will use them in inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O9sph4Qmt-b5",
    "outputId": "f6937e9c-3373-45c5-c913-2101e09d136d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "125/125 [==============================] - 76s 582ms/step - loss: 1.2518 - val_loss: 1.3571\n",
      "Epoch 2/100\n",
      "125/125 [==============================] - 78s 623ms/step - loss: 1.1648 - val_loss: 1.3252\n",
      "Epoch 3/100\n",
      "125/125 [==============================] - 76s 607ms/step - loss: 1.1430 - val_loss: 1.3220\n",
      "Epoch 4/100\n",
      "125/125 [==============================] - 83s 664ms/step - loss: 1.1232 - val_loss: 1.3059\n",
      "Epoch 5/100\n",
      "125/125 [==============================] - 79s 634ms/step - loss: 1.1051 - val_loss: 1.2682\n",
      "Epoch 6/100\n",
      "125/125 [==============================] - 80s 640ms/step - loss: 1.0850 - val_loss: 1.2648\n",
      "Epoch 7/100\n",
      "125/125 [==============================] - 82s 656ms/step - loss: 1.0671 - val_loss: 1.2481\n",
      "Epoch 8/100\n",
      "125/125 [==============================] - 82s 662ms/step - loss: 1.0504 - val_loss: 1.2229\n",
      "Epoch 9/100\n",
      "125/125 [==============================] - 84s 675ms/step - loss: 1.0352 - val_loss: 1.2063\n",
      "Epoch 10/100\n",
      "125/125 [==============================] - 80s 639ms/step - loss: 1.0164 - val_loss: 1.1729\n",
      "Epoch 11/100\n",
      "125/125 [==============================] - 79s 633ms/step - loss: 1.0001 - val_loss: 1.1860\n",
      "Epoch 12/100\n",
      "125/125 [==============================] - 75s 601ms/step - loss: 0.9865 - val_loss: 1.1619\n",
      "Epoch 13/100\n",
      "125/125 [==============================] - 74s 594ms/step - loss: 0.9731 - val_loss: 1.1354\n",
      "Epoch 14/100\n",
      "125/125 [==============================] - 77s 616ms/step - loss: 0.9611 - val_loss: 1.1278\n",
      "Epoch 15/100\n",
      "125/125 [==============================] - 74s 596ms/step - loss: 0.9499 - val_loss: 1.1213\n",
      "Epoch 16/100\n",
      "125/125 [==============================] - 72s 580ms/step - loss: 0.9397 - val_loss: 1.0788\n",
      "Epoch 17/100\n",
      "125/125 [==============================] - 75s 601ms/step - loss: 0.9304 - val_loss: 1.0921\n",
      "Epoch 18/100\n",
      "125/125 [==============================] - 80s 646ms/step - loss: 0.9284 - val_loss: 1.0854\n",
      "Epoch 19/100\n",
      "125/125 [==============================] - 77s 618ms/step - loss: 0.9151 - val_loss: 1.1199\n",
      "Epoch 20/100\n",
      "125/125 [==============================] - 76s 610ms/step - loss: 0.9095 - val_loss: 1.0551\n",
      "Epoch 21/100\n",
      "125/125 [==============================] - 74s 594ms/step - loss: 0.9013 - val_loss: 1.0815\n",
      "Epoch 22/100\n",
      "125/125 [==============================] - 79s 634ms/step - loss: 0.8947 - val_loss: 1.0729\n",
      "Epoch 23/100\n",
      "125/125 [==============================] - 76s 607ms/step - loss: 0.8864 - val_loss: 1.0591\n",
      "Epoch 24/100\n",
      "125/125 [==============================] - 77s 619ms/step - loss: 0.8815 - val_loss: 1.0688\n",
      "Epoch 25/100\n",
      "125/125 [==============================] - 74s 596ms/step - loss: 0.8756 - val_loss: 1.0382\n",
      "Epoch 26/100\n",
      "125/125 [==============================] - 80s 641ms/step - loss: 0.8687 - val_loss: 1.0185\n",
      "Epoch 27/100\n",
      "125/125 [==============================] - 77s 621ms/step - loss: 0.8640 - val_loss: 1.0173\n",
      "Epoch 28/100\n",
      "125/125 [==============================] - 75s 604ms/step - loss: 0.8591 - val_loss: 1.0573\n",
      "Epoch 29/100\n",
      "125/125 [==============================] - 75s 602ms/step - loss: 0.8528 - val_loss: 1.0328\n",
      "Epoch 30/100\n",
      "125/125 [==============================] - 75s 602ms/step - loss: 0.8496 - val_loss: 1.0361\n",
      "Epoch 31/100\n",
      "125/125 [==============================] - 77s 618ms/step - loss: 0.8437 - val_loss: 1.0047\n",
      "Epoch 32/100\n",
      "125/125 [==============================] - 76s 612ms/step - loss: 0.8396 - val_loss: 1.0251\n",
      "Epoch 33/100\n",
      "125/125 [==============================] - 79s 631ms/step - loss: 0.8344 - val_loss: 1.0215\n",
      "Epoch 34/100\n",
      "125/125 [==============================] - 77s 618ms/step - loss: 0.8302 - val_loss: 0.9978\n",
      "Epoch 35/100\n",
      "125/125 [==============================] - 73s 586ms/step - loss: 0.8261 - val_loss: 0.9649\n",
      "Epoch 36/100\n",
      "125/125 [==============================] - 77s 620ms/step - loss: 0.8218 - val_loss: 0.9869\n",
      "Epoch 37/100\n",
      "125/125 [==============================] - 73s 586ms/step - loss: 0.8183 - val_loss: 0.9845\n",
      "Epoch 38/100\n",
      "125/125 [==============================] - 77s 616ms/step - loss: 0.8143 - val_loss: 0.9970\n",
      "Epoch 39/100\n",
      "125/125 [==============================] - 81s 648ms/step - loss: 0.8118 - val_loss: 0.9622\n",
      "Epoch 40/100\n",
      "125/125 [==============================] - 76s 612ms/step - loss: 0.8077 - val_loss: 0.9619\n",
      "Epoch 41/100\n",
      "125/125 [==============================] - 75s 599ms/step - loss: 0.8044 - val_loss: 0.9572\n",
      "Epoch 42/100\n",
      "125/125 [==============================] - 77s 620ms/step - loss: 0.8010 - val_loss: 0.9448\n",
      "Epoch 43/100\n",
      "125/125 [==============================] - 73s 587ms/step - loss: 0.7965 - val_loss: 0.9780\n",
      "Epoch 44/100\n",
      "125/125 [==============================] - 82s 655ms/step - loss: 0.7934 - val_loss: 0.9861\n",
      "Epoch 45/100\n",
      "125/125 [==============================] - 76s 607ms/step - loss: 0.7897 - val_loss: 0.9333\n",
      "Epoch 46/100\n",
      "125/125 [==============================] - 72s 580ms/step - loss: 0.7863 - val_loss: 0.9872\n",
      "Epoch 47/100\n",
      "125/125 [==============================] - 73s 589ms/step - loss: 0.7844 - val_loss: 0.9566\n",
      "Epoch 48/100\n",
      "125/125 [==============================] - 77s 618ms/step - loss: 0.7809 - val_loss: 0.9432\n",
      "Epoch 49/100\n",
      "125/125 [==============================] - 78s 627ms/step - loss: 0.7788 - val_loss: 0.9483\n",
      "Epoch 50/100\n",
      "125/125 [==============================] - 77s 620ms/step - loss: 0.7763 - val_loss: 0.9350\n",
      "Epoch 51/100\n",
      "125/125 [==============================] - 76s 612ms/step - loss: 0.7732 - val_loss: 0.9372\n",
      "Epoch 52/100\n",
      "125/125 [==============================] - 79s 634ms/step - loss: 0.7705 - val_loss: 0.9389\n",
      "Epoch 53/100\n",
      "125/125 [==============================] - 76s 614ms/step - loss: 0.7697 - val_loss: 0.9111\n",
      "Epoch 54/100\n",
      "125/125 [==============================] - 78s 623ms/step - loss: 0.7661 - val_loss: 0.9340\n",
      "Epoch 55/100\n",
      "125/125 [==============================] - 77s 616ms/step - loss: 0.7628 - val_loss: 0.9830\n",
      "Epoch 56/100\n",
      "125/125 [==============================] - 80s 640ms/step - loss: 0.7627 - val_loss: 0.9167\n",
      "Epoch 57/100\n",
      "125/125 [==============================] - 73s 590ms/step - loss: 0.7585 - val_loss: 0.9344\n",
      "Epoch 58/100\n",
      "125/125 [==============================] - 74s 592ms/step - loss: 0.7564 - val_loss: 0.9353\n",
      "Epoch 59/100\n",
      "125/125 [==============================] - 74s 592ms/step - loss: 0.7545 - val_loss: 0.8943\n",
      "Epoch 60/100\n",
      "125/125 [==============================] - 74s 593ms/step - loss: 0.7508 - val_loss: 0.9440\n",
      "Epoch 61/100\n",
      "125/125 [==============================] - 73s 583ms/step - loss: 0.7492 - val_loss: 0.9088\n",
      "Epoch 62/100\n",
      "125/125 [==============================] - 76s 610ms/step - loss: 0.7467 - val_loss: 0.9188\n",
      "Epoch 63/100\n",
      "125/125 [==============================] - 77s 621ms/step - loss: 0.7452 - val_loss: 0.9394\n",
      "Epoch 64/100\n",
      "125/125 [==============================] - 73s 587ms/step - loss: 0.7418 - val_loss: 0.8919\n",
      "Epoch 65/100\n",
      "125/125 [==============================] - 71s 568ms/step - loss: 0.7408 - val_loss: 0.9014\n",
      "Epoch 66/100\n",
      "125/125 [==============================] - 78s 628ms/step - loss: 0.7393 - val_loss: 0.8966\n",
      "Epoch 67/100\n",
      "125/125 [==============================] - 76s 608ms/step - loss: 0.7366 - val_loss: 0.9105\n",
      "Epoch 68/100\n",
      "125/125 [==============================] - 74s 598ms/step - loss: 0.7349 - val_loss: 0.9289\n",
      "Epoch 69/100\n",
      "125/125 [==============================] - 76s 613ms/step - loss: 0.7335 - val_loss: 0.8797\n",
      "Epoch 70/100\n",
      "125/125 [==============================] - 77s 617ms/step - loss: 0.7326 - val_loss: 0.8839\n",
      "Epoch 71/100\n",
      "125/125 [==============================] - 79s 632ms/step - loss: 0.7286 - val_loss: 0.9057\n",
      "Epoch 72/100\n",
      "125/125 [==============================] - 75s 602ms/step - loss: 0.7274 - val_loss: 0.9015\n",
      "Epoch 73/100\n",
      "125/125 [==============================] - 79s 631ms/step - loss: 0.7258 - val_loss: 0.9038\n",
      "Epoch 74/100\n",
      "125/125 [==============================] - 77s 618ms/step - loss: 0.7233 - val_loss: 0.8983\n",
      "Epoch 75/100\n",
      "125/125 [==============================] - 78s 625ms/step - loss: 0.7234 - val_loss: 0.8988\n",
      "Epoch 76/100\n",
      "125/125 [==============================] - 76s 608ms/step - loss: 0.7205 - val_loss: 0.8928\n",
      "Epoch 77/100\n",
      "125/125 [==============================] - 78s 626ms/step - loss: 0.7190 - val_loss: 0.8706\n",
      "Epoch 78/100\n",
      "125/125 [==============================] - 82s 649ms/step - loss: 0.7180 - val_loss: 0.8726\n",
      "Epoch 79/100\n",
      "125/125 [==============================] - 79s 635ms/step - loss: 0.7152 - val_loss: 0.8682\n",
      "Epoch 80/100\n",
      "125/125 [==============================] - 79s 638ms/step - loss: 0.7137 - val_loss: 0.8725\n",
      "Epoch 81/100\n",
      "125/125 [==============================] - 80s 645ms/step - loss: 0.7127 - val_loss: 0.8778\n",
      "Epoch 82/100\n",
      "125/125 [==============================] - 79s 635ms/step - loss: 0.7102 - val_loss: 0.8796\n",
      "Epoch 83/100\n",
      "125/125 [==============================] - 78s 623ms/step - loss: 0.7095 - val_loss: 0.9306\n",
      "Epoch 84/100\n",
      "125/125 [==============================] - 79s 632ms/step - loss: 0.7112 - val_loss: 0.8765\n",
      "Epoch 85/100\n",
      "125/125 [==============================] - 76s 612ms/step - loss: 0.7065 - val_loss: 0.8729\n",
      "Epoch 86/100\n",
      "125/125 [==============================] - 76s 608ms/step - loss: 0.7059 - val_loss: 0.8654\n",
      "Epoch 87/100\n",
      "125/125 [==============================] - 75s 604ms/step - loss: 0.7039 - val_loss: 0.8720\n",
      "Epoch 88/100\n",
      "125/125 [==============================] - 77s 620ms/step - loss: 0.7031 - val_loss: 0.8934\n",
      "Epoch 89/100\n",
      "125/125 [==============================] - 71s 566ms/step - loss: 0.7009 - val_loss: 0.8752\n",
      "Epoch 90/100\n",
      "125/125 [==============================] - 79s 634ms/step - loss: 0.6981 - val_loss: 0.8572\n",
      "Epoch 91/100\n",
      "125/125 [==============================] - 76s 612ms/step - loss: 0.6974 - val_loss: 0.8600\n",
      "Epoch 92/100\n",
      "125/125 [==============================] - 77s 621ms/step - loss: 0.6955 - val_loss: 0.8664\n",
      "Epoch 93/100\n",
      "125/125 [==============================] - 76s 610ms/step - loss: 0.6950 - val_loss: 0.8641\n",
      "Epoch 94/100\n",
      "125/125 [==============================] - 77s 618ms/step - loss: 0.6932 - val_loss: 0.8923\n",
      "Epoch 95/100\n",
      "125/125 [==============================] - 79s 633ms/step - loss: 0.6918 - val_loss: 0.8723\n",
      "Epoch 96/100\n",
      "125/125 [==============================] - 73s 590ms/step - loss: 0.6910 - val_loss: 0.8719\n",
      "Epoch 97/100\n",
      "125/125 [==============================] - 75s 599ms/step - loss: 0.6896 - val_loss: 0.8444\n",
      "Epoch 98/100\n",
      "125/125 [==============================] - 76s 612ms/step - loss: 0.6875 - val_loss: 0.8626\n",
      "Epoch 99/100\n",
      "125/125 [==============================] - 74s 594ms/step - loss: 0.6868 - val_loss: 0.8687\n",
      "Epoch 100/100\n",
      "125/125 [==============================] - 75s 606ms/step - loss: 0.6871 - val_loss: 0.8567\n"
     ]
    }
   ],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "history = model.fit([encoder_input_data, decoder_input_data], decoder_target_data,batch_size=batch_size,epochs=epochs,validation_split=0.2)\n",
    "model.save('s2s.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "65qqJVyyt-b_"
   },
   "source": [
    "- line 1: Define the model that will turn `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "- line 2 to 3: Run training\n",
    "- line 4: Sava model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PtLY7KJcEvAI"
   },
   "outputs": [],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.load_weights('./s2s.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LlTq-0IHt-b_"
   },
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model( [decoder_inputs] + decoder_states_inputs,[decoder_outputs] + decoder_states)\n",
    "\n",
    "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T8fJI1P8t-cA"
   },
   "source": [
    "- line 1 to 8: inference mode (sampling).Here's the drill:\n",
    "1) encode input and retrieve initial decoder state\n",
    "2) run one step of decoder with this initial state and a \"start of sequence\" token as target.Output will be the next target token\n",
    "3) Repeat with the current target token and current states\n",
    "- Line 10 to 11:  Reverse-lookup token index to decode sequences back to something readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vd60LR0_t-cA"
   },
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RqwpagEwt-cB"
   },
   "source": [
    "- Line 1: Define sampling models\n",
    "- Line 2 to 3: Encode the input as state vectors.\n",
    "- Line 4 to 5: Generate empty target sequence of length 1.\n",
    "- Line 6 to 7: Populate the first character of target sequence with the start character.\n",
    "- Line 8 to 14: Sampling loop for a batch of sequences (to simplify, here we assume a batch of size 1).\n",
    "- Line 15 to 18: Sample a token\n",
    "- Line 19 to 24: Exit condition: either hit max length or find stop character.\n",
    "- Line 25 to 27: Update the target sequence (of length 1).\n",
    "- Line 28 to 29: Update states\n",
    "- Line 30: return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "071PxQC2GOYU",
    "outputId": "dc7af44d-654d-4ade-e511-7039fee5876c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How are you?\n",
      "1/1 [==============================] - 0s 368ms/step\n",
      "1/1 [==============================] - 0s 376ms/step\n",
      "1/1 [==============================] - 1s 928ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 1s 757ms/step\n",
      "1/1 [==============================] - 1s 883ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 1s 803ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 192ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 1s 869ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 1s 809ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 228ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 1s 849ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 1s 803ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 248ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Wie geht es geheneeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee\n"
     ]
    }
   ],
   "source": [
    "input_sentence = \"How are you?\"\n",
    "test_sentence_tokenized = np.zeros((1, max_encoder_seq_length, num_encoder_tokens), dtype='float32')\n",
    "for t, char in enumerate(input_sentence):\n",
    "  test_sentence_tokenized[0, t, input_token_index[char]] = 1.\n",
    "print(input_sentence)\n",
    "print(decode_sequence(test_sentence_tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mzEK7VXlt-cC"
   },
   "outputs": [],
   "source": [
    "for seq_index in range(100):\n",
    "    # Take one sequence (part of the training test)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TRzCicgft-cD"
   },
   "source": [
    "- Line 1: looping 100\n",
    "- Line 2 to 5: Take one sequence (part of the training test) for trying out decoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IzoqqivR9Ued"
   },
   "source": [
    "# Problem 2 Running a trained model for image captioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D_YTgWSY9ZHB"
   },
   "source": [
    "#### 1. Clone the repositories\n",
    "```bash\n",
    "git clone https://github.com/pdollar/coco.git\n",
    "cd coco/PythonAPI/\n",
    "make\n",
    "python setup.py build\n",
    "python setup.py install\n",
    "cd ../../\n",
    "git clone https://github.com/yunjey/pytorch-tutorial.git\n",
    "cd pytorch-tutorial/tutorials/03-advanced/image_captioning/\n",
    "```\n",
    "\n",
    "#### 2. Download the dataset\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "chmod +x download.sh\n",
    "./download.sh\n",
    "```\n",
    "\n",
    "#### 3. Preprocessing\n",
    "\n",
    "```bash\n",
    "python build_vocab.py   \n",
    "python resize.py\n",
    "```\n",
    "\n",
    "#### 4. Train the model\n",
    "\n",
    "```bash\n",
    "python train.py    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dBzxqspk-v2H"
   },
   "source": [
    "#### 5. Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QsF4w7AI9d3t",
    "outputId": "0bd12725-351a-4fd0-fb9a-6dd095d33b9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XaJrKSXX9xjJ",
    "outputId": "533c8026-543d-49ab-afac-3e4fd5b0608e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "<start> a group of giraffes standing next to each other . <end>\n"
     ]
    }
   ],
   "source": [
    "! python /content/gdrive/MyDrive/Dive_into_code/Assignments/Time_series_data_processing/image_captioning/sample.py \\\n",
    "    --image='/content/gdrive/MyDrive/Dive_into_code/Assignments/Time_series_data_processing/image_captioning/png/example.png' \\\n",
    "    --vocab_path='/content/gdrive/MyDrive/Dive_into_code/Assignments/Time_series_data_processing/image_captioning/models/vocab.pkl' \\\n",
    "    --decoder_path='/content/gdrive/MyDrive/Dive_into_code/Assignments/Time_series_data_processing/image_captioning/models/decoder-5-3000.pkl' \\\n",
    "    --encoder_path='/content/gdrive/MyDrive/Dive_into_code/Assignments/Time_series_data_processing/image_captioning/models/encoder-5-3000.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1r9-LYkOBFov"
   },
   "source": [
    "# Question 3 Investigate what to do if you want to run with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tWXqopL8BP1Q"
   },
   "source": [
    "If you want to run a PyTorch implementation using Keras and also utilize pre-trained weights from PyTorch, there are several steps you can follow. Please note that direct conversion of models or weights between PyTorch and Keras is not possible due to differences in their underlying frameworks. However, you can still achieve the desired outcome by adapting the model architecture and transferring the weights manually.\n",
    "\n",
    "Here's a step-by-step guide on how you can proceed:\n",
    "\n",
    "Understand the model architecture: Begin by studying the PyTorch model architecture that you want to convert to Keras. This includes understanding the layers, activation functions, and any specific modifications that might have been made to the original architecture.\n",
    "\n",
    "Implement the Keras model: Start by creating a new model in Keras with the same architecture as the PyTorch model. You can use the Keras Sequential API or the functional API, depending on the complexity of the model. Make sure to define the same layers and activations as in the PyTorch model. Pay attention to any differences in layer ordering, padding, or parameter initialization between PyTorch and Keras.\n",
    "\n",
    "Load the pre-trained PyTorch weights: Since direct conversion of weights is not possible, you need to load the pre-trained PyTorch weights separately and manually transfer them to the Keras model. First, save the PyTorch model's state dictionary using torch.save(model.state_dict(), 'model_weights.pth').\n",
    "\n",
    "Adapt the weights for Keras: To adapt the PyTorch weights for use in Keras, you need to iterate through the layers of both models and manually transfer the weights. You can load the saved PyTorch weights using state_dict = torch.load('model_weights.pth'). Then, for each layer in the Keras model, extract the corresponding weights from the PyTorch state dictionary and set them in the Keras model using layer.set_weights(weights). Pay attention to the shape and ordering of the weights in PyTorch and Keras, as they might differ.\n",
    "\n",
    "Freeze layers if necessary: If you want to keep some layers frozen (not trainable) in Keras, you can set the trainable attribute of those layers to False. This is particularly useful when using pre-trained weights, as you might want to freeze the initial layers and only fine-tune the later layers.\n",
    "\n",
    "Compile and train the Keras model: Once the model architecture and weights have been adapted, compile the Keras model by specifying the optimizer, loss function, and any metrics you want to track during training. Then, you can train the model on your dataset using the Keras API, providing the input data and corresponding labels.\n",
    "\n",
    "It's important to note that despite following these steps, the performance of the model may vary between PyTorch and Keras due to differences in optimization algorithms and numerical precision. It is advisable to evaluate and fine-tune the Keras model to ensure comparable performance to the original PyTorch implementation.\n",
    "\n",
    "By following these steps, you should be able to convert a PyTorch implementation to Keras, including the usage of pre-trained weights from PyTorch."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
