{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nbilgee/git_exercise/blob/main/Recurrent_neural_network_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "pCv6-OlYsQ4X"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Problem 1] Forward propagation implementation of SimpleRNN"
      ],
      "metadata": {
        "id": "wvob3RHewIrI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a SimpleRNN class SimpleRNN. The basic structure will be the same as the FC class.\n",
        "\n",
        "The forward propagation formula looks like this: It also describes what the shape of ndarray will be.\n",
        "\n",
        "We denote the batch size batch_size, the number of input features n_features, and the number of RNN nodes . n_nodesThe activation function proceeds as tanh, but it can be replaced with ReLU, etc., as in previous neural networks."
      ],
      "metadata": {
        "id": "3MOZrV1ozneJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Activation functions\n"
      ],
      "metadata": {
        "id": "bMqIgV2dtwJG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Sigmoid:\n",
        "    def forward(self, A):\n",
        "        self.A = A\n",
        "        return self.sigmoid(A)\n",
        "    def backward(self, dZ):\n",
        "        _sig = self.sigmoid(self.A)\n",
        "        return dZ * (1 - _sig)*_sig\n",
        "    def sigmoid(self, X):\n",
        "        return 1 / (1 + np.exp(-X))\n",
        "\n",
        "class Tanh:\n",
        "    def forward(self, A):\n",
        "        self.A = A\n",
        "        return np.tanh(A)\n",
        "    def backward(self, dZ):\n",
        "        return dZ * (1 - (np.tanh(self.A))**2)\n",
        "\n",
        "class Softmax:\n",
        "    def forward(self, X):\n",
        "        self.Z = np.exp(X) / np.sum(np.exp(X), axis=1).reshape(-1,1)\n",
        "        return self.Z\n",
        "    def backward(self, Y):\n",
        "        self.loss = self.loss_func(Y)\n",
        "        return self.Z - Y\n",
        "    def loss_func(self, Y, Z=None):\n",
        "        if Z is None:\n",
        "            Z = self.Z\n",
        "        return (-1)*np.average(np.sum(Y*np.log(Z), axis=1))\n",
        "\n",
        "class ReLU:\n",
        "    def forward(self, A):\n",
        "        self.A = A\n",
        "        return np.clip(A, 0, None)\n",
        "    def backward(self, dZ):\n",
        "        return dZ * np.clip(np.sign(self.A), 0, None)"
      ],
      "metadata": {
        "id": "QBVOaRqitrIG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleRNN:\n",
        "    def __init__(self, x, w_h, w_x,b):\n",
        "        self.w_h = w_h\n",
        "        self.w_x = w_x\n",
        "        self.b = b\n",
        "        self.batch_size = x.shape[0] # 1\n",
        "        self.n_sequences = x.shape[1] # 3\n",
        "        self.n_features = x.shape[2] # 2\n",
        "        self.n_nodes = w_x.shape[1] # 4\n",
        "        self.h = np.zeros((batch_size, n_nodes)) # (batch_size, n_nodes)\n",
        "        self.b = np.array([1, 1, 1, 1]) # (n_nodes,)\n",
        "\n",
        "    def forward(x,h):\n",
        "        for n in range(n_sequences):\n",
        "            h = np.tanh(x[:, n, :] @ w_x + h @ w_h + b)\n",
        "        return h\n",
        "\n",
        "    def backward(self, dA):\n",
        "        dZ = dA@self.W.T\n",
        "        self.dB = np.sum(dA, axis=0)\n",
        "        self.dW = self.X.T@dA\n",
        "        self.optimizer.update(self)\n",
        "        return dZ"
      ],
      "metadata": {
        "id": "hIcyjQhGtqRP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Problem 2] Forward propagation experiment with small sequences"
      ],
      "metadata": {
        "id": "3DEFKctCzqjK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider forward propagation on small arrays.\n",
        "\n",
        "Let input x, initial state h, weights w_x and w_h, bias b be:\n",
        "\n",
        "Here the axes of the array x are in order of batch size, number of series, and number of features."
      ],
      "metadata": {
        "id": "OupRYt7KzttL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9gywsvrRwDzF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class SimpleRNN:\n",
        "    def __init__(self, x, w_h, w_x, b):\n",
        "        self.batch_size = x.shape[0]\n",
        "        self.n_sequences = x.shape[1]\n",
        "        self.n_features = x.shape[2]\n",
        "        self.n_nodes = w_x.shape[1]\n",
        "        self.h = np.zeros((self.batch_size, self.n_nodes))\n",
        "        self.b = b\n",
        "\n",
        "        # Your other initialization code...\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(x, h):\n",
        "        # Forward pass implementation...\n",
        "        pass\n",
        "\n",
        "# Your code snippet...\n",
        "x = np.array([[[1, 2], [2, 3], [3, 4]]])/100\n",
        "w_x = np.array([[1, 3, 5, 7], [3, 5, 7, 8]])/100\n",
        "w_h = np.array([[1, 3, 5, 7], [2, 4, 6, 8], [3, 5, 7, 8], [4, 6, 8, 10]])/100\n",
        "RNN = SimpleRNN(x=x, w_h=w_h, w_x=w_x, b=0.1)\n",
        "answer = SimpleRNN.forward(x, RNN.h)\n"
      ]
    }
  ]
}