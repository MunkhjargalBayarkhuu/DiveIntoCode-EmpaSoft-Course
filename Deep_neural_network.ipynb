{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0xLwcrxHZVys"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.datasets import mnist\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zC_CsyUESTvP"
   },
   "source": [
    "[Problem 1] Classification of fully connected layers\n",
    "Please classify the fully connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W3yw-bAdarYN"
   },
   "outputs": [],
   "source": [
    "\n",
    "class FC:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer,activation):\n",
    "        self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.optimizer = optimizer\n",
    "        self.initializer = initializer\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        self.activation = activation\n",
    "        # Initialization\n",
    "        # Initialize self.W and self.B using the #initialr method\n",
    "\n",
    "        self.W = self.initializer.W(self.n_nodes1, self.n_nodes2)\n",
    "        self.B = self.initializer.B(self.n_nodes2)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.A = np.dot(self.X, self.W) + self.B\n",
    "        A = self.activation.forward(self.A)\n",
    "        return A\n",
    "\n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        dA = self.activation.backward(dA)\n",
    "        self.dB = np.mean(dA, axis = 0)\n",
    "        self.dW = np.dot(self.X.T, dA)/len(self.X)\n",
    "        dZ = np.dot(dA, self.W.T)\n",
    "\n",
    "        self = self.optimizer.update(self)\n",
    "        return dZ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_IMJHx7dU1As"
   },
   "source": [
    "[Problem 2] Classification of initialization method\n",
    "Classify the code that does the initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FdFZ2VrfawSt"
   },
   "outputs": [],
   "source": [
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        return W\n",
    "\n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        B = np.zeros(n_nodes2)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yWX4-VpEU6DQ"
   },
   "source": [
    "[Problem 3] Classification of optimization methods Do a class of optimization methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rQPFArR-U8x7"
   },
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "        \"\"\"\n",
    "        layer.W -= self.lr*layer.dW\n",
    "        layer.B -= self.lr*layer.dB\n",
    "\n",
    "        return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BvQq2NTHU9HS"
   },
   "source": [
    "[Problem 4] Classification of activation function Please classify the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V3xcwbOrVA5f"
   },
   "outputs": [],
   "source": [
    "class softmax():\n",
    "  \"\"\"\n",
    "  Activation function - softmax\n",
    "  \"\"\"\n",
    "  def __init__(self):\n",
    "      pass\n",
    "\n",
    "  def forward(self, A):\n",
    "    #print(\"A:\", A)\n",
    "    #print(\"temp:\", np.exp(A - np.max(A)))\n",
    "    #print(\"forward temp:\", np.sum(np.exp(A-np.max(A))))\n",
    "    temp = np.exp(A - np.max(A))/np.sum(np.exp(A-np.max(A)), axis = 1, keepdims= True)\n",
    "    return temp\n",
    "\n",
    "  def backward(self, dZ):\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ww4836mOVBRj"
   },
   "source": [
    "[Problem 5] Create ReLU class Implement ReLU (Rectified Linear Unit), a currently commonly used activation function, as a ReLU class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NiFkU5omVFC-"
   },
   "outputs": [],
   "source": [
    "class ReLU():\n",
    "  \"\"\"\n",
    "  Activation function - relu\n",
    "  \"\"\"\n",
    "  def __init__(self):\n",
    "      pass\n",
    "\n",
    "  def forward(self, A):\n",
    "    self.A = A\n",
    "    temp = np.maximum(self.A, 0)\n",
    "    return temp\n",
    "\n",
    "  def backward(self, dZ):\n",
    "    ret = np.where(self.A>0, dZ, 0)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0g0ptMsybJoN"
   },
   "outputs": [],
   "source": [
    "class tanh():\n",
    "  \"\"\"\n",
    "  Activation function - tangent\n",
    "  \"\"\"\n",
    "  def __init__(self):\n",
    "      pass\n",
    "\n",
    "  def forward(self, A):\n",
    "    self.A = A\n",
    "    self.Z = np.tanh(self.A)\n",
    "\n",
    "    return self.Z\n",
    "\n",
    "  def backward(self, dZ):\n",
    "    ret = dZ * (1-self.Z**2)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xuwSmUexVFbN"
   },
   "source": [
    "[Problem 6] Initial value of weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8tzRlePMVJfs"
   },
   "outputs": [],
   "source": [
    "class XavierInitializer:\n",
    "    \"\"\"\n",
    "    Xavier initialization\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        Weight initialization\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          Number of nodes in the previous layer\n",
    "        n_nodes2 : int\n",
    "          Number of nodes in the later layer\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W : ndarray shape with (n_nodes1, n_nodes2)\n",
    "          weights of hidden layer\n",
    "        \"\"\"\n",
    "        W = np.random.randn(n_nodes1, n_nodes2)/np.sqrt(n_nodes1)\n",
    "        return W\n",
    "\n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        bias initializer\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          Number of nodes in the later layer\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B : ndarray shape with (n_nodes2, 1)\n",
    "          bias of hidden layer\n",
    "        \"\"\"\n",
    "        B = np.zeros(n_nodes2)\n",
    "        return B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O29y5dSuXcAS"
   },
   "outputs": [],
   "source": [
    "class HeInitializer:\n",
    "    \"\"\"\n",
    "    He initialization\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        Weight initialization\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          Number of nodes in the previous layer\n",
    "        n_nodes2 : int\n",
    "          Number of nodes in the later layer\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W : ndarray shape with (n_nodes1, n_nodes2)\n",
    "          weights of hidden layer\n",
    "        \"\"\"\n",
    "        W = np.random.randn(n_nodes1, n_nodes2)/np.sqrt(2/n_nodes1)\n",
    "        return W\n",
    "\n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        bias initializer\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          Number of nodes in the later layer\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B : ndarray shape with (n_nodes2, 1)\n",
    "          bias of hidden layer\n",
    "        \"\"\"\n",
    "        B = np.zeros(n_nodes2)\n",
    "        return B\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PEwgrRNNVJy8"
   },
   "source": [
    "[Problem 7] Optimization method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ISHGPxODVPKg"
   },
   "outputs": [],
   "source": [
    "class Adagrad:\n",
    "  \"\"\"\n",
    "  stochastic gradient descent method\n",
    "\n",
    "  Parameters\n",
    "  -----------\n",
    "  lr : learning rate\n",
    "  \"\"\"\n",
    "  def __init__(self, lr):\n",
    "      self.lr = lr\n",
    "      self.hW = 0\n",
    "      self.hB = 0\n",
    "\n",
    "  def update(self,layer):\n",
    "    \"\"\"\n",
    "    Updating the weights and biases of a layer\n",
    "    Parameters\n",
    "    -----------\n",
    "    layer : Instance of the layer before the update\n",
    "    \"\"\"\n",
    "    self.hW += layer.dW * layer.dW\n",
    "    self.hB = layer.dB * layer.dB\n",
    "\n",
    "    layer.W -= self.lr * layer.dW/(np.sqrt(self.hW) + 1e-7)\n",
    "    layer.B -=self.lr * layer.dB/(np.sqrt(self.hB) + 1e-7)\n",
    "\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2jNL5QDobd_o"
   },
   "outputs": [],
   "source": [
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    Iterator to get a mini-batch\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray, shape (n_samples, n_features)\n",
    "      training data\n",
    "    y : ndarray, shape (n_samples, 1)\n",
    "      Label of training data\n",
    "    batch_size : int\n",
    "      batch size\n",
    "    seed : int\n",
    "      NumPy random seed\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 20, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self._X = X[shuffle_index]\n",
    "        self._y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self._X[p0:p1], self._y[p0:p1]\n",
    "\n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self._X[p0:p1], self._y[p0:p1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E9K9BHm5VPYk"
   },
   "source": [
    "[Problem 8] Class Completion Complete the ScratchDeepNeuralNetrowkClassifier class for training and estimation with any configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GCk71TuRVSr_"
   },
   "outputs": [],
   "source": [
    "class ScratchDeepNeuralNetrowkClassifier():\n",
    "    \"\"\"\n",
    "    A Simple three-layer neural network classifier\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_epoch = 50, n_features = 784, n_nodes1 = 400, n_nodes2 = 200, n_output = 10,\n",
    "                sigma = 0.01, n_batch = 20, learning_rate = 0.01, verbose = False):\n",
    "        # Record hyperparameters as attributes\n",
    "        self.verbose = verbose\n",
    "        self.n_epoch = n_epoch\n",
    "        self.n_features = n_features\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        self.n_output = n_output\n",
    "        self.sigma = sigma\n",
    "        self.n_batch = n_batch\n",
    "        self.lr = learning_rate\n",
    "        self.log_loss = np.zeros(self.n_epoch)\n",
    "\n",
    "    def loss_function(self, y, yt):\n",
    "        delta = 1e-7\n",
    "        return -np.mean(yt*np.log(y + delta))\n",
    "\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        Train a neural network classifier.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray, shape (n_samples, n_features)\n",
    "            Features of training data\n",
    "        y : ndarray, shape (n_samples, )\n",
    "            Label of training data\n",
    "        X_val : ndarray, shape (n_samples, n_features)\n",
    "            Features of validation data\n",
    "        y_val : ndarray, shape (n_samples, )\n",
    "            Label of validation data\n",
    "        \"\"\"\n",
    "        #self.log_loss = []\n",
    "        # initialize weights\n",
    "        optimizer1 = Adagrad(self.lr)\n",
    "        optimizer2 = Adagrad(self.lr)\n",
    "        optimizer3 = Adagrad(self.lr)\n",
    "\n",
    "        initializer1 = XavierInitializer()\n",
    "        initializer2 = HeInitializer()\n",
    "        initializer3 = SimpleInitializer(self.sigma)\n",
    "\n",
    "        # the loss function for each epoch\n",
    "\n",
    "\n",
    "        self.FC1 = FC(self.n_features, self.n_nodes1, initializer1, optimizer1,tanh())\n",
    "        self.FC2 = FC(self.n_nodes1, self.n_nodes2, initializer2, optimizer2,tanh())\n",
    "        self.FC3 = FC(self.n_nodes2, self.n_output, initializer3, optimizer3,softmax())\n",
    "\n",
    "        for epoch in range(self.n_epoch):\n",
    "            get_mini_batch = GetMiniBatch(X, y , batch_size = self.n_batch)\n",
    "            self.loss = 0\n",
    "\n",
    "            for mini_X_train , mini_y_train in get_mini_batch:\n",
    "                #forward propagation\n",
    "                #1st layer\n",
    "                self.z1 = self.FC1.forward(mini_X_train)\n",
    "                #2nd layer\n",
    "                self.z2 = self.FC2.forward(self.z1)\n",
    "                #3rd layer\n",
    "                self.z3 = self.FC3.forward(self.z2)\n",
    "\n",
    "                # back propagation\n",
    "                #3rd layer\n",
    "                self.dA3 = (self.z3 - mini_y_train)/self.n_batch\n",
    "                #2nd layer\n",
    "                self.dz2 = self.FC3.backward(self.dA3)\n",
    "                #1st layer\n",
    "                self.dz1 = self.FC2.backward(self.dz2)\n",
    "                self.dz0 = self.FC1.backward(self.dz1)\n",
    "\n",
    "                #backpropagation\n",
    "                self.loss += self.loss_function(self.z3, mini_y_train)\n",
    "\n",
    "            # record loss function for each epoch\n",
    "            self.log_loss[epoch] = self.loss/len(get_mini_batch)\n",
    "\n",
    "            if self.verbose:\n",
    "                print('epoch:{} loss:{}'.format(epoch, self.loss/self.n_batch))\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Estimate using a neural network classifier.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray, shape (n_samples, n_features)\n",
    "            training sample\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pred:    ndarray, shape (n_samples, 1)\n",
    "            predicted result\n",
    "        \"\"\"\n",
    "\n",
    "        pred_z1 = self.FC1.forward(X)\n",
    "        pred_z2 = self.FC2.forward(pred_z1)\n",
    "        pred = np.argmax(self.FC3.forward(pred_z2), axis=1)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uRp2pPU4VTBd"
   },
   "source": [
    "[Problem 9] Learning and Estimation Create several networks with varying numbers of layers and activation functions. Then learn and estimate the MNIST data and calculate Accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "maqGoIMwVXQn",
    "outputId": "3365a770-194a-46e7-a1de-915e81dc9034"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 0s 0us/step\n",
      "X_train data shape:  (60000, 28, 28)\n",
      "X_test data shape:  (10000, 28, 28)\n",
      "X_train flatten data shape:  (60000, 784)\n",
      "X_test flatten data shape:  (10000, 784)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-9595c8a4a04c>:14: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  X_train = X_train.astype(np.float)\n",
      "<ipython-input-13-9595c8a4a04c>:15: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  X_test = X_test.astype(np.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train max value: 1.0\n",
      "X_train min value: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/_encoders.py:828: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/_encoders.py:828: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: (48000, 784)\n",
      "Validation dataset: (12000, 784)\n",
      "epoch:0 loss:6.115341368262553\n",
      "epoch:1 loss:3.8819392454392814\n",
      "epoch:2 loss:3.451263065183187\n",
      "epoch:3 loss:3.1544111614020856\n",
      "epoch:4 loss:3.0308962236553296\n",
      "epoch:5 loss:2.847552346435332\n",
      "epoch:6 loss:2.7953375029310505\n",
      "epoch:7 loss:2.7377692594455754\n",
      "epoch:8 loss:2.615314933337907\n",
      "epoch:9 loss:2.6270267429572325\n",
      "epoch:10 loss:2.543193182996141\n",
      "epoch:11 loss:2.4442948543447267\n",
      "epoch:12 loss:2.388421980111398\n",
      "epoch:13 loss:2.3749544858940483\n",
      "epoch:14 loss:2.356599475586729\n",
      "epoch:15 loss:2.345938797164099\n",
      "epoch:16 loss:2.282332915563569\n",
      "epoch:17 loss:2.266623525429945\n",
      "epoch:18 loss:2.2334548198918247\n",
      "epoch:19 loss:2.19175003033553\n",
      "epoch:20 loss:2.128527497338418\n",
      "epoch:21 loss:2.180805774274918\n",
      "epoch:22 loss:2.1627360310033397\n",
      "epoch:23 loss:2.142525782383358\n",
      "epoch:24 loss:2.071844628486685\n",
      "[3 5 1 ... 7 1 9]\n",
      "Accuracy: 0.94125\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "print(\"X_train data shape: \", X_train.shape) # (60000, 28, 28)\n",
    "print(\"X_test data shape: \", X_test.shape) # (10000, 28, 28)\n",
    "\n",
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)\n",
    "\n",
    "print(\"X_train flatten data shape: \", X_train.shape) # (60000, 28, 28)\n",
    "print(\"X_test flatten data shape: \", X_test.shape) # (10000, 28, 28)\n",
    "\n",
    "# Preprocessing\n",
    "\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print('X_train max value:', X_train.max()) # 1.0\n",
    "print('X_train min value:', X_train.min()) # 0.0\n",
    "\n",
    "# the correct label is an integer from 0 to 9, but it is converted to a one-hot representation\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train.reshape(-1,1))\n",
    "y_val_one_hot = enc.fit_transform(y_val.reshape(-1,1))\n",
    "print(\"Train dataset:\", X_train.shape) # (48000, 784)\n",
    "print(\"Validation dataset:\", X_val.shape) # (12000, 784)\n",
    "\n",
    "clf = ScratchDeepNeuralNetrowkClassifier(n_epoch = 25, n_features = 784, n_nodes1 = 400, n_nodes2 = 200, n_output = 10,\n",
    "                sigma = 0.01, n_batch = 20, learning_rate = 0.01, verbose = True)\n",
    "\n",
    "clf.fit(X_train, y_train_one_hot)\n",
    "pred = clf.predict(X_val)\n",
    "print(pred)\n",
    "acc = accuracy_score(y_val, pred)\n",
    "print(\"Accuracy:\", acc)\n",
    "\n",
    "fig = plt.subplots()\n",
    "plt.title(\"Loss learning graph\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(clf.log_loss)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
