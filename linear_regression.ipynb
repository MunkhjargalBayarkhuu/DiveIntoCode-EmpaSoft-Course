{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "732abb2f-5036-4cd9-a468-96f3c987a4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d0d8f1-c191-4329-8b4c-3a7091a76474",
   "metadata": {},
   "source": [
    "## Linear Regression Stratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "783e518f-18f9-4ffd-9bf7-3baba538d2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchLinearRegression():\n",
    "    \n",
    "    \"\"\"\n",
    "    線形回帰のスクラッチ実装\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    num_iter : int\n",
    "      イテレーション数\n",
    "    lr : float\n",
    "      学習率\n",
    "    no_bias : bool\n",
    "      バイアス項を入れない場合はTrue\n",
    "    verbose : bool\n",
    "      学習過程を出力する場合はTrue\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    self.coef_ : 次の形のndarray, shape (n_features,)\n",
    "      パラメータ\n",
    "    self.loss : 次の形のndarray, shape (self.iter,)\n",
    "      訓練データに対する損失の記録\n",
    "    self.val_loss : 次の形のndarray, shape (self.iter,)\n",
    "      検証データに対する損失の記録\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_iter, lr, no_bias, verbose): \n",
    "        \n",
    "        self.iter = num_iter\n",
    "        self.lr = lr\n",
    "        self.bias = no_bias\n",
    "        self.verbose = verbose\n",
    "     \n",
    "        self.loss = np.zeros(self.iter)\n",
    "        self.val_loss = np.zeros(self.iter)\n",
    "      \n",
    "\n",
    "    # 問題6（学習と推定）\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        線形回帰を学習する。検証データが入力された場合はそれに対する損失と精度もイテレーションごとに計算する。\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            訓練データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, n_features)\n",
    "            検証データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証データの正解値\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.bias == True:\n",
    "            bias = np.ones((X.shape[0], 1))\n",
    "            X = np.hstack((bias, X))\n",
    "            if X_val is not None:\n",
    "                bias = np.ones((X_val.shape[0], 1))\n",
    "                X_val = np.hstack((bias, X_val))\n",
    "            self.coef_ = np.random.rand(X.shape[1])\n",
    "            self.coef_ = self.coef_.reshape(X.shape[1], 1)\n",
    "    \n",
    "\n",
    "        for epoch in range(self.iter):\n",
    "            y_pred = self._linear_hypothesis(X)\n",
    "            self.loss[epoch] = np.mean((y-y_pred)**2)\n",
    "            \n",
    "            if X_val is not None:\n",
    "                pred_val = self._linear_hypothesis(X_val)\n",
    "                self.val_loss[epoch] = np.mean((y_val-pred_val)**2)\n",
    "                \n",
    "            self.coef_ = self._gradient_descent(X, (y_pred-y))\n",
    "           \n",
    "            if self.verbose == True:\n",
    "                print('{}-th epoch train loss {}'.format(epoch, self.loss[epoch]))\n",
    "                if X_val is not None:\n",
    "                    print('{}-th epoch val loss {}'.format(epoch, self.val_loss[epoch] ))\n",
    "\n",
    "\n",
    "    # 問題1\n",
    "    def _linear_hypothesis(self, X):\n",
    "        \"\"\"\n",
    "        仮定関数の出力を計算する\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "          訓練データ\n",
    "        Returns\n",
    "        -------\n",
    "        次の形のndarray, shape (n_samples, 1)\n",
    "        線形の仮定関数による推定結果\n",
    "        \"\"\"\n",
    "        pred = X @ self.coef_\n",
    "        \n",
    "        return pred\n",
    "\n",
    "    # 問題2\n",
    "    def _gradient_descent(self, X, error):\n",
    "\n",
    "        for i in range(X.shape[1]):\n",
    "            gradient = error*X[:, i]\n",
    "            self.coef_[i, :] = self.coef_[i, :] - self.lr * np.mean(gradient)\n",
    "\n",
    "        return self.coef_\n",
    "        \n",
    "\n",
    "    # 問題3\n",
    "    def predict(self, X):\n",
    "        if self.bias == True:\n",
    "            bias = np.ones(X.shape[0]).reshape(X.shape[0], 1)\n",
    "            X = np.hstack([bias, X])\n",
    "        pred_y = self._linear_hypothesis(X)\n",
    "        return pred_y\n",
    "\n",
    "    # 問題4\n",
    "    def _mse(self, y_pred, y):\n",
    "        \"\"\"\n",
    "        平均二乗誤差の計算\n",
    "        \"\"\"\n",
    "        mse = np.mean((y-y_pred)**2)\n",
    "        \n",
    "        return mse\n",
    "\n",
    "    # 問題5\n",
    "    def _loss_func(self, pred, y):\n",
    "        \"\"\"\n",
    "        損失関数\n",
    "        \"\"\"\n",
    "        loss = self._mse(pred, y)/2\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816a4c91-6241-4c9c-a7b5-ccaa1e1b493e",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d98c652b-be72-4a56-b60a-a381d1f4f23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "dataset = pd.read_csv(\"data/train.csv\")\n",
    "X = dataset.loc[:, ['GrLivArea', 'YearBuilt']]\n",
    "y = dataset.loc[:, ['SalePrice']]\n",
    "X = X.values\n",
    "X = MinMaxScaler().fit_transform(X)\n",
    "y = np.log(y.values)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21043081-1145-4c9a-a5fc-85124e5da187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-th epoch train loss 118.63238896768907\n",
      "0-th epoch val loss 118.68297836486391\n",
      "1-th epoch train loss 114.95156140923724\n",
      "1-th epoch val loss 114.9750349167016\n",
      "2-th epoch train loss 111.38539497562134\n",
      "2-th epoch val loss 111.38295245455848\n",
      "3-th epoch train loss 107.93032617733896\n",
      "3-th epoch val loss 107.90312467544035\n",
      "4-th epoch train loss 104.5829021433841\n",
      "4-th epoch val loss 104.53205730997756\n",
      "5-th epoch train loss 101.3397771894218\n",
      "5-th epoch val loss 101.26636464535301\n",
      "6-th epoch train loss 98.19770949240002\n",
      "6-th epoch val loss 98.10276615609166\n",
      "7-th epoch train loss 95.15355786829852\n",
      "7-th epoch val loss 95.03808323936688\n",
      "8-th epoch train loss 92.20427864981599\n",
      "8-th epoch val loss 92.0692360515817\n",
      "9-th epoch train loss 89.3469226608968\n",
      "9-th epoch val loss 89.1932404430845\n",
      "10-th epoch train loss 86.57863228509375\n",
      "10-th epoch val loss 86.40720498797555\n",
      "11-th epoch train loss 83.89663862485763\n",
      "11-th epoch val loss 83.70832810605498\n",
      "12-th epoch train loss 81.29825874893315\n",
      "12-th epoch val loss 81.09389527405497\n",
      "13-th epoch train loss 78.78089302512957\n",
      "13-th epoch val loss 78.56127632338676\n",
      "14-th epoch train loss 76.34202253581823\n",
      "14-th epoch val loss 76.10792282171947\n",
      "15-th epoch train loss 73.97920657359157\n",
      "15-th epoch val loss 73.73136553579062\n",
      "16-th epoch train loss 71.69008021459786\n",
      "16-th epoch val loss 71.4292119729291\n",
      "17-th epoch train loss 69.47235196714266\n",
      "17-th epoch val loss 69.19914399884937\n",
      "18-th epoch train loss 67.32380149322324\n",
      "18-th epoch val loss 67.03891552935121\n",
      "19-th epoch train loss 65.24227740073374\n",
      "19-th epoch val loss 64.94635029363312\n",
      "20-th epoch train loss 63.22569510414993\n",
      "20-th epoch val loss 62.91933966699787\n",
      "21-th epoch train loss 61.272034751569564\n",
      "21-th epoch val loss 60.955840570798436\n",
      "22-th epoch train loss 59.37933921605102\n",
      "22-th epoch val loss 59.053873437538556\n",
      "23-th epoch train loss 57.545712149255586\n",
      "23-th epoch val loss 57.21152023910713\n",
      "24-th epoch train loss 55.7693160954618\n",
      "24-th epoch val loss 55.42692257618854\n",
      "25-th epoch train loss 54.04837066407938\n",
      "25-th epoch val loss 53.69827982695108\n",
      "26-th epoch train loss 52.38115075884856\n",
      "26-th epoch val loss 52.02384735317545\n",
      "27-th epoch train loss 50.765984861967034\n",
      "27-th epoch val loss 50.40193476204118\n",
      "28-th epoch train loss 49.20125337144076\n",
      "28-th epoch val loss 48.83090422184517\n",
      "29-th epoch train loss 47.68538699000851\n",
      "29-th epoch val loss 47.30916882997926\n",
      "30-th epoch train loss 46.2168651640405\n",
      "30-th epoch val loss 45.83519103154603\n",
      "31-th epoch train loss 44.79421457086104\n",
      "31-th epoch val loss 44.40748108704225\n",
      "32-th epoch train loss 43.41600765299406\n",
      "32-th epoch val loss 43.024595587587946\n",
      "33-th epoch train loss 42.08086119787573\n",
      "33-th epoch val loss 41.68513601622643\n",
      "34-th epoch train loss 40.78743496162442\n",
      "34-th epoch val loss 40.387747353866224\n",
      "35-th epoch train loss 39.53443033550158\n",
      "35-th epoch val loss 39.131116728480265\n",
      "36-th epoch train loss 38.320589053739624\n",
      "36-th epoch val loss 37.91397210622068\n",
      "37-th epoch train loss 37.14469194145383\n",
      "37-th epoch val loss 36.73508102314884\n",
      "38-th epoch train loss 36.00555770139512\n",
      "38-th epoch val loss 35.59324935632102\n",
      "39-th epoch train loss 34.90204173833923\n",
      "39-th epoch val loss 34.48732013300896\n",
      "40-th epoch train loss 33.83303501994492\n",
      "40-th epoch val loss 33.41617237687222\n",
      "41-th epoch train loss 32.79746297295032\n",
      "41-th epoch val loss 32.378719989936435\n",
      "42-th epoch train loss 31.794284413611297\n",
      "42-th epoch val loss 31.373910669266536\n",
      "43-th epoch train loss 30.822490511320012\n",
      "43-th epoch val loss 30.400724857258897\n",
      "44-th epoch train loss 29.88110378437454\n",
      "44-th epoch val loss 29.458174724509504\n",
      "45-th epoch train loss 28.969177126902384\n",
      "45-th epoch val loss 28.54530318424768\n",
      "46-th epoch train loss 28.085792865971772\n",
      "46-th epoch val loss 27.661182937356145\n",
      "47-th epoch train loss 27.23006184795447\n",
      "47-th epoch val loss 26.804915547028763\n",
      "48-th epoch train loss 26.401122553232742\n",
      "48-th epoch val loss 25.97563054214643\n",
      "49-th epoch train loss 25.598140238371585\n",
      "49-th epoch val loss 25.172484548480373\n",
      "50-th epoch train loss 24.820306104904276\n",
      "50-th epoch val loss 24.394660446859625\n",
      "51-th epoch train loss 24.06683649390585\n",
      "51-th epoch val loss 23.641366557466153\n",
      "52-th epoch train loss 23.336972105554725\n",
      "52-th epoch val loss 22.91183584944718\n",
      "53-th epoch train loss 22.62997724290741\n",
      "53-th epoch val loss 22.205325175059304\n",
      "54-th epoch train loss 21.94513907913539\n",
      "54-th epoch val loss 21.521114527583478\n",
      "55-th epoch train loss 21.281766947496408\n",
      "55-th epoch val loss 20.858506322273254\n",
      "56-th epoch train loss 20.639191653335153\n",
      "56-th epoch val loss 20.21682469962194\n",
      "57-th epoch train loss 20.01676480742988\n",
      "57-th epoch val loss 19.595414850256127\n",
      "58-th epoch train loss 19.413858180023176\n",
      "58-th epoch val loss 18.993642360784808\n",
      "59-th epoch train loss 18.82986307489506\n",
      "59-th epoch val loss 18.41089257995387\n",
      "60-th epoch train loss 18.264189722856983\n",
      "60-th epoch val loss 17.84657000447611\n",
      "61-th epoch train loss 17.716266694064227\n",
      "61-th epoch val loss 17.30009768392626\n",
      "62-th epoch train loss 17.185540328563086\n",
      "62-th epoch val loss 16.770916644109718\n",
      "63-th epoch train loss 16.67147418450725\n",
      "63-th epoch val loss 16.25848532833166\n",
      "64-th epoch train loss 16.173548503495287\n",
      "64-th epoch val loss 15.762279056011337\n",
      "65-th epoch train loss 15.691259692498203\n",
      "65-th epoch val loss 15.281789498103336\n",
      "66-th epoch train loss 15.224119821862555\n",
      "66-th epoch val loss 14.816524168804373\n",
      "67-th epoch train loss 14.771656138890405\n",
      "67-th epoch val loss 14.366005933040457\n",
      "68-th epoch train loss 14.333410596513001\n",
      "68-th epoch val loss 13.929772529244667\n",
      "69-th epoch train loss 13.908939396590023\n",
      "69-th epoch val loss 13.50737610695125\n",
      "70-th epoch train loss 13.497812547380674\n",
      "70-th epoch val loss 13.098382778746311\n",
      "71-th epoch train loss 13.099613434747123\n",
      "71-th epoch val loss 12.702372186129642\n",
      "72-th epoch train loss 12.713938406664246\n",
      "72-th epoch val loss 12.318937078856095\n",
      "73-th epoch train loss 12.340396370622983\n",
      "73-th epoch val loss 11.94768290733825\n",
      "74-th epoch train loss 11.978608403527343\n",
      "74-th epoch val loss 11.58822742770509\n",
      "75-th epoch train loss 11.628207373697503\n",
      "75-th epoch val loss 11.240200319124062\n",
      "76-th epoch train loss 11.288837574603514\n",
      "76-th epoch val loss 10.903242813005939\n",
      "77-th epoch train loss 10.960154369965691\n",
      "77-th epoch val loss 10.577007333723806\n",
      "78-th epoch train loss 10.641823849869127\n",
      "78-th epoch val loss 10.261157150488895\n",
      "79-th epoch train loss 10.333522497550637\n",
      "79-th epoch val loss 9.955366040037077\n",
      "80-th epoch train loss 10.034936866527124\n",
      "80-th epoch val loss 9.659317959790549\n",
      "81-th epoch train loss 9.745763267744508\n",
      "81-th epoch val loss 9.372706731169671\n",
      "82-th epoch train loss 9.465707466436417\n",
      "82-th epoch val loss 9.095235732739994\n",
      "83-th epoch train loss 9.194484388391423\n",
      "83-th epoch val loss 8.826617602889275\n",
      "84-th epoch train loss 8.931817835336949\n",
      "84-th epoch val loss 8.566573951738762\n",
      "85-th epoch train loss 8.677440209157023\n",
      "85-th epoch val loss 8.31483508200215\n",
      "86-th epoch train loss 8.431092244669887\n",
      "86-th epoch val loss 8.071139718514607\n",
      "87-th epoch train loss 8.192522750699842\n",
      "87-th epoch val loss 7.835234746162744\n",
      "88-th epoch train loss 7.961488359186101\n",
      "88-th epoch val loss 7.606874955954908\n",
      "89-th epoch train loss 7.737753282079258\n",
      "89-th epoch val loss 7.385822798979042\n",
      "90-th epoch train loss 7.521089075783824\n",
      "90-th epoch val loss 7.171848148003488\n",
      "91-th epoch train loss 7.311274412912675\n",
      "91-th epoch val loss 6.964728066483395\n",
      "92-th epoch train loss 7.108094861126652\n",
      "92-th epoch val loss 6.764246584743002\n",
      "93-th epoch train loss 6.911342668839432\n",
      "93-th epoch val loss 6.570194483111033\n",
      "94-th epoch train loss 6.7208165575746985\n",
      "94-th epoch val loss 6.382369081793416\n",
      "95-th epoch train loss 6.536321520769266\n",
      "95-th epoch val loss 6.200574037274249\n",
      "96-th epoch train loss 6.357668628822122\n",
      "96-th epoch val loss 6.0246191450423545\n",
      "97-th epoch train loss 6.18467484019566\n",
      "97-th epoch val loss 5.854320148447139\n",
      "98-th epoch train loss 6.0171628183812835\n",
      "98-th epoch val loss 5.689498553493482\n",
      "99-th epoch train loss 5.854960754547485\n",
      "99-th epoch val loss 5.529981449391321\n",
      "100-th epoch train loss 5.69790219569405\n",
      "100-th epoch val loss 5.375601334681342\n",
      "101-th epoch train loss 5.545825878141578\n",
      "101-th epoch val loss 5.226195948763638\n",
      "102-th epoch train loss 5.398575566190801\n",
      "102-th epoch val loss 5.0816081086616975\n",
      "103-th epoch train loss 5.2559998957912475\n",
      "103-th epoch val loss 4.941685550859159\n",
      "104-th epoch train loss 5.117952223063878\n",
      "104-th epoch val loss 4.806280778051889\n",
      "105-th epoch train loss 4.98429047752706\n",
      "105-th epoch val loss 4.675250910662817\n",
      "106-th epoch train loss 4.854877019879941\n",
      "106-th epoch val loss 4.548457542971651\n",
      "107-th epoch train loss 4.729578504201833\n",
      "107-th epoch val loss 4.425766603716223\n",
      "108-th epoch train loss 4.608265744430582\n",
      "108-th epoch val loss 4.3070482210266965\n",
      "109-th epoch train loss 4.490813584987123\n",
      "109-th epoch val loss 4.1921765915580185\n",
      "110-th epoch train loss 4.377100775417601\n",
      "110-th epoch val loss 4.081029853690409\n",
      "111-th epoch train loss 4.2670098489284065\n",
      "111-th epoch val loss 3.9734899646715203\n",
      "112-th epoch train loss 4.1604270046932506\n",
      "112-th epoch val loss 3.869442581577881\n",
      "113-th epoch train loss 4.057241993815329\n",
      "113-th epoch val loss 3.768776945977108\n",
      "114-th epoch train loss 3.957348008831094\n",
      "114-th epoch val loss 3.6713857721759373\n",
      "115-th epoch train loss 3.8606415766457602\n",
      "115-th epoch val loss 3.5771651389427803\n",
      "116-th epoch train loss 3.7670224547940236\n",
      "116-th epoch val loss 3.4860143845968845\n",
      "117-th epoch train loss 3.6763935309228453\n",
      "117-th epoch val loss 3.39783600535959\n",
      "118-th epoch train loss 3.5886607253962777\n",
      "118-th epoch val loss 3.3125355568664157\n",
      "119-th epoch train loss 3.5037328969254617\n",
      "119-th epoch val loss 3.230021558741784\n",
      "120-th epoch train loss 3.421521751129925\n",
      "120-th epoch val loss 3.1502054021413253\n",
      "121-th epoch train loss 3.341941751939178\n",
      "121-th epoch val loss 3.0730012601695758\n",
      "122-th epoch train loss 3.2649100357464964\n",
      "122-th epoch val loss 2.998326001083818\n",
      "123-th epoch train loss 3.1903463282294626\n",
      "123-th epoch val loss 2.9260991041975113\n",
      "124-th epoch train loss 3.118172863754503\n",
      "124-th epoch val loss 2.856242578399505\n",
      "125-th epoch train loss 3.048314307285215\n",
      "125-th epoch val loss 2.7886808832077716\n",
      "126-th epoch train loss 2.980697678716809\n",
      "126-th epoch val loss 2.7233408522789793\n",
      "127-th epoch train loss 2.9152522795613165\n",
      "127-th epoch val loss 2.6601516192976016\n",
      "128-th epoch train loss 2.851909621910657\n",
      "128-th epoch val loss 2.5990445461707004\n",
      "129-th epoch train loss 2.7906033596068225\n",
      "129-th epoch val loss 2.539953153456724\n",
      "130-th epoch train loss 2.7312692215507073\n",
      "130-th epoch val loss 2.482813052958983\n",
      "131-th epoch train loss 2.6738449470831616\n",
      "131-th epoch val loss 2.42756188241652\n",
      "132-th epoch train loss 2.618270223374005\n",
      "132-th epoch val loss 2.3741392422272747\n",
      "133-th epoch train loss 2.56448662475662\n",
      "133-th epoch val loss 2.3224866341403745\n",
      "134-th epoch train loss 2.5124375539477675\n",
      "134-th epoch val loss 2.2725474018564156\n",
      "135-th epoch train loss 2.462068185094093\n",
      "135-th epoch val loss 2.224266673476446\n",
      "136-th epoch train loss 2.413325408588622\n",
      "136-th epoch val loss 2.177591305742232\n",
      "137-th epoch train loss 2.3661577776023064\n",
      "137-th epoch val loss 2.132469830012155\n",
      "138-th epoch train loss 2.3205154562773633\n",
      "138-th epoch val loss 2.0888523999188155\n",
      "139-th epoch train loss 2.2763501695308572\n",
      "139-th epoch val loss 2.046690740656119\n",
      "140-th epoch train loss 2.233615154418492\n",
      "140-th epoch val loss 2.0059380998451855\n",
      "141-th epoch train loss 2.1922651130102175\n",
      "141-th epoch val loss 1.966549199930058\n",
      "142-th epoch train loss 2.1522561667306803\n",
      "142-th epoch val loss 1.928480192055658\n",
      "143-th epoch train loss 2.1135458121190793\n",
      "143-th epoch val loss 1.891688611381949\n",
      "144-th epoch train loss 2.0760928779643213\n",
      "144-th epoch val loss 1.8561333337896686\n",
      "145-th epoch train loss 2.0398574837728143\n",
      "145-th epoch val loss 1.8217745339343956\n",
      "146-th epoch train loss 2.004800999527493\n",
      "146-th epoch val loss 1.788573644607049\n",
      "147-th epoch train loss 1.9708860066980074\n",
      "147-th epoch val loss 1.756493317360225\n",
      "148-th epoch train loss 1.9380762604632122\n",
      "148-th epoch val loss 1.725497384361025\n",
      "149-th epoch train loss 1.9063366531083257\n",
      "149-th epoch val loss 1.6955508214322672\n",
      "150-th epoch train loss 1.8756331785602878\n",
      "150-th epoch val loss 1.6666197122451447\n",
      "151-th epoch train loss 1.845932898025952\n",
      "151-th epoch val loss 1.6386712136275325\n",
      "152-th epoch train loss 1.817203906698908\n",
      "152-th epoch val loss 1.6116735219532967\n",
      "153-th epoch train loss 1.789415301501704\n",
      "153-th epoch val loss 1.5855958405789599\n",
      "154-th epoch train loss 1.7625371498313527\n",
      "154-th epoch val loss 1.5604083482952107\n",
      "155-th epoch train loss 1.736540459276943\n",
      "155-th epoch val loss 1.5360821687616815\n",
      "156-th epoch train loss 1.7113971482791877\n",
      "156-th epoch val loss 1.5125893408944422\n",
      "157-th epoch train loss 1.687080017702639\n",
      "157-th epoch val loss 1.4899027901765916\n",
      "158-th epoch train loss 1.6635627232922348\n",
      "158-th epoch val loss 1.4679963008632353\n",
      "159-th epoch train loss 1.6408197489867058\n",
      "159-th epoch val loss 1.4468444890530592\n",
      "160-th epoch train loss 1.6188263810622496\n",
      "160-th epoch val loss 1.426422776599549\n",
      "161-th epoch train loss 1.5975586830806563\n",
      "161-th epoch val loss 1.4067073658357436\n",
      "162-th epoch train loss 1.5769934716169185\n",
      "162-th epoch val loss 1.3876752150872254\n",
      "163-th epoch train loss 1.5571082927421196\n",
      "163-th epoch val loss 1.3693040149488516\n",
      "164-th epoch train loss 1.537881399238116\n",
      "164-th epoch val loss 1.3515721653014543\n",
      "165-th epoch train loss 1.5192917285213123\n",
      "165-th epoch val loss 1.334458753045514\n",
      "166-th epoch train loss 1.5013188812534812\n",
      "166-th epoch val loss 1.3179435305294989\n",
      "167-th epoch train loss 1.483943100618291\n",
      "167-th epoch val loss 1.3020068946512653\n",
      "168-th epoch train loss 1.46714525224287\n",
      "168-th epoch val loss 1.286629866611596\n",
      "169-th epoch train loss 1.4509068047443572\n",
      "169-th epoch val loss 1.2717940722995715\n",
      "170-th epoch train loss 1.4352098108820246\n",
      "170-th epoch val loss 1.2574817232901359\n",
      "171-th epoch train loss 1.4200368892961661\n",
      "171-th epoch val loss 1.2436755984348076\n",
      "172-th epoch train loss 1.4053712068155189\n",
      "172-th epoch val loss 1.230359026027087\n",
      "173-th epoch train loss 1.3911964613155414\n",
      "173-th epoch val loss 1.217515866524666\n",
      "174-th epoch train loss 1.377496865110455\n",
      "174-th epoch val loss 1.2051304958111395\n",
      "175-th epoch train loss 1.364257128862443\n",
      "175-th epoch val loss 1.1931877889804097\n",
      "176-th epoch train loss 1.351462445991947\n",
      "176-th epoch val loss 1.181673104627529\n",
      "177-th epoch train loss 1.3390984775734989\n",
      "177-th epoch val loss 1.1705722696302296\n",
      "178-th epoch train loss 1.3271513377019744\n",
      "178-th epoch val loss 1.1598715644058477\n",
      "179-th epoch train loss 1.3156075793146782\n",
      "179-th epoch val loss 1.1495577086288693\n",
      "180-th epoch train loss 1.3044541804550778\n",
      "180-th epoch val loss 1.1396178473947514\n",
      "181-th epoch train loss 1.293678530964463\n",
      "181-th epoch val loss 1.1300395378161228\n",
      "182-th epoch train loss 1.2832684195882336\n",
      "182-th epoch val loss 1.1208107360379165\n",
      "183-th epoch train loss 1.2732120214839282\n",
      "183-th epoch val loss 1.1119197846583828\n",
      "184-th epoch train loss 1.2634978861185073\n",
      "184-th epoch val loss 1.103355400543357\n",
      "185-th epoch train loss 1.2541149255427875\n",
      "185-th epoch val loss 1.0951066630215245\n",
      "186-th epoch train loss 1.245052403031311\n",
      "186-th epoch val loss 1.0871630024488366\n",
      "187-th epoch train loss 1.2362999220762783\n",
      "187-th epoch val loss 1.0795141891305668\n",
      "188-th epoch train loss 1.2278474157245571\n",
      "188-th epoch val loss 1.0721503225898856\n",
      "189-th epoch train loss 1.2196851362470795\n",
      "189-th epoch val loss 1.0650618211721499\n",
      "190-th epoch train loss 1.2118036451303071\n",
      "190-th epoch val loss 1.0582394119744578\n",
      "191-th epoch train loss 1.2041938033797512\n",
      "191-th epoch val loss 1.0516741210903382\n",
      "192-th epoch train loss 1.196846762125835\n",
      "192-th epoch val loss 1.045357264159753\n",
      "193-th epoch train loss 1.1897539535227104\n",
      "193-th epoch val loss 1.0392804372149091\n",
      "194-th epoch train loss 1.18290708193091\n",
      "194-th epoch val loss 1.0334355078126618\n",
      "195-th epoch train loss 1.1762981153750143\n",
      "195-th epoch val loss 1.027814606444577\n",
      "196-th epoch train loss 1.169919277267774\n",
      "196-th epoch val loss 1.022410118216003\n",
      "197-th epoch train loss 1.163763038392409\n",
      "197-th epoch val loss 1.0172146747857702\n",
      "198-th epoch train loss 1.1578221091350531\n",
      "198-th epoch val loss 1.0122211465584012\n",
      "199-th epoch train loss 1.1520894319595594\n",
      "199-th epoch val loss 1.0074226351209492\n",
      "200-th epoch train loss 1.1465581741171318\n",
      "200-th epoch val loss 1.0028124659168518\n",
      "201-th epoch train loss 1.1412217205834756\n",
      "201-th epoch val loss 0.9983841811494043\n",
      "202-th epoch train loss 1.1360736672163978\n",
      "202-th epoch val loss 0.9941315329077061\n",
      "203-th epoch train loss 1.1311078141269855\n",
      "203-th epoch val loss 0.9900484765081248\n",
      "204-th epoch train loss 1.1263181592577307\n",
      "204-th epoch val loss 0.9861291640445777\n",
      "205-th epoch train loss 1.121698892161156\n",
      "205-th epoch val loss 0.9823679381411101\n",
      "206-th epoch train loss 1.1172443879727016\n",
      "206-th epoch val loss 0.9787593259004576\n",
      "207-th epoch train loss 1.112949201571839\n",
      "207-th epoch val loss 0.9752980330424942\n",
      "208-th epoch train loss 1.1088080619255396\n",
      "208-th epoch val loss 0.9719789382266288\n",
      "209-th epoch train loss 1.104815866608438\n",
      "209-th epoch val loss 0.9687970875524196\n",
      "210-th epoch train loss 1.1009676764941776\n",
      "210-th epoch val loss 0.9657476892328434\n",
      "211-th epoch train loss 1.0972587106126235\n",
      "211-th epoch val loss 0.9628261084348413\n",
      "212-th epoch train loss 1.0936843411677708\n",
      "212-th epoch val loss 0.9600278622819061\n",
      "213-th epoch train loss 1.090240088711353\n",
      "213-th epoch val loss 0.9573486150136701\n",
      "214-th epoch train loss 1.086921617467293\n",
      "214-th epoch val loss 0.9547841732975826\n",
      "215-th epoch train loss 1.0837247308023215\n",
      "215-th epoch val loss 0.9523304816879377\n",
      "216-th epoch train loss 1.0806453668381844\n",
      "216-th epoch val loss 0.9499836182276457\n",
      "217-th epoch train loss 1.0776795942010602\n",
      "217-th epoch val loss 0.9477397901882992\n",
      "218-th epoch train loss 1.0748236079038886\n",
      "218-th epoch val loss 0.9455953299442079\n",
      "219-th epoch train loss 1.0720737253574908\n",
      "219-th epoch val loss 0.9435466909762255\n",
      "220-th epoch train loss 1.0694263825064627\n",
      "220-th epoch val loss 0.9415904440013125\n",
      "221-th epoch train loss 1.066878130085955\n",
      "221-th epoch val loss 0.9397232732239098\n",
      "222-th epoch train loss 1.0644256299955792\n",
      "222-th epoch val loss 0.9379419727053105\n",
      "223-th epoch train loss 1.0620656517867835\n",
      "223-th epoch val loss 0.9362434428473565\n",
      "224-th epoch train loss 1.059795069260176\n",
      "224-th epoch val loss 0.9346246869868775\n",
      "225-th epoch train loss 1.0576108571693577\n",
      "225-th epoch val loss 0.9330828080974134\n",
      "226-th epoch train loss 1.0555100880279564\n",
      "226-th epoch val loss 0.9316150055948722\n",
      "227-th epoch train loss 1.0534899290166406\n",
      "227-th epoch val loss 0.9302185722438715\n",
      "228-th epoch train loss 1.051547638987\n",
      "228-th epoch val loss 0.9288908911616119\n",
      "229-th epoch train loss 1.0496805655592756\n",
      "229-th epoch val loss 0.9276294329162447\n",
      "230-th epoch train loss 1.0478861423110117\n",
      "230-th epoch val loss 0.926431752716766\n",
      "231-th epoch train loss 1.0461618860537978\n",
      "231-th epoch val loss 0.9252954876915818\n",
      "232-th epoch train loss 1.044505394195354\n",
      "232-th epoch val loss 0.9242183542529732\n",
      "233-th epoch train loss 1.0429143421843028\n",
      "233-th epoch val loss 0.923198145544767\n",
      "234-th epoch train loss 1.0413864810350422\n",
      "234-th epoch val loss 0.922232728970615\n",
      "235-th epoch train loss 1.0399196349302318\n",
      "235-th epoch val loss 0.921320043800359\n",
      "236-th epoch train loss 1.038511698898464\n",
      "236-th epoch val loss 0.9204580988520299\n",
      "237-th epoch train loss 1.0371606365647827\n",
      "237-th epoch val loss 0.9196449702471279\n",
      "238-th epoch train loss 1.0358644779717718\n",
      "238-th epoch val loss 0.918878799236879\n",
      "239-th epoch train loss 1.0346213174690155\n",
      "239-th epoch val loss 0.9181577900972425\n",
      "240-th epoch train loss 1.0334293116688005\n",
      "240-th epoch val loss 0.9174802080905333\n",
      "241-th epoch train loss 1.0322866774659845\n",
      "241-th epoch val loss 0.9168443774915527\n",
      "242-th epoch train loss 1.0311916901200417\n",
      "242-th epoch val loss 0.9162486796762199\n",
      "243-th epoch train loss 1.0301426813973356\n",
      "243-th epoch val loss 0.9156915512707433\n",
      "244-th epoch train loss 1.0291380377717434\n",
      "244-th epoch val loss 0.91517148235943\n",
      "245-th epoch train loss 1.0281761986818108\n",
      "245-th epoch val loss 0.9146870147493049\n",
      "246-th epoch train loss 1.0272556548426748\n",
      "246-th epoch val loss 0.9142367402897469\n",
      "247-th epoch train loss 1.0263749466110401\n",
      "247-th epoch val loss 0.913819299245428\n",
      "248-th epoch train loss 1.0255326624015595\n",
      "248-th epoch val loss 0.9134333787208788\n",
      "249-th epoch train loss 1.0247274371530053\n",
      "249-th epoch val loss 0.9130777111350604\n",
      "250-th epoch train loss 1.0239579508426853\n",
      "250-th epoch val loss 0.9127510727443771\n",
      "251-th epoch train loss 1.0232229270475937\n",
      "251-th epoch val loss 0.9124522822126088\n",
      "252-th epoch train loss 1.0225211315508327\n",
      "252-th epoch val loss 0.9121801992262882\n",
      "253-th epoch train loss 1.0218513709918995\n",
      "253-th epoch val loss 0.9119337231541031\n",
      "254-th epoch train loss 1.0212124915594625\n",
      "254-th epoch val loss 0.9117117917489356\n",
      "255-th epoch train loss 1.0206033777252985\n",
      "255-th epoch val loss 0.9115133798911982\n",
      "256-th epoch train loss 1.0200229510181085\n",
      "256-th epoch val loss 0.9113374983721781\n",
      "257-th epoch train loss 1.0194701688359675\n",
      "257-th epoch val loss 0.9111831927161265\n",
      "258-th epoch train loss 1.0189440232961922\n",
      "258-th epoch val loss 0.9110495420398745\n",
      "259-th epoch train loss 1.018443540121466\n",
      "259-th epoch val loss 0.9109356579488018\n",
      "260-th epoch train loss 1.0179677775610847\n",
      "260-th epoch val loss 0.9108406834680125\n",
      "261-th epoch train loss 1.0175158253462224\n",
      "261-th epoch val loss 0.9107637920076084\n",
      "262-th epoch train loss 1.0170868036781582\n",
      "262-th epoch val loss 0.9107041863609946\n",
      "263-th epoch train loss 1.0166798622484283\n",
      "263-th epoch val loss 0.91066109773517\n",
      "264-th epoch train loss 1.0162941792899114\n",
      "264-th epoch val loss 0.9106337848120059\n",
      "265-th epoch train loss 1.0159289606578699\n",
      "265-th epoch val loss 0.910621532839527\n",
      "266-th epoch train loss 1.0155834389400205\n",
      "266-th epoch val loss 0.9106236527522585\n",
      "267-th epoch train loss 1.015256872594717\n",
      "267-th epoch val loss 0.9106394803197234\n",
      "268-th epoch train loss 1.014948545116374\n",
      "268-th epoch val loss 0.9106683753221975\n",
      "269-th epoch train loss 1.0146577642272663\n",
      "269-th epoch val loss 0.9107097207528715\n",
      "270-th epoch train loss 1.014383861094895\n",
      "270-th epoch val loss 0.9107629220455821\n",
      "271-th epoch train loss 1.0141261895741036\n",
      "271-th epoch val loss 0.91082740632731\n",
      "272-th epoch train loss 1.0138841254731803\n",
      "272-th epoch val loss 0.9109026216946613\n",
      "273-th epoch train loss 1.0136570658431912\n",
      "273-th epoch val loss 0.9109880365135801\n",
      "274-th epoch train loss 1.0134444282898132\n",
      "274-th epoch val loss 0.9110831387415527\n",
      "275-th epoch train loss 1.0132456503069684\n",
      "275-th epoch val loss 0.9111874352716018\n",
      "276-th epoch train loss 1.0130601886315742\n",
      "276-th epoch val loss 0.9113004512973784\n",
      "277-th epoch train loss 1.0128875186187436\n",
      "277-th epoch val loss 0.9114217296986838\n",
      "278-th epoch train loss 1.0127271336367982\n",
      "278-th epoch val loss 0.9115508304467834\n",
      "279-th epoch train loss 1.012578544481476\n",
      "279-th epoch val loss 0.9116873300288784\n",
      "280-th epoch train loss 1.0124412788087238\n",
      "280-th epoch val loss 0.9118308208911362\n",
      "281-th epoch train loss 1.0123148805854987\n",
      "281-th epoch val loss 0.9119809108996949\n",
      "282-th epoch train loss 1.0121989095580082\n",
      "282-th epoch val loss 0.9121372228190626\n",
      "283-th epoch train loss 1.0120929407368486\n",
      "283-th epoch val loss 0.9122993938073743\n",
      "284-th epoch train loss 1.0119965638985025\n",
      "284-th epoch val loss 0.912467074927967\n",
      "285-th epoch train loss 1.0119093831026909\n",
      "285-th epoch val loss 0.9126399306767548\n",
      "286-th epoch train loss 1.011831016225074\n",
      "286-th epoch val loss 0.9128176385249075\n",
      "287-th epoch train loss 1.011761094504827\n",
      "287-th epoch val loss 0.9129998884763485\n",
      "288-th epoch train loss 1.0116992621066152\n",
      "288-th epoch val loss 0.9131863826395971\n",
      "289-th epoch train loss 1.0116451756965241\n",
      "289-th epoch val loss 0.9133768348135083\n",
      "290-th epoch train loss 1.0115985040314983\n",
      "290-th epoch val loss 0.9135709700864616\n",
      "291-th epoch train loss 1.0115589275618715\n",
      "291-th epoch val loss 0.9137685244485799\n",
      "292-th epoch train loss 1.011526138046574\n",
      "292-th epoch val loss 0.9139692444165604\n",
      "293-th epoch train loss 1.0114998381806144\n",
      "293-th epoch val loss 0.9141728866707157\n",
      "294-th epoch train loss 1.0114797412344554\n",
      "294-th epoch val loss 0.9143792177038449\n",
      "295-th epoch train loss 1.0114655707049114\n",
      "295-th epoch val loss 0.9145880134815498\n",
      "296-th epoch train loss 1.0114570599771924\n",
      "296-th epoch val loss 0.9147990591136397\n",
      "297-th epoch train loss 1.011453951997764\n",
      "297-th epoch val loss 0.9150121485362677\n",
      "298-th epoch train loss 1.0114559989576617\n",
      "298-th epoch val loss 0.9152270842044619\n",
      "299-th epoch train loss 1.011462961985946\n",
      "299-th epoch val loss 0.9154436767947126\n",
      "300-th epoch train loss 1.0114746108529737\n",
      "300-th epoch val loss 0.91566174491731\n",
      "301-th epoch train loss 1.0114907236831687\n",
      "301-th epoch val loss 0.915881114838101\n",
      "302-th epoch train loss 1.0115110866770096\n",
      "302-th epoch val loss 0.9161016202093875\n",
      "303-th epoch train loss 1.0115354938419279\n",
      "303-th epoch val loss 0.9163231018096595\n",
      "304-th epoch train loss 1.0115637467318435\n",
      "304-th epoch val loss 0.9165454072918842\n",
      "305-th epoch train loss 1.0115956541950626\n",
      "305-th epoch val loss 0.9167683909400857\n",
      "306-th epoch train loss 1.011631032130278\n",
      "306-th epoch val loss 0.9169919134339388\n",
      "307-th epoch train loss 1.011669703250411\n",
      "307-th epoch val loss 0.9172158416211332\n",
      "308-th epoch train loss 1.01171149685405\n",
      "308-th epoch val loss 0.9174400482972498\n",
      "309-th epoch train loss 1.0117562486042468\n",
      "309-th epoch val loss 0.9176644119929157\n",
      "310-th epoch train loss 1.0118038003144378\n",
      "310-th epoch val loss 0.9178888167680022\n",
      "311-th epoch train loss 1.0118539997412612\n",
      "311-th epoch val loss 0.9181131520126383\n",
      "312-th epoch train loss 1.0119067003840598\n",
      "312-th epoch val loss 0.9183373122548302\n",
      "313-th epoch train loss 1.0119617612908496\n",
      "313-th epoch val loss 0.9185611969744638\n",
      "314-th epoch train loss 1.0120190468705605\n",
      "314-th epoch val loss 0.918784710423495\n",
      "315-th epoch train loss 1.0120784267113352\n",
      "315-th epoch val loss 0.9190077614521207\n",
      "316-th epoch train loss 1.012139775404711\n",
      "316-th epoch val loss 0.919230263340752\n",
      "317-th epoch train loss 1.0122029723754897\n",
      "317-th epoch val loss 0.9194521336375846\n",
      "318-th epoch train loss 1.012267901717113\n",
      "318-th epoch val loss 0.9196732940016055\n",
      "319-th epoch train loss 1.0123344520323774\n",
      "319-th epoch val loss 0.9198936700508488\n",
      "320-th epoch train loss 1.0124025162793124\n",
      "320-th epoch val loss 0.9201131912157373\n",
      "321-th epoch train loss 1.0124719916220617\n",
      "321-th epoch val loss 0.9203317905973437\n",
      "322-th epoch train loss 1.012542779286608\n",
      "322-th epoch val loss 0.9205494048304131\n",
      "323-th epoch train loss 1.0126147844211861\n",
      "323-th epoch val loss 0.9207659739510009\n",
      "324-th epoch train loss 1.0126879159612407\n",
      "324-th epoch val loss 0.9209814412685603\n",
      "325-th epoch train loss 1.0127620864987776\n",
      "325-th epoch val loss 0.9211957532423567\n",
      "326-th epoch train loss 1.0128372121559737\n",
      "326-th epoch val loss 0.921408859362054\n",
      "327-th epoch train loss 1.0129132124629094\n",
      "327-th epoch val loss 0.9216207120323431\n",
      "328-th epoch train loss 1.0129900102392928\n",
      "328-th epoch val loss 0.9218312664614835\n",
      "329-th epoch train loss 1.0130675314800468\n",
      "329-th epoch val loss 0.9220404805536323\n",
      "330-th epoch train loss 1.013145705244637\n",
      "330-th epoch val loss 0.9222483148048289\n",
      "331-th epoch train loss 1.0132244635500254\n",
      "331-th epoch val loss 0.9224547322025304\n",
      "332-th epoch train loss 1.0133037412671269\n",
      "332-th epoch val loss 0.9226596981285757\n",
      "333-th epoch train loss 1.0133834760206637\n",
      "333-th epoch val loss 0.922863180265461\n",
      "334-th epoch train loss 1.0134636080923085\n",
      "334-th epoch val loss 0.9230651485058358\n",
      "335-th epoch train loss 1.0135440803270048\n",
      "335-th epoch val loss 0.923265574865094\n",
      "336-th epoch train loss 1.0136248380423751\n",
      "336-th epoch val loss 0.9234644333969798\n",
      "337-th epoch train loss 1.0137058289411036\n",
      "337-th epoch val loss 0.9236617001120914\n",
      "338-th epoch train loss 1.0137870030262155\n",
      "338-th epoch val loss 0.9238573528992061\n",
      "339-th epoch train loss 1.013868312519145\n",
      "339-th epoch val loss 0.9240513714493191\n",
      "340-th epoch train loss 1.0139497117805123\n",
      "340-th epoch val loss 0.9242437371823203\n",
      "341-th epoch train loss 1.014031157233517\n",
      "341-th epoch val loss 0.9244344331762108\n",
      "342-th epoch train loss 1.0141126072898734\n",
      "342-th epoch val loss 0.9246234440987879\n",
      "343-th epoch train loss 1.0141940222781927\n",
      "343-th epoch val loss 0.9248107561417103\n",
      "344-th epoch train loss 1.0142753643747466\n",
      "344-th epoch val loss 0.9249963569568664\n",
      "345-th epoch train loss 1.0143565975365303\n",
      "345-th epoch val loss 0.9251802355949748\n",
      "346-th epoch train loss 1.0144376874365497\n",
      "346-th epoch val loss 0.925362382446336\n",
      "347-th epoch train loss 1.014518601401264\n",
      "347-th epoch val loss 0.9255427891836748\n",
      "348-th epoch train loss 1.0145993083501148\n",
      "348-th epoch val loss 0.9257214487069925\n",
      "349-th epoch train loss 1.0146797787370743\n",
      "349-th epoch val loss 0.9258983550903745\n",
      "350-th epoch train loss 1.014759984494146\n",
      "350-th epoch val loss 0.9260735035306829\n",
      "351-th epoch train loss 1.0148398989767589\n",
      "351-th epoch val loss 0.92624689029807\n",
      "352-th epoch train loss 1.014919496910987\n",
      "352-th epoch val loss 0.9264185126882585\n",
      "353-th epoch train loss 1.0149987543425483\n",
      "353-th epoch val loss 0.9265883689765252\n",
      "354-th epoch train loss 1.0150776485875086\n",
      "354-th epoch val loss 0.9267564583733336\n",
      "355-th epoch train loss 1.0151561581846542\n",
      "355-th epoch val loss 0.9269227809815641\n",
      "356-th epoch train loss 1.0152342628494615\n",
      "356-th epoch val loss 0.9270873377552786\n",
      "357-th epoch train loss 1.0153119434296278\n",
      "357-th epoch val loss 0.9272501304599834\n",
      "358-th epoch train loss 1.0153891818620997\n",
      "358-th epoch val loss 0.9274111616343276\n",
      "359-th epoch train loss 1.0154659611315608\n",
      "359-th epoch val loss 0.9275704345531943\n",
      "360-th epoch train loss 1.0155422652303256\n",
      "360-th epoch val loss 0.9277279531921434\n",
      "361-th epoch train loss 1.0156180791195974\n",
      "361-th epoch val loss 0.9278837221931489\n",
      "362-th epoch train loss 1.015693388692043\n",
      "362-th epoch val loss 0.9280377468316005\n",
      "363-th epoch train loss 1.015768180735644\n",
      "363-th epoch val loss 0.9281900329845162\n",
      "364-th epoch train loss 1.015842442898782\n",
      "364-th epoch val loss 0.9283405870999342\n",
      "365-th epoch train loss 1.0159161636565184\n",
      "365-th epoch val loss 0.9284894161674371\n",
      "366-th epoch train loss 1.0159893322780316\n",
      "366-th epoch val loss 0.9286365276897767\n",
      "367-th epoch train loss 1.016061938795168\n",
      "367-th epoch val loss 0.9287819296555602\n",
      "368-th epoch train loss 1.0161339739720778\n",
      "368-th epoch val loss 0.9289256305129558\n",
      "369-th epoch train loss 1.0162054292758982\n",
      "369-th epoch val loss 0.929067639144397\n",
      "370-th epoch train loss 1.0162762968484438\n",
      "370-th epoch val loss 0.9292079648422376\n",
      "371-th epoch train loss 1.016346569478885\n",
      "371-th epoch val loss 0.9293466172853354\n",
      "372-th epoch train loss 1.016416240577363\n",
      "372-th epoch val loss 0.9294836065165265\n",
      "373-th epoch train loss 1.0164853041495312\n",
      "373-th epoch val loss 0.9296189429209643\n",
      "374-th epoch train loss 1.0165537547719758\n",
      "374-th epoch val loss 0.9297526372052912\n",
      "375-th epoch train loss 1.0166215875685005\n",
      "375-th epoch val loss 0.9298847003776175\n",
      "376-th epoch train loss 1.016688798187233\n",
      "376-th epoch val loss 0.9300151437282763\n",
      "377-th epoch train loss 1.01675538277854\n",
      "377-th epoch val loss 0.9301439788113295\n",
      "378-th epoch train loss 1.0168213379737163\n",
      "378-th epoch val loss 0.9302712174268017\n",
      "379-th epoch train loss 1.0168866608644227\n",
      "379-th epoch val loss 0.9303968716036144\n",
      "380-th epoch train loss 1.016951348982853\n",
      "380-th epoch val loss 0.9305209535831926\n",
      "381-th epoch train loss 1.0170154002825968\n",
      "381-th epoch val loss 0.930643475803731\n",
      "382-th epoch train loss 1.0170788131201902\n",
      "382-th epoch val loss 0.9307644508850871\n",
      "383-th epoch train loss 1.0171415862373134\n",
      "383-th epoch val loss 0.9308838916142841\n",
      "384-th epoch train loss 1.0172037187436307\n",
      "384-th epoch val loss 0.9310018109316034\n",
      "385-th epoch train loss 1.0172652101002377\n",
      "385-th epoch val loss 0.931118221917242\n",
      "386-th epoch train loss 1.0173260601037086\n",
      "386-th epoch val loss 0.9312331377785197\n",
      "387-th epoch train loss 1.0173862688707114\n",
      "387-th epoch val loss 0.9313465718376152\n",
      "388-th epoch train loss 1.0174458368231813\n",
      "388-th epoch val loss 0.9314585375198103\n",
      "389-th epoch train loss 1.0175047646740294\n",
      "389-th epoch val loss 0.9315690483422298\n",
      "390-th epoch train loss 1.0175630534133702\n",
      "390-th epoch val loss 0.9316781179030539\n",
      "391-th epoch train loss 1.0176207042952496\n",
      "391-th epoch val loss 0.9317857598711912\n",
      "392-th epoch train loss 1.017677718824857\n",
      "392-th epoch val loss 0.9318919879763891\n",
      "393-th epoch train loss 1.0177340987462107\n",
      "393-th epoch val loss 0.9319968159997826\n",
      "394-th epoch train loss 1.0177898460302872\n",
      "394-th epoch val loss 0.9321002577648375\n",
      "395-th epoch train loss 1.0178449628635966\n",
      "395-th epoch val loss 0.9322023271287063\n",
      "396-th epoch train loss 1.0178994516371758\n",
      "396-th epoch val loss 0.9323030379739595\n",
      "397-th epoch train loss 1.017953314935992\n",
      "397-th epoch val loss 0.9324024042006849\n",
      "398-th epoch train loss 1.0180065555287425\n",
      "398-th epoch val loss 0.9325004397189477\n",
      "399-th epoch train loss 1.0180591763580342\n",
      "399-th epoch val loss 0.9325971584415896\n",
      "400-th epoch train loss 1.0181111805309322\n",
      "400-th epoch val loss 0.932692574277362\n",
      "401-th epoch train loss 1.0181625713098665\n",
      "401-th epoch val loss 0.9327867011243752\n",
      "402-th epoch train loss 1.0182133521038819\n",
      "402-th epoch val loss 0.9328795528638577\n",
      "403-th epoch train loss 1.0182635264602202\n",
      "403-th epoch val loss 0.9329711433542089\n",
      "404-th epoch train loss 1.0183130980562252\n",
      "404-th epoch val loss 0.9330614864253405\n",
      "405-th epoch train loss 1.0183620706915575\n",
      "405-th epoch val loss 0.9331505958732893\n",
      "406-th epoch train loss 1.0184104482807075\n",
      "406-th epoch val loss 0.9332384854550978\n",
      "407-th epoch train loss 1.0184582348458036\n",
      "407-th epoch val loss 0.9333251688839502\n",
      "408-th epoch train loss 1.0185054345096938\n",
      "408-th epoch val loss 0.9334106598245502\n",
      "409-th epoch train loss 1.0185520514893007\n",
      "409-th epoch val loss 0.9334949718887396\n",
      "410-th epoch train loss 1.0185980900892389\n",
      "410-th epoch val loss 0.9335781186313437\n",
      "411-th epoch train loss 1.018643554695682\n",
      "411-th epoch val loss 0.9336601135462355\n",
      "412-th epoch train loss 1.018688449770474\n",
      "412-th epoch val loss 0.9337409700626097\n",
      "413-th epoch train loss 1.0187327798454766\n",
      "413-th epoch val loss 0.9338207015414643\n",
      "414-th epoch train loss 1.0187765495171417\n",
      "414-th epoch val loss 0.9338993212722724\n",
      "415-th epoch train loss 1.018819763441305\n",
      "415-th epoch val loss 0.9339768424698452\n",
      "416-th epoch train loss 1.0188624263281894\n",
      "416-th epoch val loss 0.9340532782713767\n",
      "417-th epoch train loss 1.0189045429376133\n",
      "417-th epoch val loss 0.9341286417336587\n",
      "418-th epoch train loss 1.018946118074397\n",
      "418-th epoch val loss 0.9342029458304668\n",
      "419-th epoch train loss 1.0189871565839568\n",
      "419-th epoch val loss 0.9342762034501063\n",
      "420-th epoch train loss 1.019027663348087\n",
      "420-th epoch val loss 0.9343484273931137\n",
      "421-th epoch train loss 1.0190676432809125\n",
      "421-th epoch val loss 0.9344196303701016\n",
      "422-th epoch train loss 1.0191071013250188\n",
      "422-th epoch val loss 0.934489824999754\n",
      "423-th epoch train loss 1.0191460424477432\n",
      "423-th epoch val loss 0.9345590238069553\n",
      "424-th epoch train loss 1.0191844716376275\n",
      "424-th epoch val loss 0.9346272392210488\n",
      "425-th epoch train loss 1.0192223939010197\n",
      "425-th epoch val loss 0.9346944835742255\n",
      "426-th epoch train loss 1.0192598142588314\n",
      "426-th epoch val loss 0.9347607691000338\n",
      "427-th epoch train loss 1.019296737743428\n",
      "427-th epoch val loss 0.9348261079320019\n",
      "428-th epoch train loss 1.019333169395668\n",
      "428-th epoch val loss 0.9348905121023777\n",
      "429-th epoch train loss 1.0193691142620631\n",
      "429-th epoch val loss 0.9349539935409727\n",
      "430-th epoch train loss 1.0194045773920788\n",
      "430-th epoch val loss 0.9350165640741099\n",
      "431-th epoch train loss 1.019439563835546\n",
      "431-th epoch val loss 0.9350782354236693\n",
      "432-th epoch train loss 1.0194740786402015\n",
      "432-th epoch val loss 0.9351390192062289\n",
      "433-th epoch train loss 1.0195081268493373\n",
      "433-th epoch val loss 0.9351989269322964\n",
      "434-th epoch train loss 1.0195417134995635\n",
      "434-th epoch val loss 0.9352579700056274\n",
      "435-th epoch train loss 1.019574843618676\n",
      "435-th epoch val loss 0.9353161597226279\n",
      "436-th epoch train loss 1.019607522223629\n",
      "436-th epoch val loss 0.935373507271835\n",
      "437-th epoch train loss 1.019639754318602\n",
      "437-th epoch val loss 0.9354300237334721\n",
      "438-th epoch train loss 1.019671544893167\n",
      "438-th epoch val loss 0.9354857200790827\n",
      "439-th epoch train loss 1.0197028989205457\n",
      "439-th epoch val loss 0.9355406071712281\n",
      "440-th epoch train loss 1.0197338213559524\n",
      "440-th epoch val loss 0.9355946957632553\n",
      "441-th epoch train loss 1.019764317135028\n",
      "441-th epoch val loss 0.9356479964991257\n",
      "442-th epoch train loss 1.0197943911723475\n",
      "442-th epoch val loss 0.9357005199133072\n",
      "443-th epoch train loss 1.019824048360016\n",
      "443-th epoch val loss 0.9357522764307243\n",
      "444-th epoch train loss 1.0198532935663336\n",
      "444-th epoch val loss 0.9358032763667616\n",
      "445-th epoch train loss 1.0198821316345317\n",
      "445-th epoch val loss 0.9358535299273211\n",
      "446-th epoch train loss 1.0199105673815902\n",
      "446-th epoch val loss 0.9359030472089289\n",
      "447-th epoch train loss 1.0199386055971118\n",
      "447-th epoch val loss 0.935951838198895\n",
      "448-th epoch train loss 1.0199662510422631\n",
      "448-th epoch val loss 0.9359999127755112\n",
      "449-th epoch train loss 1.0199935084487848\n",
      "449-th epoch val loss 0.9360472807082976\n",
      "450-th epoch train loss 1.0200203825180534\n",
      "450-th epoch val loss 0.9360939516582882\n",
      "451-th epoch train loss 1.0200468779202063\n",
      "451-th epoch val loss 0.936139935178359\n",
      "452-th epoch train loss 1.0200729992933215\n",
      "452-th epoch val loss 0.9361852407135879\n",
      "453-th epoch train loss 1.020098751242648\n",
      "453-th epoch val loss 0.9362298776016551\n",
      "454-th epoch train loss 1.0201241383398931\n",
      "454-th epoch val loss 0.9362738550732773\n",
      "455-th epoch train loss 1.020149165122551\n",
      "455-th epoch val loss 0.9363171822526672\n",
      "456-th epoch train loss 1.0201738360932868\n",
      "456-th epoch val loss 0.9363598681580313\n",
      "457-th epoch train loss 1.0201981557193618\n",
      "457-th epoch val loss 0.9364019217020926\n",
      "458-th epoch train loss 1.0202221284321016\n",
      "458-th epoch val loss 0.9364433516926387\n",
      "459-th epoch train loss 1.0202457586264093\n",
      "459-th epoch val loss 0.9364841668330997\n",
      "460-th epoch train loss 1.0202690506603165\n",
      "460-th epoch val loss 0.9365243757231445\n",
      "461-th epoch train loss 1.0202920088545717\n",
      "461-th epoch val loss 0.9365639868593046\n",
      "462-th epoch train loss 1.0203146374922718\n",
      "462-th epoch val loss 0.9366030086356181\n",
      "463-th epoch train loss 1.0203369408185194\n",
      "463-th epoch val loss 0.9366414493442885\n",
      "464-th epoch train loss 1.0203589230401235\n",
      "464-th epoch val loss 0.9366793171763724\n",
      "465-th epoch train loss 1.0203805883253276\n",
      "465-th epoch val loss 0.9367166202224759\n",
      "466-th epoch train loss 1.0204019408035683\n",
      "466-th epoch val loss 0.9367533664734711\n",
      "467-th epoch train loss 1.0204229845652684\n",
      "467-th epoch val loss 0.9367895638212261\n",
      "468-th epoch train loss 1.0204437236616528\n",
      "468-th epoch val loss 0.9368252200593505\n",
      "469-th epoch train loss 1.0204641621045953\n",
      "469-th epoch val loss 0.9368603428839551\n",
      "470-th epoch train loss 1.020484303866494\n",
      "470-th epoch val loss 0.9368949398944231\n",
      "471-th epoch train loss 1.020504152880164\n",
      "471-th epoch val loss 0.9369290185941912\n",
      "472-th epoch train loss 1.0205237130387614\n",
      "472-th epoch val loss 0.9369625863915401\n",
      "473-th epoch train loss 1.0205429881957262\n",
      "473-th epoch val loss 0.9369956506003997\n",
      "474-th epoch train loss 1.02056198216475\n",
      "474-th epoch val loss 0.9370282184411592\n",
      "475-th epoch train loss 1.0205806987197603\n",
      "475-th epoch val loss 0.9370602970414844\n",
      "476-th epoch train loss 1.0205991415949276\n",
      "476-th epoch val loss 0.9370918934371423\n",
      "477-th epoch train loss 1.0206173144846928\n",
      "477-th epoch val loss 0.9371230145728376\n",
      "478-th epoch train loss 1.0206352210438099\n",
      "478-th epoch val loss 0.9371536673030477\n",
      "479-th epoch train loss 1.0206528648874047\n",
      "479-th epoch val loss 0.9371838583928637\n",
      "480-th epoch train loss 1.0206702495910556\n",
      "480-th epoch val loss 0.9372135945188417\n",
      "481-th epoch train loss 1.020687378690883\n",
      "481-th epoch val loss 0.9372428822698522\n",
      "482-th epoch train loss 1.0207042556836607\n",
      "482-th epoch val loss 0.9372717281479334\n",
      "483-th epoch train loss 1.0207208840269344\n",
      "483-th epoch val loss 0.9373001385691496\n",
      "484-th epoch train loss 1.0207372671391564\n",
      "484-th epoch val loss 0.9373281198644491\n",
      "485-th epoch train loss 1.0207534083998389\n",
      "485-th epoch val loss 0.9373556782805273\n",
      "486-th epoch train loss 1.0207693111497063\n",
      "486-th epoch val loss 0.9373828199806843\n",
      "487-th epoch train loss 1.020784978690872\n",
      "487-th epoch val loss 0.937409551045691\n",
      "488-th epoch train loss 1.0208004142870182\n",
      "488-th epoch val loss 0.9374358774746503\n",
      "489-th epoch train loss 1.0208156211635884\n",
      "489-th epoch val loss 0.937461805185859\n",
      "490-th epoch train loss 1.020830602507991\n",
      "490-th epoch val loss 0.9374873400176705\n",
      "491-th epoch train loss 1.020845361469808\n",
      "491-th epoch val loss 0.9375124877293552\n",
      "492-th epoch train loss 1.0208599011610155\n",
      "492-th epoch val loss 0.9375372540019583\n",
      "493-th epoch train loss 1.020874224656212\n",
      "493-th epoch val loss 0.9375616444391591\n",
      "494-th epoch train loss 1.020888334992854\n",
      "494-th epoch val loss 0.9375856645681265\n",
      "495-th epoch train loss 1.0209022351714958\n",
      "495-th epoch val loss 0.9376093198403701\n",
      "496-th epoch train loss 1.0209159281560425\n",
      "496-th epoch val loss 0.9376326156325925\n",
      "497-th epoch train loss 1.0209294168740035\n",
      "497-th epoch val loss 0.9376555572475361\n",
      "498-th epoch train loss 1.020942704216753\n",
      "498-th epoch val loss 0.9376781499148256\n",
      "499-th epoch train loss 1.020955793039798\n",
      "499-th epoch val loss 0.9377003987918104\n",
      "500-th epoch train loss 1.0209686861630498\n",
      "500-th epoch val loss 0.9377223089644009\n",
      "501-th epoch train loss 1.0209813863710997\n",
      "501-th epoch val loss 0.937743885447901\n",
      "502-th epoch train loss 1.020993896413501\n",
      "502-th epoch val loss 0.9377651331878362\n",
      "503-th epoch train loss 1.021006219005054\n",
      "503-th epoch val loss 0.9377860570607787\n",
      "504-th epoch train loss 1.0210183568260913\n",
      "504-th epoch val loss 0.9378066618751647\n",
      "505-th epoch train loss 1.0210303125227753\n",
      "505-th epoch val loss 0.9378269523721123\n",
      "506-th epoch train loss 1.021042088707388\n",
      "506-th epoch val loss 0.9378469332262291\n",
      "507-th epoch train loss 1.0210536879586325\n",
      "507-th epoch val loss 0.9378666090464164\n",
      "508-th epoch train loss 1.0210651128219301\n",
      "508-th epoch val loss 0.9378859843766693\n",
      "509-th epoch train loss 1.021076365809726\n",
      "509-th epoch val loss 0.9379050636968707\n",
      "510-th epoch train loss 1.0210874494017919\n",
      "510-th epoch val loss 0.9379238514235789\n",
      "511-th epoch train loss 1.0210983660455335\n",
      "511-th epoch val loss 0.9379423519108105\n",
      "512-th epoch train loss 1.0211091181562972\n",
      "512-th epoch val loss 0.9379605694508172\n",
      "513-th epoch train loss 1.0211197081176804\n",
      "513-th epoch val loss 0.9379785082748554\n",
      "514-th epoch train loss 1.0211301382818414\n",
      "514-th epoch val loss 0.9379961725539526\n",
      "515-th epoch train loss 1.0211404109698106\n",
      "515-th epoch val loss 0.9380135663996658\n",
      "516-th epoch train loss 1.0211505284718032\n",
      "516-th epoch val loss 0.9380306938648331\n",
      "517-th epoch train loss 1.0211604930475306\n",
      "517-th epoch val loss 0.9380475589443209\n",
      "518-th epoch train loss 1.021170306926515\n",
      "518-th epoch val loss 0.9380641655757634\n",
      "519-th epoch train loss 1.0211799723083987\n",
      "519-th epoch val loss 0.938080517640294\n",
      "520-th epoch train loss 1.0211894913632633\n",
      "520-th epoch val loss 0.938096618963277\n",
      "521-th epoch train loss 1.0211988662319356\n",
      "521-th epoch val loss 0.9381124733150226\n",
      "522-th epoch train loss 1.021208099026306\n",
      "522-th epoch val loss 0.9381280844115045\n",
      "523-th epoch train loss 1.021217191829638\n",
      "523-th epoch val loss 0.9381434559150659\n",
      "524-th epoch train loss 1.021226146696881\n",
      "524-th epoch val loss 0.93815859143512\n",
      "525-th epoch train loss 1.0212349656549797\n",
      "525-th epoch val loss 0.9381734945288444\n",
      "526-th epoch train loss 1.0212436507031868\n",
      "526-th epoch val loss 0.9381881687018664\n",
      "527-th epoch train loss 1.0212522038133711\n",
      "527-th epoch val loss 0.9382026174089475\n",
      "528-th epoch train loss 1.0212606269303246\n",
      "528-th epoch val loss 0.9382168440546518\n",
      "529-th epoch train loss 1.0212689219720728\n",
      "529-th epoch val loss 0.9382308519940177\n",
      "530-th epoch train loss 1.021277090830178\n",
      "530-th epoch val loss 0.9382446445332151\n",
      "531-th epoch train loss 1.0212851353700458\n",
      "531-th epoch val loss 0.9382582249301998\n",
      "532-th epoch train loss 1.021293057431226\n",
      "532-th epoch val loss 0.938271596395359\n",
      "533-th epoch train loss 1.0213008588277188\n",
      "533-th epoch val loss 0.9382847620921534\n",
      "534-th epoch train loss 1.0213085413482703\n",
      "534-th epoch val loss 0.9382977251377485\n",
      "535-th epoch train loss 1.0213161067566747\n",
      "535-th epoch val loss 0.9383104886036397\n",
      "536-th epoch train loss 1.02132355679207\n",
      "536-th epoch val loss 0.9383230555162746\n",
      "537-th epoch train loss 1.021330893169232\n",
      "537-th epoch val loss 0.9383354288576623\n",
      "538-th epoch train loss 1.0213381175788707\n",
      "538-th epoch val loss 0.9383476115659836\n",
      "539-th epoch train loss 1.021345231687919\n",
      "539-th epoch val loss 0.9383596065361851\n",
      "540-th epoch train loss 1.0213522371398236\n",
      "540-th epoch val loss 0.9383714166205762\n",
      "541-th epoch train loss 1.0213591355548333\n",
      "541-th epoch val loss 0.9383830446294124\n",
      "542-th epoch train loss 1.0213659285302827\n",
      "542-th epoch val loss 0.9383944933314744\n",
      "543-th epoch train loss 1.0213726176408775\n",
      "543-th epoch val loss 0.9384057654546423\n",
      "544-th epoch train loss 1.0213792044389745\n",
      "544-th epoch val loss 0.9384168636864594\n",
      "545-th epoch train loss 1.0213856904548626\n",
      "545-th epoch val loss 0.9384277906746937\n",
      "546-th epoch train loss 1.0213920771970373\n",
      "546-th epoch val loss 0.9384385490278887\n",
      "547-th epoch train loss 1.0213983661524775\n",
      "547-th epoch val loss 0.9384491413159112\n",
      "548-th epoch train loss 1.021404558786918\n",
      "548-th epoch val loss 0.9384595700704895\n",
      "549-th epoch train loss 1.0214106565451186\n",
      "549-th epoch val loss 0.9384698377857484\n",
      "550-th epoch train loss 1.0214166608511337\n",
      "550-th epoch val loss 0.9384799469187364\n",
      "551-th epoch train loss 1.021422573108575\n",
      "551-th epoch val loss 0.9384898998899439\n",
      "552-th epoch train loss 1.021428394700879\n",
      "552-th epoch val loss 0.9384996990838196\n",
      "553-th epoch train loss 1.0214341269915637\n",
      "553-th epoch val loss 0.9385093468492786\n",
      "554-th epoch train loss 1.0214397713244907\n",
      "554-th epoch val loss 0.9385188455002035\n",
      "555-th epoch train loss 1.0214453290241179\n",
      "555-th epoch val loss 0.9385281973159391\n",
      "556-th epoch train loss 1.0214508013957562\n",
      "556-th epoch val loss 0.9385374045417845\n",
      "557-th epoch train loss 1.0214561897258192\n",
      "557-th epoch val loss 0.9385464693894737\n",
      "558-th epoch train loss 1.021461495282071\n",
      "558-th epoch val loss 0.9385553940376548\n",
      "559-th epoch train loss 1.0214667193138756\n",
      "559-th epoch val loss 0.9385641806323597\n",
      "560-th epoch train loss 1.0214718630524389\n",
      "560-th epoch val loss 0.9385728312874733\n",
      "561-th epoch train loss 1.0214769277110483\n",
      "561-th epoch val loss 0.9385813480851869\n",
      "562-th epoch train loss 1.0214819144853151\n",
      "562-th epoch val loss 0.9385897330764562\n",
      "563-th epoch train loss 1.02148682455341\n",
      "563-th epoch val loss 0.9385979882814496\n",
      "564-th epoch train loss 1.0214916590762961\n",
      "564-th epoch val loss 0.9386061156899869\n",
      "565-th epoch train loss 1.0214964191979612\n",
      "565-th epoch val loss 0.9386141172619791\n",
      "566-th epoch train loss 1.0215011060456467\n",
      "566-th epoch val loss 0.9386219949278559\n",
      "567-th epoch train loss 1.021505720730075\n",
      "567-th epoch val loss 0.9386297505889953\n",
      "568-th epoch train loss 1.021510264345672\n",
      "568-th epoch val loss 0.9386373861181372\n",
      "569-th epoch train loss 1.0215147379707925\n",
      "569-th epoch val loss 0.9386449033598038\n",
      "570-th epoch train loss 1.0215191426679333\n",
      "570-th epoch val loss 0.9386523041307023\n",
      "571-th epoch train loss 1.0215234794839574\n",
      "571-th epoch val loss 0.9386595902201352\n",
      "572-th epoch train loss 1.0215277494503028\n",
      "572-th epoch val loss 0.93866676339039\n",
      "573-th epoch train loss 1.0215319535831973\n",
      "573-th epoch val loss 0.9386738253771395\n",
      "574-th epoch train loss 1.0215360928838664\n",
      "574-th epoch val loss 0.9386807778898228\n",
      "575-th epoch train loss 1.0215401683387437\n",
      "575-th epoch val loss 0.9386876226120343\n",
      "576-th epoch train loss 1.0215441809196706\n",
      "576-th epoch val loss 0.9386943612018942\n",
      "577-th epoch train loss 1.0215481315841037\n",
      "577-th epoch val loss 0.9387009952924253\n",
      "578-th epoch train loss 1.0215520212753124\n",
      "578-th epoch val loss 0.9387075264919175\n",
      "579-th epoch train loss 1.021555850922576\n",
      "579-th epoch val loss 0.938713956384292\n",
      "580-th epoch train loss 1.02155962144138\n",
      "580-th epoch val loss 0.9387202865294554\n",
      "581-th epoch train loss 1.0215633337336116\n",
      "581-th epoch val loss 0.9387265184636572\n",
      "582-th epoch train loss 1.021566988687745\n",
      "582-th epoch val loss 0.9387326536998312\n",
      "583-th epoch train loss 1.0215705871790333\n",
      "583-th epoch val loss 0.9387386937279432\n",
      "584-th epoch train loss 1.0215741300696952\n",
      "584-th epoch val loss 0.9387446400153266\n",
      "585-th epoch train loss 1.0215776182090963\n",
      "585-th epoch val loss 0.9387504940070169\n",
      "586-th epoch train loss 1.021581052433933\n",
      "586-th epoch val loss 0.9387562571260805\n",
      "587-th epoch train loss 1.0215844335684108\n",
      "587-th epoch val loss 0.9387619307739384\n",
      "588-th epoch train loss 1.0215877624244196\n",
      "588-th epoch val loss 0.938767516330687\n",
      "589-th epoch train loss 1.0215910398017136\n",
      "589-th epoch val loss 0.938773015155413\n",
      "590-th epoch train loss 1.0215942664880788\n",
      "590-th epoch val loss 0.9387784285865053\n",
      "591-th epoch train loss 1.0215974432595052\n",
      "591-th epoch val loss 0.9387837579419601\n",
      "592-th epoch train loss 1.0216005708803573\n",
      "592-th epoch val loss 0.9387890045196847\n",
      "593-th epoch train loss 1.0216036501035377\n",
      "593-th epoch val loss 0.9387941695977963\n",
      "594-th epoch train loss 1.021606681670652\n",
      "594-th epoch val loss 0.9387992544349145\n",
      "595-th epoch train loss 1.0216096663121708\n",
      "595-th epoch val loss 0.9388042602704512\n",
      "596-th epoch train loss 1.0216126047475917\n",
      "596-th epoch val loss 0.9388091883248998\n",
      "597-th epoch train loss 1.0216154976855922\n",
      "597-th epoch val loss 0.9388140398001111\n",
      "598-th epoch train loss 1.0216183458241916\n",
      "598-th epoch val loss 0.9388188158795774\n",
      "599-th epoch train loss 1.0216211498509002\n",
      "599-th epoch val loss 0.9388235177287025\n",
      "600-th epoch train loss 1.0216239104428735\n",
      "600-th epoch val loss 0.9388281464950737\n",
      "601-th epoch train loss 1.0216266282670605\n",
      "601-th epoch val loss 0.9388327033087256\n",
      "602-th epoch train loss 1.0216293039803532\n",
      "602-th epoch val loss 0.9388371892824056\n",
      "603-th epoch train loss 1.0216319382297316\n",
      "603-th epoch val loss 0.9388416055118316\n",
      "604-th epoch train loss 1.0216345316524067\n",
      "604-th epoch val loss 0.9388459530759456\n",
      "605-th epoch train loss 1.0216370848759646\n",
      "605-th epoch val loss 0.9388502330371657\n",
      "606-th epoch train loss 1.0216395985185047\n",
      "606-th epoch val loss 0.9388544464416364\n",
      "607-th epoch train loss 1.0216420731887799\n",
      "607-th epoch val loss 0.938858594319469\n",
      "608-th epoch train loss 1.021644509486331\n",
      "608-th epoch val loss 0.9388626776849841\n",
      "609-th epoch train loss 1.021646908001623\n",
      "609-th epoch val loss 0.9388666975369505\n",
      "610-th epoch train loss 1.0216492693161774\n",
      "610-th epoch val loss 0.9388706548588162\n",
      "611-th epoch train loss 1.0216515940027022\n",
      "611-th epoch val loss 0.9388745506189402\n",
      "612-th epoch train loss 1.0216538826252217\n",
      "612-th epoch val loss 0.9388783857708205\n",
      "613-th epoch train loss 1.021656135739206\n",
      "613-th epoch val loss 0.9388821612533158\n",
      "614-th epoch train loss 1.021658353891694\n",
      "614-th epoch val loss 0.9388858779908692\n",
      "615-th epoch train loss 1.0216605376214167\n",
      "615-th epoch val loss 0.9388895368937225\n",
      "616-th epoch train loss 1.0216626874589234\n",
      "616-th epoch val loss 0.9388931388581321\n",
      "617-th epoch train loss 1.0216648039266993\n",
      "617-th epoch val loss 0.9388966847665805\n",
      "618-th epoch train loss 1.021666887539284\n",
      "618-th epoch val loss 0.9389001754879838\n",
      "619-th epoch train loss 1.0216689388033924\n",
      "619-th epoch val loss 0.9389036118778966\n",
      "620-th epoch train loss 1.0216709582180254\n",
      "620-th epoch val loss 0.9389069947787149\n",
      "621-th epoch train loss 1.02167294627459\n",
      "621-th epoch val loss 0.9389103250198745\n",
      "622-th epoch train loss 1.0216749034570052\n",
      "622-th epoch val loss 0.9389136034180467\n",
      "623-th epoch train loss 1.0216768302418173\n",
      "623-th epoch val loss 0.9389168307773335\n",
      "624-th epoch train loss 1.0216787270983092\n",
      "624-th epoch val loss 0.9389200078894558\n",
      "625-th epoch train loss 1.0216805944886067\n",
      "625-th epoch val loss 0.938923135533944\n",
      "626-th epoch train loss 1.0216824328677847\n",
      "626-th epoch val loss 0.9389262144783203\n",
      "627-th epoch train loss 1.0216842426839736\n",
      "627-th epoch val loss 0.9389292454782807\n",
      "628-th epoch train loss 1.0216860243784622\n",
      "628-th epoch val loss 0.9389322292778781\n",
      "629-th epoch train loss 1.0216877783857992\n",
      "629-th epoch val loss 0.9389351666096967\n",
      "630-th epoch train loss 1.021689505133893\n",
      "630-th epoch val loss 0.9389380581950243\n",
      "631-th epoch train loss 1.0216912050441127\n",
      "631-th epoch val loss 0.938940904744029\n",
      "632-th epoch train loss 1.0216928785313848\n",
      "632-th epoch val loss 0.9389437069559242\n",
      "633-th epoch train loss 1.0216945260042882\n",
      "633-th epoch val loss 0.9389464655191377\n",
      "634-th epoch train loss 1.0216961478651518\n",
      "634-th epoch val loss 0.938949181111474\n",
      "635-th epoch train loss 1.0216977445101452\n",
      "635-th epoch val loss 0.9389518544002773\n",
      "636-th epoch train loss 1.021699316329374\n",
      "636-th epoch val loss 0.9389544860425929\n",
      "637-th epoch train loss 1.021700863706968\n",
      "637-th epoch val loss 0.938957076685319\n",
      "638-th epoch train loss 1.021702387021171\n",
      "638-th epoch val loss 0.9389596269653664\n",
      "639-th epoch train loss 1.0217038866444326\n",
      "639-th epoch val loss 0.9389621375098092\n",
      "640-th epoch train loss 1.0217053629434898\n",
      "640-th epoch val loss 0.9389646089360338\n",
      "641-th epoch train loss 1.0217068162794574\n",
      "641-th epoch val loss 0.9389670418518883\n",
      "642-th epoch train loss 1.0217082470079106\n",
      "642-th epoch val loss 0.9389694368558271\n",
      "643-th epoch train loss 1.0217096554789682\n",
      "643-th epoch val loss 0.9389717945370566\n",
      "644-th epoch train loss 1.0217110420373758\n",
      "644-th epoch val loss 0.9389741154756722\n",
      "645-th epoch train loss 1.0217124070225856\n",
      "645-th epoch val loss 0.9389764002428044\n",
      "646-th epoch train loss 1.0217137507688365\n",
      "646-th epoch val loss 0.9389786494007477\n",
      "647-th epoch train loss 1.0217150736052343\n",
      "647-th epoch val loss 0.9389808635031032\n",
      "648-th epoch train loss 1.021716375855826\n",
      "648-th epoch val loss 0.9389830430949068\n",
      "649-th epoch train loss 1.0217176578396796\n",
      "649-th epoch val loss 0.9389851887127628\n",
      "650-th epoch train loss 1.0217189198709564\n",
      "650-th epoch val loss 0.9389873008849714\n",
      "651-th epoch train loss 1.021720162258988\n",
      "651-th epoch val loss 0.9389893801316579\n",
      "652-th epoch train loss 1.0217213853083476\n",
      "652-th epoch val loss 0.9389914269648972\n",
      "653-th epoch train loss 1.0217225893189208\n",
      "653-th epoch val loss 0.938993441888835\n",
      "654-th epoch train loss 1.02172377458598\n",
      "654-th epoch val loss 0.9389954253998135\n",
      "655-th epoch train loss 1.0217249414002518\n",
      "655-th epoch val loss 0.938997377986488\n",
      "656-th epoch train loss 1.0217260900479863\n",
      "656-th epoch val loss 0.938999300129946\n",
      "657-th epoch train loss 1.0217272208110253\n",
      "657-th epoch val loss 0.9390011923038227\n",
      "658-th epoch train loss 1.0217283339668697\n",
      "658-th epoch val loss 0.9390030549744169\n",
      "659-th epoch train loss 1.0217294297887436\n",
      "659-th epoch val loss 0.9390048886008019\n",
      "660-th epoch train loss 1.0217305085456616\n",
      "660-th epoch val loss 0.9390066936349373\n",
      "661-th epoch train loss 1.021731570502491\n",
      "661-th epoch val loss 0.9390084705217785\n",
      "662-th epoch train loss 1.021732615920015\n",
      "662-th epoch val loss 0.9390102196993839\n",
      "663-th epoch train loss 1.0217336450549972\n",
      "663-th epoch val loss 0.939011941599022\n",
      "664-th epoch train loss 1.021734658160239\n",
      "664-th epoch val loss 0.9390136366452747\n",
      "665-th epoch train loss 1.021735655484641\n",
      "665-th epoch val loss 0.9390153052561396\n",
      "666-th epoch train loss 1.021736637273266\n",
      "666-th epoch val loss 0.939016947843133\n",
      "667-th epoch train loss 1.0217376037673924\n",
      "667-th epoch val loss 0.9390185648113888\n",
      "668-th epoch train loss 1.0217385552045746\n",
      "668-th epoch val loss 0.9390201565597569\n",
      "669-th epoch train loss 1.0217394918186997\n",
      "669-th epoch val loss 0.939021723480899\n",
      "670-th epoch train loss 1.021740413840042\n",
      "670-th epoch val loss 0.9390232659613859\n",
      "671-th epoch train loss 1.0217413214953204\n",
      "671-th epoch val loss 0.9390247843817892\n",
      "672-th epoch train loss 1.0217422150077504\n",
      "672-th epoch val loss 0.939026279116775\n",
      "673-th epoch train loss 1.021743094597099\n",
      "673-th epoch val loss 0.9390277505351959\n",
      "674-th epoch train loss 1.0217439604797376\n",
      "674-th epoch val loss 0.9390291990001778\n",
      "675-th epoch train loss 1.0217448128686915\n",
      "675-th epoch val loss 0.9390306248692091\n",
      "676-th epoch train loss 1.0217456519736945\n",
      "676-th epoch val loss 0.9390320284942301\n",
      "677-th epoch train loss 1.021746478001237\n",
      "677-th epoch val loss 0.9390334102217169\n",
      "678-th epoch train loss 1.0217472911546164\n",
      "678-th epoch val loss 0.9390347703927636\n",
      "679-th epoch train loss 1.0217480916339852\n",
      "679-th epoch val loss 0.9390361093431693\n",
      "680-th epoch train loss 1.0217488796363996\n",
      "680-th epoch val loss 0.939037427403516\n",
      "681-th epoch train loss 1.0217496553558678\n",
      "681-th epoch val loss 0.9390387248992527\n",
      "682-th epoch train loss 1.0217504189833955\n",
      "682-th epoch val loss 0.9390400021507718\n",
      "683-th epoch train loss 1.021751170707032\n",
      "683-th epoch val loss 0.9390412594734887\n",
      "684-th epoch train loss 1.0217519107119155\n",
      "684-th epoch val loss 0.9390424971779184\n",
      "685-th epoch train loss 1.0217526391803198\n",
      "685-th epoch val loss 0.9390437155697516\n",
      "686-th epoch train loss 1.0217533562916943\n",
      "686-th epoch val loss 0.9390449149499275\n",
      "687-th epoch train loss 1.021754062222711\n",
      "687-th epoch val loss 0.9390460956147096\n",
      "688-th epoch train loss 1.0217547571473042\n",
      "688-th epoch val loss 0.9390472578557556\n",
      "689-th epoch train loss 1.0217554412367151\n",
      "689-th epoch val loss 0.9390484019601916\n",
      "690-th epoch train loss 1.0217561146595322\n",
      "690-th epoch val loss 0.9390495282106793\n",
      "691-th epoch train loss 1.0217567775817311\n",
      "691-th epoch val loss 0.9390506368854858\n",
      "692-th epoch train loss 1.0217574301667152\n",
      "692-th epoch val loss 0.939051728258553\n",
      "693-th epoch train loss 1.0217580725753561\n",
      "693-th epoch val loss 0.9390528025995629\n",
      "694-th epoch train loss 1.0217587049660315\n",
      "694-th epoch val loss 0.9390538601740037\n",
      "695-th epoch train loss 1.0217593274946646\n",
      "695-th epoch val loss 0.9390549012432367\n",
      "696-th epoch train loss 1.0217599403147597\n",
      "696-th epoch val loss 0.9390559260645561\n",
      "697-th epoch train loss 1.0217605435774417\n",
      "697-th epoch val loss 0.9390569348912562\n",
      "698-th epoch train loss 1.021761137431492\n",
      "698-th epoch val loss 0.9390579279726904\n",
      "699-th epoch train loss 1.0217617220233843\n",
      "699-th epoch val loss 0.9390589055543346\n",
      "700-th epoch train loss 1.02176229749732\n",
      "700-th epoch val loss 0.9390598678778443\n",
      "701-th epoch train loss 1.0217628639952632\n",
      "701-th epoch val loss 0.939060815181117\n",
      "702-th epoch train loss 1.0217634216569764\n",
      "702-th epoch val loss 0.9390617476983485\n",
      "703-th epoch train loss 1.021763970620052\n",
      "703-th epoch val loss 0.9390626656600891\n",
      "704-th epoch train loss 1.0217645110199494\n",
      "704-th epoch val loss 0.9390635692933048\n",
      "705-th epoch train loss 1.0217650429900227\n",
      "705-th epoch val loss 0.9390644588214265\n",
      "706-th epoch train loss 1.021765566661558\n",
      "706-th epoch val loss 0.939065334464409\n",
      "707-th epoch train loss 1.0217660821638037\n",
      "707-th epoch val loss 0.9390661964387849\n",
      "708-th epoch train loss 1.021766589624\n",
      "708-th epoch val loss 0.9390670449577148\n",
      "709-th epoch train loss 1.0217670891674127\n",
      "709-th epoch val loss 0.9390678802310432\n",
      "710-th epoch train loss 1.0217675809173623\n",
      "710-th epoch val loss 0.9390687024653462\n",
      "711-th epoch train loss 1.0217680649952534\n",
      "711-th epoch val loss 0.9390695118639857\n",
      "712-th epoch train loss 1.0217685415206061\n",
      "712-th epoch val loss 0.9390703086271561\n",
      "713-th epoch train loss 1.021769010611083\n",
      "713-th epoch val loss 0.9390710929519364\n",
      "714-th epoch train loss 1.0217694723825197\n",
      "714-th epoch val loss 0.9390718650323359\n",
      "715-th epoch train loss 1.0217699269489515\n",
      "715-th epoch val loss 0.9390726250593435\n",
      "716-th epoch train loss 1.0217703744226434\n",
      "716-th epoch val loss 0.9390733732209748\n",
      "717-th epoch train loss 1.0217708149141143\n",
      "717-th epoch val loss 0.9390741097023169\n",
      "718-th epoch train loss 1.021771248532166\n",
      "718-th epoch val loss 0.9390748346855737\n",
      "719-th epoch train loss 1.02177167538391\n",
      "719-th epoch val loss 0.939075548350113\n",
      "720-th epoch train loss 1.0217720955747933\n",
      "720-th epoch val loss 0.9390762508725077\n",
      "721-th epoch train loss 1.0217725092086225\n",
      "721-th epoch val loss 0.9390769424265802\n",
      "722-th epoch train loss 1.0217729163875917\n",
      "722-th epoch val loss 0.9390776231834469\n",
      "723-th epoch train loss 1.021773317212306\n",
      "723-th epoch val loss 0.9390782933115578\n",
      "724-th epoch train loss 1.0217737117818058\n",
      "724-th epoch val loss 0.939078952976738\n",
      "725-th epoch train loss 1.0217741001935925\n",
      "725-th epoch val loss 0.9390796023422312\n",
      "726-th epoch train loss 1.021774482543651\n",
      "726-th epoch val loss 0.9390802415687357\n",
      "727-th epoch train loss 1.021774858926473\n",
      "727-th epoch val loss 0.9390808708144477\n",
      "728-th epoch train loss 1.021775229435081\n",
      "728-th epoch val loss 0.9390814902350976\n",
      "729-th epoch train loss 1.0217755941610511\n",
      "729-th epoch val loss 0.93908209998399\n",
      "730-th epoch train loss 1.0217759531945347\n",
      "730-th epoch val loss 0.9390827002120403\n",
      "731-th epoch train loss 1.0217763066242809\n",
      "731-th epoch val loss 0.9390832910678127\n",
      "732-th epoch train loss 1.021776654537657\n",
      "732-th epoch val loss 0.9390838726975544\n",
      "733-th epoch train loss 1.0217769970206732\n",
      "733-th epoch val loss 0.939084445245236\n",
      "734-th epoch train loss 1.0217773341579994\n",
      "734-th epoch val loss 0.9390850088525817\n",
      "735-th epoch train loss 1.0217776660329903\n",
      "735-th epoch val loss 0.9390855636591102\n",
      "736-th epoch train loss 1.0217779927277015\n",
      "736-th epoch val loss 0.9390861098021629\n",
      "737-th epoch train loss 1.0217783143229124\n",
      "737-th epoch val loss 0.9390866474169415\n",
      "738-th epoch train loss 1.0217786308981458\n",
      "738-th epoch val loss 0.9390871766365403\n",
      "739-th epoch train loss 1.0217789425316857\n",
      "739-th epoch val loss 0.9390876975919779\n",
      "740-th epoch train loss 1.021779249300598\n",
      "740-th epoch val loss 0.9390882104122322\n",
      "741-th epoch train loss 1.021779551280751\n",
      "741-th epoch val loss 0.9390887152242706\n",
      "742-th epoch train loss 1.021779848546828\n",
      "742-th epoch val loss 0.9390892121530783\n",
      "743-th epoch train loss 1.0217801411723533\n",
      "743-th epoch val loss 0.9390897013216951\n",
      "744-th epoch train loss 1.0217804292297044\n",
      "744-th epoch val loss 0.939090182851241\n",
      "745-th epoch train loss 1.0217807127901333\n",
      "745-th epoch val loss 0.939090656860948\n",
      "746-th epoch train loss 1.0217809919237817\n",
      "746-th epoch val loss 0.9390911234681892\n",
      "747-th epoch train loss 1.0217812666996993\n",
      "747-th epoch val loss 0.9390915827885069\n",
      "748-th epoch train loss 1.0217815371858605\n",
      "748-th epoch val loss 0.9390920349356426\n",
      "749-th epoch train loss 1.0217818034491817\n",
      "749-th epoch val loss 0.9390924800215632\n",
      "750-th epoch train loss 1.0217820655555365\n",
      "750-th epoch val loss 0.939092918156491\n",
      "751-th epoch train loss 1.021782323569773\n",
      "751-th epoch val loss 0.9390933494489282\n",
      "752-th epoch train loss 1.0217825775557292\n",
      "752-th epoch val loss 0.9390937740056857\n",
      "753-th epoch train loss 1.0217828275762482\n",
      "753-th epoch val loss 0.939094191931908\n",
      "754-th epoch train loss 1.0217830736931954\n",
      "754-th epoch val loss 0.9390946033311014\n",
      "755-th epoch train loss 1.0217833159674712\n",
      "755-th epoch val loss 0.9390950083051554\n",
      "756-th epoch train loss 1.0217835544590286\n",
      "756-th epoch val loss 0.9390954069543721\n",
      "757-th epoch train loss 1.0217837892268853\n",
      "757-th epoch val loss 0.9390957993774892\n",
      "758-th epoch train loss 1.02178402032914\n",
      "758-th epoch val loss 0.9390961856717033\n",
      "759-th epoch train loss 1.021784247822987\n",
      "759-th epoch val loss 0.939096565932696\n",
      "760-th epoch train loss 1.0217844717647278\n",
      "760-th epoch val loss 0.9390969402546561\n",
      "761-th epoch train loss 1.0217846922097882\n",
      "761-th epoch val loss 0.9390973087303024\n",
      "762-th epoch train loss 1.0217849092127298\n",
      "762-th epoch val loss 0.9390976714509086\n",
      "763-th epoch train loss 1.0217851228272639\n",
      "763-th epoch val loss 0.9390980285063242\n",
      "764-th epoch train loss 1.0217853331062645\n",
      "764-th epoch val loss 0.9390983799849971\n",
      "765-th epoch train loss 1.0217855401017817\n",
      "765-th epoch val loss 0.9390987259739934\n",
      "766-th epoch train loss 1.0217857438650548\n",
      "766-th epoch val loss 0.9390990665590248\n",
      "767-th epoch train loss 1.0217859444465245\n",
      "767-th epoch val loss 0.9390994018244627\n",
      "768-th epoch train loss 1.021786141895845\n",
      "768-th epoch val loss 0.939099731853363\n",
      "769-th epoch train loss 1.0217863362618964\n",
      "769-th epoch val loss 0.9391000567274863\n",
      "770-th epoch train loss 1.0217865275927973\n",
      "770-th epoch val loss 0.9391003765273175\n",
      "771-th epoch train loss 1.0217867159359157\n",
      "771-th epoch val loss 0.9391006913320845\n",
      "772-th epoch train loss 1.021786901337882\n",
      "772-th epoch val loss 0.9391010012197807\n",
      "773-th epoch train loss 1.0217870838445988\n",
      "773-th epoch val loss 0.9391013062671811\n",
      "774-th epoch train loss 1.0217872635012537\n",
      "774-th epoch val loss 0.9391016065498641\n",
      "775-th epoch train loss 1.0217874403523302\n",
      "775-th epoch val loss 0.9391019021422284\n",
      "776-th epoch train loss 1.021787614441617\n",
      "776-th epoch val loss 0.9391021931175113\n",
      "777-th epoch train loss 1.0217877858122206\n",
      "777-th epoch val loss 0.9391024795478063\n",
      "778-th epoch train loss 1.0217879545065758\n",
      "778-th epoch val loss 0.9391027615040842\n",
      "779-th epoch train loss 1.0217881205664558\n",
      "779-th epoch val loss 0.9391030390562071\n",
      "780-th epoch train loss 1.0217882840329815\n",
      "780-th epoch val loss 0.9391033122729462\n",
      "781-th epoch train loss 1.0217884449466335\n",
      "781-th epoch val loss 0.9391035812220004\n",
      "782-th epoch train loss 1.0217886033472614\n",
      "782-th epoch val loss 0.9391038459700126\n",
      "783-th epoch train loss 1.021788759274092\n",
      "783-th epoch val loss 0.939104106582584\n",
      "784-th epoch train loss 1.0217889127657414\n",
      "784-th epoch val loss 0.9391043631242934\n",
      "785-th epoch train loss 1.0217890638602236\n",
      "785-th epoch val loss 0.9391046156587115\n",
      "786-th epoch train loss 1.0217892125949597\n",
      "786-th epoch val loss 0.9391048642484159\n",
      "787-th epoch train loss 1.0217893590067852\n",
      "787-th epoch val loss 0.9391051089550078\n",
      "788-th epoch train loss 1.0217895031319657\n",
      "788-th epoch val loss 0.9391053498391287\n",
      "789-th epoch train loss 1.0217896450061954\n",
      "789-th epoch val loss 0.9391055869604713\n",
      "790-th epoch train loss 1.0217897846646158\n",
      "790-th epoch val loss 0.9391058203777974\n",
      "791-th epoch train loss 1.0217899221418194\n",
      "791-th epoch val loss 0.9391060501489523\n",
      "792-th epoch train loss 1.0217900574718572\n",
      "792-th epoch val loss 0.9391062763308768\n",
      "793-th epoch train loss 1.021790190688253\n",
      "793-th epoch val loss 0.9391064989796255\n",
      "794-th epoch train loss 1.0217903218240036\n",
      "794-th epoch val loss 0.9391067181503755\n",
      "795-th epoch train loss 1.021790450911593\n",
      "795-th epoch val loss 0.939106933897444\n",
      "796-th epoch train loss 1.0217905779829983\n",
      "796-th epoch val loss 0.939107146274299\n",
      "797-th epoch train loss 1.0217907030696982\n",
      "797-th epoch val loss 0.9391073553335758\n",
      "798-th epoch train loss 1.021790826202678\n",
      "798-th epoch val loss 0.9391075611270863\n",
      "799-th epoch train loss 1.021790947412443\n",
      "799-th epoch val loss 0.9391077637058349\n",
      "800-th epoch train loss 1.0217910667290202\n",
      "800-th epoch val loss 0.9391079631200286\n",
      "801-th epoch train loss 1.0217911841819691\n",
      "801-th epoch val loss 0.9391081594190916\n",
      "802-th epoch train loss 1.0217912998003877\n",
      "802-th epoch val loss 0.9391083526516762\n",
      "803-th epoch train loss 1.0217914136129196\n",
      "803-th epoch val loss 0.9391085428656748\n",
      "804-th epoch train loss 1.0217915256477623\n",
      "804-th epoch val loss 0.9391087301082328\n",
      "805-th epoch train loss 1.0217916359326729\n",
      "805-th epoch val loss 0.939108914425759\n",
      "806-th epoch train loss 1.0217917444949751\n",
      "806-th epoch val loss 0.9391090958639381\n",
      "807-th epoch train loss 1.021791851361567\n",
      "807-th epoch val loss 0.9391092744677404\n",
      "808-th epoch train loss 1.021791956558926\n",
      "808-th epoch val loss 0.9391094502814367\n",
      "809-th epoch train loss 1.0217920601131176\n",
      "809-th epoch val loss 0.9391096233486039\n",
      "810-th epoch train loss 1.0217921620497987\n",
      "810-th epoch val loss 0.9391097937121393\n",
      "811-th epoch train loss 1.0217922623942277\n",
      "811-th epoch val loss 0.9391099614142717\n",
      "812-th epoch train loss 1.021792361171268\n",
      "812-th epoch val loss 0.939110126496569\n",
      "813-th epoch train loss 1.0217924584053948\n",
      "813-th epoch val loss 0.9391102889999512\n",
      "814-th epoch train loss 1.0217925541207018\n",
      "814-th epoch val loss 0.9391104489646985\n",
      "815-th epoch train loss 1.021792648340906\n",
      "815-th epoch val loss 0.9391106064304624\n",
      "816-th epoch train loss 1.0217927410893557\n",
      "816-th epoch val loss 0.9391107614362765\n",
      "817-th epoch train loss 1.0217928323890326\n",
      "817-th epoch val loss 0.9391109140205629\n",
      "818-th epoch train loss 1.0217929222625617\n",
      "818-th epoch val loss 0.9391110642211451\n",
      "819-th epoch train loss 1.021793010732214\n",
      "819-th epoch val loss 0.9391112120752558\n",
      "820-th epoch train loss 1.0217930978199128\n",
      "820-th epoch val loss 0.9391113576195462\n",
      "821-th epoch train loss 1.0217931835472394\n",
      "821-th epoch val loss 0.9391115008900954\n",
      "822-th epoch train loss 1.0217932679354387\n",
      "822-th epoch val loss 0.9391116419224186\n",
      "823-th epoch train loss 1.0217933510054227\n",
      "823-th epoch val loss 0.9391117807514769\n",
      "824-th epoch train loss 1.0217934327777791\n",
      "824-th epoch val loss 0.9391119174116855\n",
      "825-th epoch train loss 1.0217935132727716\n",
      "825-th epoch val loss 0.9391120519369216\n",
      "826-th epoch train loss 1.0217935925103503\n",
      "826-th epoch val loss 0.9391121843605349\n",
      "827-th epoch train loss 1.0217936705101516\n",
      "827-th epoch val loss 0.9391123147153518\n",
      "828-th epoch train loss 1.0217937472915068\n",
      "828-th epoch val loss 0.9391124430336886\n",
      "829-th epoch train loss 1.0217938228734451\n",
      "829-th epoch val loss 0.9391125693473555\n",
      "830-th epoch train loss 1.0217938972746983\n",
      "830-th epoch val loss 0.9391126936876671\n",
      "831-th epoch train loss 1.0217939705137062\n",
      "831-th epoch val loss 0.9391128160854474\n",
      "832-th epoch train loss 1.021794042608621\n",
      "832-th epoch val loss 0.9391129365710408\n",
      "833-th epoch train loss 1.02179411357731\n",
      "833-th epoch val loss 0.9391130551743154\n",
      "834-th epoch train loss 1.0217941834373634\n",
      "834-th epoch val loss 0.9391131719246757\n",
      "835-th epoch train loss 1.0217942522060963\n",
      "835-th epoch val loss 0.939113286851065\n",
      "836-th epoch train loss 1.0217943199005528\n",
      "836-th epoch val loss 0.9391133999819764\n",
      "837-th epoch train loss 1.0217943865375114\n",
      "837-th epoch val loss 0.9391135113454554\n",
      "838-th epoch train loss 1.021794452133489\n",
      "838-th epoch val loss 0.9391136209691126\n",
      "839-th epoch train loss 1.0217945167047437\n",
      "839-th epoch val loss 0.9391137288801245\n",
      "840-th epoch train loss 1.0217945802672805\n",
      "840-th epoch val loss 0.9391138351052449\n",
      "841-th epoch train loss 1.0217946428368545\n",
      "841-th epoch val loss 0.9391139396708094\n",
      "842-th epoch train loss 1.0217947044289741\n",
      "842-th epoch val loss 0.9391140426027427\n",
      "843-th epoch train loss 1.0217947650589072\n",
      "843-th epoch val loss 0.9391141439265637\n",
      "844-th epoch train loss 1.0217948247416813\n",
      "844-th epoch val loss 0.9391142436673932\n",
      "845-th epoch train loss 1.0217948834920905\n",
      "845-th epoch val loss 0.9391143418499599\n",
      "846-th epoch train loss 1.021794941324698\n",
      "846-th epoch val loss 0.9391144384986063\n",
      "847-th epoch train loss 1.0217949982538388\n",
      "847-th epoch val loss 0.9391145336372939\n",
      "848-th epoch train loss 1.0217950542936238\n",
      "848-th epoch val loss 0.939114627289611\n",
      "849-th epoch train loss 1.0217951094579463\n",
      "849-th epoch val loss 0.9391147194787777\n",
      "850-th epoch train loss 1.0217951637604783\n",
      "850-th epoch val loss 0.9391148102276495\n",
      "851-th epoch train loss 1.0217952172146825\n",
      "851-th epoch val loss 0.9391148995587271\n",
      "852-th epoch train loss 1.021795269833809\n",
      "852-th epoch val loss 0.9391149874941588\n",
      "853-th epoch train loss 1.0217953216309013\n",
      "853-th epoch val loss 0.9391150740557472\n",
      "854-th epoch train loss 1.0217953726187998\n",
      "854-th epoch val loss 0.9391151592649543\n",
      "855-th epoch train loss 1.021795422810144\n",
      "855-th epoch val loss 0.9391152431429065\n",
      "856-th epoch train loss 1.0217954722173757\n",
      "856-th epoch val loss 0.9391153257104004\n",
      "857-th epoch train loss 1.021795520852743\n",
      "857-th epoch val loss 0.9391154069879083\n",
      "858-th epoch train loss 1.0217955687283027\n",
      "858-th epoch val loss 0.9391154869955823\n",
      "859-th epoch train loss 1.0217956158559243\n",
      "859-th epoch val loss 0.9391155657532605\n",
      "860-th epoch train loss 1.0217956622472892\n",
      "860-th epoch val loss 0.9391156432804696\n",
      "861-th epoch train loss 1.0217957079138982\n",
      "861-th epoch val loss 0.939115719596432\n",
      "862-th epoch train loss 1.0217957528670736\n",
      "862-th epoch val loss 0.9391157947200709\n",
      "863-th epoch train loss 1.0217957971179576\n",
      "863-th epoch val loss 0.9391158686700115\n",
      "864-th epoch train loss 1.0217958406775225\n",
      "864-th epoch val loss 0.9391159414645917\n",
      "865-th epoch train loss 1.0217958835565664\n",
      "865-th epoch val loss 0.9391160131218599\n",
      "866-th epoch train loss 1.0217959257657185\n",
      "866-th epoch val loss 0.9391160836595831\n",
      "867-th epoch train loss 1.0217959673154449\n",
      "867-th epoch val loss 0.939116153095252\n",
      "868-th epoch train loss 1.0217960082160453\n",
      "868-th epoch val loss 0.9391162214460833\n",
      "869-th epoch train loss 1.0217960484776596\n",
      "869-th epoch val loss 0.9391162887290245\n",
      "870-th epoch train loss 1.0217960881102697\n",
      "870-th epoch val loss 0.9391163549607593\n",
      "871-th epoch train loss 1.0217961271237013\n",
      "871-th epoch val loss 0.9391164201577094\n",
      "872-th epoch train loss 1.0217961655276264\n",
      "872-th epoch val loss 0.939116484336041\n",
      "873-th epoch train loss 1.0217962033315668\n",
      "873-th epoch val loss 0.9391165475116682\n",
      "874-th epoch train loss 1.0217962405448944\n",
      "874-th epoch val loss 0.9391166097002553\n",
      "875-th epoch train loss 1.0217962771768354\n",
      "875-th epoch val loss 0.9391166709172226\n",
      "876-th epoch train loss 1.0217963132364722\n",
      "876-th epoch val loss 0.93911673117775\n",
      "877-th epoch train loss 1.0217963487327444\n",
      "877-th epoch val loss 0.9391167904967787\n",
      "878-th epoch train loss 1.0217963836744535\n",
      "878-th epoch val loss 0.9391168488890181\n",
      "879-th epoch train loss 1.0217964180702617\n",
      "879-th epoch val loss 0.9391169063689474\n",
      "880-th epoch train loss 1.0217964519286975\n",
      "880-th epoch val loss 0.9391169629508196\n",
      "881-th epoch train loss 1.0217964852581547\n",
      "881-th epoch val loss 0.9391170186486645\n",
      "882-th epoch train loss 1.0217965180668982\n",
      "882-th epoch val loss 0.9391170734762937\n",
      "883-th epoch train loss 1.021796550363061\n",
      "883-th epoch val loss 0.9391171274473015\n",
      "884-th epoch train loss 1.0217965821546509\n",
      "884-th epoch val loss 0.9391171805750714\n",
      "885-th epoch train loss 1.0217966134495504\n",
      "885-th epoch val loss 0.9391172328727779\n",
      "886-th epoch train loss 1.021796644255519\n",
      "886-th epoch val loss 0.9391172843533886\n",
      "887-th epoch train loss 1.0217966745801943\n",
      "887-th epoch val loss 0.9391173350296684\n",
      "888-th epoch train loss 1.0217967044310952\n",
      "888-th epoch val loss 0.9391173849141846\n",
      "889-th epoch train loss 1.021796733815623\n",
      "889-th epoch val loss 0.9391174340193068\n",
      "890-th epoch train loss 1.0217967627410638\n",
      "890-th epoch val loss 0.9391174823572108\n",
      "891-th epoch train loss 1.0217967912145882\n",
      "891-th epoch val loss 0.9391175299398833\n",
      "892-th epoch train loss 1.0217968192432574\n",
      "892-th epoch val loss 0.9391175767791238\n",
      "893-th epoch train loss 1.0217968468340204\n",
      "893-th epoch val loss 0.9391176228865463\n",
      "894-th epoch train loss 1.0217968739937187\n",
      "894-th epoch val loss 0.9391176682735849\n",
      "895-th epoch train loss 1.0217969007290864\n",
      "895-th epoch val loss 0.9391177129514948\n",
      "896-th epoch train loss 1.0217969270467517\n",
      "896-th epoch val loss 0.9391177569313539\n",
      "897-th epoch train loss 1.0217969529532414\n",
      "897-th epoch val loss 0.9391178002240683\n",
      "898-th epoch train loss 1.0217969784549779\n",
      "898-th epoch val loss 0.9391178428403735\n",
      "899-th epoch train loss 1.0217970035582848\n",
      "899-th epoch val loss 0.9391178847908374\n",
      "900-th epoch train loss 1.021797028269387\n",
      "900-th epoch val loss 0.9391179260858626\n",
      "901-th epoch train loss 1.021797052594411\n",
      "901-th epoch val loss 0.9391179667356896\n",
      "902-th epoch train loss 1.0217970765393882\n",
      "902-th epoch val loss 0.939118006750398\n",
      "903-th epoch train loss 1.021797100110257\n",
      "903-th epoch val loss 0.9391180461399112\n",
      "904-th epoch train loss 1.021797123312861\n",
      "904-th epoch val loss 0.9391180849139966\n",
      "905-th epoch train loss 1.0217971461529538\n",
      "905-th epoch val loss 0.9391181230822695\n",
      "906-th epoch train loss 1.0217971686361986\n",
      "906-th epoch val loss 0.9391181606541944\n",
      "907-th epoch train loss 1.0217971907681709\n",
      "907-th epoch val loss 0.9391181976390887\n",
      "908-th epoch train loss 1.0217972125543573\n",
      "908-th epoch val loss 0.9391182340461236\n",
      "909-th epoch train loss 1.0217972340001615\n",
      "909-th epoch val loss 0.9391182698843282\n",
      "910-th epoch train loss 1.0217972551108998\n",
      "910-th epoch val loss 0.9391183051625885\n",
      "911-th epoch train loss 1.0217972758918077\n",
      "911-th epoch val loss 0.9391183398896537\n",
      "912-th epoch train loss 1.0217972963480377\n",
      "912-th epoch val loss 0.9391183740741347\n",
      "913-th epoch train loss 1.021797316484663\n",
      "913-th epoch val loss 0.9391184077245097\n",
      "914-th epoch train loss 1.0217973363066755\n",
      "914-th epoch val loss 0.9391184408491224\n",
      "915-th epoch train loss 1.0217973558189914\n",
      "915-th epoch val loss 0.9391184734561876\n",
      "916-th epoch train loss 1.0217973750264486\n",
      "916-th epoch val loss 0.9391185055537914\n",
      "917-th epoch train loss 1.02179739393381\n",
      "917-th epoch val loss 0.9391185371498931\n",
      "918-th epoch train loss 1.0217974125457638\n",
      "918-th epoch val loss 0.9391185682523286\n",
      "919-th epoch train loss 1.0217974308669262\n",
      "919-th epoch val loss 0.9391185988688104\n",
      "920-th epoch train loss 1.0217974489018382\n",
      "920-th epoch val loss 0.9391186290069304\n",
      "921-th epoch train loss 1.0217974666549734\n",
      "921-th epoch val loss 0.9391186586741632\n",
      "922-th epoch train loss 1.021797484130734\n",
      "922-th epoch val loss 0.9391186878778657\n",
      "923-th epoch train loss 1.0217975013334535\n",
      "923-th epoch val loss 0.9391187166252802\n",
      "924-th epoch train loss 1.021797518267397\n",
      "924-th epoch val loss 0.9391187449235353\n",
      "925-th epoch train loss 1.0217975349367647\n",
      "925-th epoch val loss 0.9391187727796488\n",
      "926-th epoch train loss 1.0217975513456892\n",
      "926-th epoch val loss 0.9391188002005291\n",
      "927-th epoch train loss 1.0217975674982396\n",
      "927-th epoch val loss 0.939118827192975\n",
      "928-th epoch train loss 1.0217975833984216\n",
      "928-th epoch val loss 0.9391188537636812\n",
      "929-th epoch train loss 1.0217975990501782\n",
      "929-th epoch val loss 0.939118879919237\n",
      "930-th epoch train loss 1.02179761445739\n",
      "930-th epoch val loss 0.9391189056661284\n",
      "931-th epoch train loss 1.0217976296238782\n",
      "931-th epoch val loss 0.9391189310107405\n",
      "932-th epoch train loss 1.0217976445534032\n",
      "932-th epoch val loss 0.9391189559593587\n",
      "933-th epoch train loss 1.0217976592496674\n",
      "933-th epoch val loss 0.9391189805181684\n",
      "934-th epoch train loss 1.0217976737163157\n",
      "934-th epoch val loss 0.9391190046932626\n",
      "935-th epoch train loss 1.0217976879569346\n",
      "935-th epoch val loss 0.9391190284906344\n",
      "936-th epoch train loss 1.0217977019750564\n",
      "936-th epoch val loss 0.9391190519161866\n",
      "937-th epoch train loss 1.0217977157741565\n",
      "937-th epoch val loss 0.9391190749757279\n",
      "938-th epoch train loss 1.0217977293576563\n",
      "938-th epoch val loss 0.9391190976749768\n",
      "939-th epoch train loss 1.0217977427289255\n",
      "939-th epoch val loss 0.9391191200195622\n",
      "940-th epoch train loss 1.0217977558912794\n",
      "940-th epoch val loss 0.9391191420150263\n",
      "941-th epoch train loss 1.0217977688479818\n",
      "941-th epoch val loss 0.939119163666823\n",
      "942-th epoch train loss 1.0217977816022463\n",
      "942-th epoch val loss 0.9391191849803231\n",
      "943-th epoch train loss 1.0217977941572347\n",
      "943-th epoch val loss 0.9391192059608104\n",
      "944-th epoch train loss 1.0217978065160613\n",
      "944-th epoch val loss 0.939119226613489\n",
      "945-th epoch train loss 1.0217978186817909\n",
      "945-th epoch val loss 0.9391192469434804\n",
      "946-th epoch train loss 1.0217978306574398\n",
      "946-th epoch val loss 0.9391192669558264\n",
      "947-th epoch train loss 1.0217978424459784\n",
      "947-th epoch val loss 0.9391192866554894\n",
      "948-th epoch train loss 1.0217978540503287\n",
      "948-th epoch val loss 0.9391193060473553\n",
      "949-th epoch train loss 1.0217978654733701\n",
      "949-th epoch val loss 0.9391193251362332\n",
      "950-th epoch train loss 1.0217978767179345\n",
      "950-th epoch val loss 0.939119343926857\n",
      "951-th epoch train loss 1.0217978877868106\n",
      "951-th epoch val loss 0.9391193624238864\n",
      "952-th epoch train loss 1.021797898682743\n",
      "952-th epoch val loss 0.9391193806319086\n",
      "953-th epoch train loss 1.0217979094084342\n",
      "953-th epoch val loss 0.9391193985554398\n",
      "954-th epoch train loss 1.0217979199665432\n",
      "954-th epoch val loss 0.939119416198924\n",
      "955-th epoch train loss 1.0217979303596891\n",
      "955-th epoch val loss 0.9391194335667368\n",
      "956-th epoch train loss 1.0217979405904487\n",
      "956-th epoch val loss 0.9391194506631858\n",
      "957-th epoch train loss 1.02179795066136\n",
      "957-th epoch val loss 0.9391194674925112\n",
      "958-th epoch train loss 1.0217979605749192\n",
      "958-th epoch val loss 0.9391194840588857\n",
      "959-th epoch train loss 1.0217979703335858\n",
      "959-th epoch val loss 0.9391195003664187\n",
      "960-th epoch train loss 1.0217979799397794\n",
      "960-th epoch val loss 0.9391195164191538\n",
      "961-th epoch train loss 1.0217979893958822\n",
      "961-th epoch val loss 0.9391195322210718\n",
      "962-th epoch train loss 1.0217979987042394\n",
      "962-th epoch val loss 0.9391195477760919\n",
      "963-th epoch train loss 1.0217980078671582\n",
      "963-th epoch val loss 0.9391195630880714\n",
      "964-th epoch train loss 1.0217980168869127\n",
      "964-th epoch val loss 0.9391195781608078\n",
      "965-th epoch train loss 1.0217980257657395\n",
      "965-th epoch val loss 0.9391195929980396\n",
      "966-th epoch train loss 1.021798034505839\n",
      "966-th epoch val loss 0.939119607603446\n",
      "967-th epoch train loss 1.0217980431093805\n",
      "967-th epoch val loss 0.9391196219806491\n",
      "968-th epoch train loss 1.0217980515784961\n",
      "968-th epoch val loss 0.9391196361332145\n",
      "969-th epoch train loss 1.0217980599152865\n",
      "969-th epoch val loss 0.9391196500646515\n",
      "970-th epoch train loss 1.0217980681218195\n",
      "970-th epoch val loss 0.9391196637784159\n",
      "971-th epoch train loss 1.0217980762001295\n",
      "971-th epoch val loss 0.9391196772779081\n",
      "972-th epoch train loss 1.021798084152221\n",
      "972-th epoch val loss 0.9391196905664763\n",
      "973-th epoch train loss 1.021798091980065\n",
      "973-th epoch val loss 0.9391197036474158\n",
      "974-th epoch train loss 1.021798099685603\n",
      "974-th epoch val loss 0.9391197165239709\n",
      "975-th epoch train loss 1.0217981072707465\n",
      "975-th epoch val loss 0.9391197291993344\n",
      "976-th epoch train loss 1.021798114737376\n",
      "976-th epoch val loss 0.9391197416766509\n",
      "977-th epoch train loss 1.0217981220873436\n",
      "977-th epoch val loss 0.939119753959014\n",
      "978-th epoch train loss 1.0217981293224714\n",
      "978-th epoch val loss 0.9391197660494698\n",
      "979-th epoch train loss 1.0217981364445539\n",
      "979-th epoch val loss 0.9391197779510166\n",
      "980-th epoch train loss 1.0217981434553585\n",
      "980-th epoch val loss 0.9391197896666067\n",
      "981-th epoch train loss 1.0217981503566214\n",
      "981-th epoch val loss 0.9391198011991443\n",
      "982-th epoch train loss 1.0217981571500563\n",
      "982-th epoch val loss 0.9391198125514907\n",
      "983-th epoch train loss 1.0217981638373468\n",
      "983-th epoch val loss 0.9391198237264612\n",
      "984-th epoch train loss 1.021798170420152\n",
      "984-th epoch val loss 0.9391198347268261\n",
      "985-th epoch train loss 1.0217981769001037\n",
      "985-th epoch val loss 0.9391198455553148\n",
      "986-th epoch train loss 1.0217981832788097\n",
      "986-th epoch val loss 0.9391198562146124\n",
      "987-th epoch train loss 1.0217981895578512\n",
      "987-th epoch val loss 0.9391198667073617\n",
      "988-th epoch train loss 1.0217981957387858\n",
      "988-th epoch val loss 0.9391198770361662\n",
      "989-th epoch train loss 1.021798201823146\n",
      "989-th epoch val loss 0.9391198872035863\n",
      "990-th epoch train loss 1.0217982078124408\n",
      "990-th epoch val loss 0.9391198972121443\n",
      "991-th epoch train loss 1.0217982137081556\n",
      "991-th epoch val loss 0.9391199070643214\n",
      "992-th epoch train loss 1.0217982195117523\n",
      "992-th epoch val loss 0.939119916762562\n",
      "993-th epoch train loss 1.0217982252246707\n",
      "993-th epoch val loss 0.9391199263092703\n",
      "994-th epoch train loss 1.021798230848327\n",
      "994-th epoch val loss 0.9391199357068153\n",
      "995-th epoch train loss 1.021798236384117\n",
      "995-th epoch val loss 0.9391199449575266\n",
      "996-th epoch train loss 1.021798241833412\n",
      "996-th epoch val loss 0.9391199540636987\n",
      "997-th epoch train loss 1.021798247197564\n",
      "997-th epoch val loss 0.9391199630275894\n",
      "998-th epoch train loss 1.0217982524779035\n",
      "998-th epoch val loss 0.9391199718514227\n",
      "999-th epoch train loss 1.0217982576757405\n",
      "999-th epoch val loss 0.9391199805373862\n"
     ]
    }
   ],
   "source": [
    "slr = ScratchLinearRegression(num_iter=1000, lr=0.01, no_bias=True, verbose=True)\n",
    "slr.fit(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fc7bd91-f5bf-4925-90fd-a0fc924b0261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x71b23a10>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAGdCAYAAAA44ojeAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA1HklEQVR4nO3de3xU9YH///eZTGYyucyEBDIhkmBqqajgFcWotbbmV7x8rWzpxf5oS9WVXsCKtFVpi9vdaqN217pYK7Xfrra7KFu7SpVfS5cGC7XGyEVUFAGVShQTLiEzuU7m8vn9MWHKYARCZubMJK/n43EqnHPmzHs+Inn3zDmfYxljjAAAALKIw+4AAAAAh6OgAACArENBAQAAWYeCAgAAsg4FBQAAZB0KCgAAyDoUFAAAkHUoKAAAIOs47Q5wPGKxmHbv3q2SkhJZlmV3HAAAcAyMMers7FRVVZUcjiOfI8nJgrJ7925VV1fbHQMAAByHlpYWTZgw4Yj75GRBKSkpkRT/gF6v1+Y0AADgWASDQVVXVyd+jh9JThaUg1/reL1eCgoAADnmWC7P4CJZAACQdSgoAAAg61BQAABA1qGgAACArENBAQAAWYeCAgAAss6QC8q6det01VVXqaqqSpZlacWKFYlt4XBYt956q6ZOnaqioiJVVVXpy1/+snbv3p10jPb2ds2ePVter1elpaW6/vrr1dXVNewPAwAARoYhF5Tu7m6dccYZeuCBB963raenR5s2bdLixYu1adMmPfHEE9q2bZs+9alPJe03e/Zsvfrqq1q9erVWrlypdevWae7cucf/KQAAwIhiGWPMcb/YsvTkk09q5syZH7jP+vXrdd555+ntt99WTU2Ntm7dqlNPPVXr16/XtGnTJEmrVq3SFVdcoXfeeUdVVVVHfd9gMCifz6dAIMBEbQAA5Iih/PxO+zUogUBAlmWptLRUktTU1KTS0tJEOZGk+vp6ORwONTc3D3qMUCikYDCYtAAAgJErrQWlr69Pt956q77whS8kmlJra6sqKiqS9nM6nSorK1Nra+ugx2loaJDP50ssPCgQAICRLW0FJRwO63Of+5yMMXrwwQeHdaxFixYpEAgklpaWlhSlBAAA2SgtDws8WE7efvttrVmzJul7psrKSu3Zsydp/0gkovb2dlVWVg56PLfbLbfbnY6oSXZtXqOuTb+V5T9Vp1w5P+3vBwAABpfyMygHy8mOHTv0pz/9SeXl5Unb6+rq1NHRoY0bNybWrVmzRrFYTNOnT091nCHZvX2TTt21TH2v/d7WHAAAjHZDPoPS1dWlN954I/H7nTt3avPmzSorK9P48eP1mc98Rps2bdLKlSsVjUYT15WUlZXJ5XLplFNO0WWXXaYbbrhBS5cuVTgc1vz583XNNdcc0x086ZTnLpIk5Ud6bM0BAMBoN+SCsmHDBn384x9P/H7hwoWSpDlz5ugHP/iBnnrqKUnSmWeemfS6Z555RpdccokkadmyZZo/f74uvfRSORwOzZo1S0uWLDnOj5A6eQXFkiRnrM/mJAAAjG5DLiiXXHKJjjR1yrFMq1JWVqZHH310qG+dds6BguKO9dqcBACA0Y1n8Rwi31MiSXJRUAAAsBUF5RCugYJSYPiKBwAAO1FQDuEujBcUDwUFAABbUVAO4S6KX4NSaIVkYlGb0wAAMHpRUA5RWPT3CeX6erptTAIAwOhGQTmEZ+ArHknq6eaBhAAA2IWCcghHXp56THxK/VBPp81pAAAYvSgoh+m1CiRJoR7OoAAAYBcKymFCAwWlv5czKAAA2IWCcpg+yyNJ6uciWQAAbENBOUzYET+DEunjDAoAAHahoBymPy9+BiXa12VzEgAARi8KymEiAwUlFqKgAABgFwrKYaLOQklSrJ9rUAAAsAsF5TAHC4pCFBQAAOxCQTmMyY8XFBOmoAAAYBcKyuHyiyRJjnCPzUEAABi9KCiHMS4KCgAAdqOgHMbhjheUvCgFBQAAu1BQDmMNnEFxRnptTgIAwOhFQTmMs6A4/s8oBQUAALtQUA6TN1BQXDEKCgAAdqGgHMZFQQEAwHYUlMPkF5ZIkgpMn81JAAAYvSgoh3F74mdQKCgAANiHgnKYgiKfJMmjkEwsZnMaAABGJwrKYdxF8a948q2oQiHOogAAYAcKymEKB65BkaS+7k4bkwAAMHpRUA7jdLnVb5ySpN7ugM1pAAAYnSgog+ixCiRJod4um5MAADA6UVAG0aeBgtLDVzwAANiBgjKIfke8oIR7KSgAANiBgjKIkMMjSYrwFQ8AALagoAwifLCghCgoAADYgYIyiHBevKBE+7ptTgIAwOhEQRlE1FkoSTKcQQEAwBYUlEFE8ygoAADYiYIyiJirSJJkwnzFAwCAHSgogzADBcXRzxkUAADsQEEZjCv+PB4HZ1AAALAFBWUQjoJiSZKTggIAgC0oKIPIK/BKkvKjFBQAAOxAQRlEnif+FU9+tMfmJAAAjE4UlEHkDxQUd4yCAgCAHSgog3AXlkqSCigoAADYgoIyCHdR/BqUQtNrcxIAAEanIReUdevW6aqrrlJVVZUsy9KKFSuSthtjdPvtt2v8+PHyeDyqr6/Xjh07kvZpb2/X7Nmz5fV6VVpaquuvv15dXdkz54in2CdJKlSfTCxmcxoAAEafIReU7u5unXHGGXrggQcG3X7PPfdoyZIlWrp0qZqbm1VUVKQZM2aor68vsc/s2bP16quvavXq1Vq5cqXWrVunuXPnHv+nSDFPcakkKd+KqrePr3kAAMg0yxhjjvvFlqUnn3xSM2fOlBQ/e1JVVaVvfetb+va3vy1JCgQC8vv9euSRR3TNNddo69atOvXUU7V+/XpNmzZNkrRq1SpdccUVeuedd1RVVXXU9w0Gg/L5fAoEAvJ6vccb/wOZaETWD8slSXu//qrG+Sek/D0AABhthvLzO6XXoOzcuVOtra2qr69PrPP5fJo+fbqampokSU1NTSotLU2UE0mqr6+Xw+FQc3PzoMcNhUIKBoNJSzpZeU71GLckqa8rve8FAADeL6UFpbW1VZLk9/uT1vv9/sS21tZWVVRUJG13Op0qKytL7HO4hoYG+Xy+xFJdXZ3K2IPqtTySpFB3IO3vBQAAkuXEXTyLFi1SIBBILC0tLWl/TwoKAAD2SWlBqayslCS1tbUlrW9ra0tsq6ys1J49e5K2RyIRtbe3J/Y5nNvtltfrTVrSLeQolCSFe/mKBwCATEtpQamtrVVlZaUaGxsT64LBoJqbm1VXVydJqqurU0dHhzZu3JjYZ82aNYrFYpo+fXoq4wxLfx4FBQAAuziH+oKuri698cYbid/v3LlTmzdvVllZmWpqarRgwQLdcccdmjRpkmpra7V48WJVVVUl7vQ55ZRTdNlll+mGG27Q0qVLFQ6HNX/+fF1zzTXHdAdPpoSdhVK/FO3NnvlZAAAYLYZcUDZs2KCPf/zjid8vXLhQkjRnzhw98sgjuuWWW9Td3a25c+eqo6NDF110kVatWqWCgoLEa5YtW6b58+fr0ksvlcPh0KxZs7RkyZIUfJzUiTiLJEmxEGdQAADItGHNg2KXdM+DIknr7/+izt3/tP5a81VdeN09aXkPAABGE9vmQRlJYvnFkiQrxFc8AABkGgXlg7jiBcURpqAAAJBpFJQPYLlLJEmOcLfNSQAAGH0oKB/AURAvKPkRCgoAAJlGQfkAeQcLSpSnGQMAkGkUlA/gLIxfXeyioAAAkHEUlA/gKoyfQXHHKCgAAGQaBeUDuAp9kiSP6bU5CQAAow8F5QMUFMULSiEFBQCAjKOgfADPwYKiPkUiUZvTAAAwulBQPkBhSakkyWEZdXfxPB4AADKJgvIBXJ5iRY0lSerpDticBgCA0YWC8kEsSz1W/AnMfV0UFAAAMomCcgS9KpQk9XEGBQCAjKKgHEGfwyNJCvdQUAAAyCQKyhGEHEWSpHA3F8kCAJBJFJQj6HcOFBTOoAAAkFEUlCMIO4slSdFeCgoAAJlEQTmCiCv+PB6F+IoHAIBMoqAcQcwVf6KxQp32BgEAYJShoByJO34GxdHPGRQAADKJgnIEVkH8DIqznzMoAABkEgXlCPI88QcGOiPdNicBAGB0oaAcgbOwVJLkinTZGwQAgFGGgnIE+YXxr3gKYhQUAAAyiYJyBO7iMZKkwhhf8QAAkEkUlCPwlAwUFNNrcxIAAEYXCsoRFA4UlGL1KByJ2pwGAIDRg4JyBEXeeEHJs4w6O5nuHgCATKGgHIHTXaSwyZMk9QT325wGAIDRg4JyJJalbqtQktQbPGBzGAAARg8KylH0WEWSpN7uDnuDAAAwilBQjqIvL34GJUxBAQAgYygoRxHKK5YkRbq5SBYAgEyhoBxF2BkvKNHeDnuDAAAwilBQjiKSXyJJivXxRGMAADKFgnIUMVf8DIpCQXuDAAAwilBQjsK44w8MdFBQAADIGArK0RTEC0pemK94AADIFArKUTgKSiVJzjBPNAYAIFMoKEfh9MTPoLginEEBACBTKChHkV/kkyS5o5xBAQAgUygoR+Euij/R2BOjoAAAkCkUlKMoKIkXlELTY3MSAABGDwrKURSWlEqSitWjaMzYGwYAgFGCgnIURd4ySZLbiqiru8vmNAAAjA4pLyjRaFSLFy9WbW2tPB6PTjrpJP3whz+UMX8/+2CM0e23367x48fL4/Govr5eO3bsSHWUlHAV+hQzliSpK9BucxoAAEaHlBeUu+++Ww8++KB++tOfauvWrbr77rt1zz336P7770/sc88992jJkiVaunSpmpubVVRUpBkzZqivry/VcYbP4VC35ZEkdQf22xwGAIDRwZnqAz733HO6+uqrdeWVV0qSTjzxRD322GN64YUXJMXPntx33336/ve/r6uvvlqS9Otf/1p+v18rVqzQNddck+pIw9ZlFavE9KgvSEEBACATUn4G5YILLlBjY6O2b98uSXrppZf07LPP6vLLL5ck7dy5U62traqvr0+8xufzafr06Wpqahr0mKFQSMFgMGnJpN68+BONQ118xQMAQCak/AzKbbfdpmAwqMmTJysvL0/RaFR33nmnZs+eLUlqbW2VJPn9/qTX+f3+xLbDNTQ06J//+Z9THfWYhZwlUkQKdx+wLQMAAKNJys+g/OY3v9GyZcv06KOPatOmTfrVr36lf/3Xf9WvfvWr4z7mokWLFAgEEktLS0sKEx9df358uvtYDwUFAIBMSPkZlO985zu67bbbEteSTJ06VW+//bYaGho0Z84cVVZWSpLa2to0fvz4xOva2tp05plnDnpMt9stt9ud6qjHLOqKT3dvejtsywAAwGiS8jMoPT09cjiSD5uXl6dYLCZJqq2tVWVlpRobGxPbg8GgmpubVVdXl+o4KRFzxwuK1ddhbxAAAEaJlJ9Bueqqq3TnnXeqpqZGp512ml588UXde++9uu666yRJlmVpwYIFuuOOOzRp0iTV1tZq8eLFqqqq0syZM1MdJzU88YLi6M/sxbkAAIxWKS8o999/vxYvXqxvfOMb2rNnj6qqqvTVr35Vt99+e2KfW265Rd3d3Zo7d646Ojp00UUXadWqVSooKEh1nJRwFMafx5NPQQEAICMsc+gUrzkiGAzK5/MpEAjI6/Wm/f1eWfVLTX1+oV7OP12nf+8vaX8/AABGoqH8/OZZPMcgvyh+BsUT7bQ5CQAAowMF5RgUDDwwsDDWbXMSAABGBwrKMSj0lkuSSkyXcvAbMQAAcg4F5RgUl46VJHmtHvX09ducBgCAkY+Ccgw8JWWJXwd5ojEAAGlHQTkGltOtXsVnsu2moAAAkHYUlGPUZRVLknqDFBQAANKNgnKMehzxgtLX2W5zEgAARj4KyjHqc8YnlAl3UVAAAEg3Csox6neWSJKiPQdsTgIAwMhHQTlGEVf8DIrp7bA3CAAAowAF5RjF3KWSJKuvw9YcAACMBhSUY+UplSQ5QgF7cwAAMApQUI6R5fFJkvL7gzYnAQBg5KOgHCPnwBON8yM80RgAgHSjoByj/KL4dPceCgoAAGlHQTlGBd54QSmMUVAAAEg3CsoxKvLFn2hcYrpkjLE5DQAAIxsF5RiVjPFLkrzqVldfv81pAAAY2Sgox8jjjZ9BybOMggf22ZwGAICRjYJyrJwudcsjSeo8sMfmMAAAjGwUlCHodMSnu+8J7LU5CQAAIxsFZQh68uIFpT9IQQEAIJ0oKEPQlx+fTTbcyTUoAACkEwVlCMKuUklStLvd3iAAAIxwFJQhiBbEp7u3eikoAACkEwVlKDzx2WTz+g7YHAQAgJGNgjIEjqJySZKrv8PeIAAAjHAUlCHIL44XFHc4YHMSAABGNgrKELgHZpMtjAZtTgIAwMhGQRmCQl+FJKkkRkEBACCdKChDUFwWf2BgqToVikRtTgMAwMhFQRmC4tJxkqQCK6yOANehAACQLhSUIXAUlCgspyQeGAgAQDpRUIbCshS0SiRJ3Qd4Hg8AAOlCQRmi7oEnGod4YCAAAGlDQRmi3oEHBvbzwEAAANKGgjJE/QMFJdK93+YkAACMXBSUIYoMPDBQPTwwEACAdKGgDJEpiD8w0OrlgYEAAKQLBWWIHEXxgpIfoqAAAJAuFJQhciYeGNhhbxAAAEYwCsoQFZRWSpIKIx32BgEAYASjoAxRYWn8eTzeKFPdAwCQLhSUIfKOHS9JKlNAvf08MBAAgHSgoAxR0Zj4GRSP1a/2Di6UBQAgHdJSUN5991198YtfVHl5uTwej6ZOnaoNGzYkthtjdPvtt2v8+PHyeDyqr6/Xjh070hEl5SxXsfrkkiQF9+22OQ0AACNTygvKgQMHdOGFFyo/P19/+MMf9Nprr+nf/u3fNGbMmMQ+99xzj5YsWaKlS5equblZRUVFmjFjhvr6+lIdJ/UsSwFHqSSp50CrvVkAABihnKk+4N13363q6mo9/PDDiXW1tbWJXxtjdN999+n73/++rr76aknSr3/9a/n9fq1YsULXXHNNqiOlXLezVOrfo96ONrujAAAwIqX8DMpTTz2ladOm6bOf/awqKip01lln6Re/+EVi+86dO9Xa2qr6+vrEOp/Pp+nTp6upqWnQY4ZCIQWDwaTFTn2u+GRt4c49tuYAAGCkSnlBeeutt/Tggw9q0qRJ+uMf/6ivf/3r+uY3v6lf/epXkqTW1vjXIn6/P+l1fr8/se1wDQ0N8vl8iaW6ujrVsYck7I4XFHXxRGMAANIh5QUlFovp7LPP1o9+9COdddZZmjt3rm644QYtXbr0uI+5aNEiBQKBxNLS0pLCxENnCsdKkqweCgoAAOmQ8oIyfvx4nXrqqUnrTjnlFO3atUuSVFkZn4m1rS35+o22trbEtsO53W55vd6kxU5W8ThJUn7ffltzAAAwUqW8oFx44YXatm1b0rrt27dr4sSJkuIXzFZWVqqxsTGxPRgMqrm5WXV1damOkxYub4UkqSDMPCgAAKRDyu/iufnmm3XBBRfoRz/6kT73uc/phRde0EMPPaSHHnpIkmRZlhYsWKA77rhDkyZNUm1trRYvXqyqqirNnDkz1XHSwu2LXz9THKGgAACQDikvKOeee66efPJJLVq0SP/yL/+i2tpa3XfffZo9e3Zin1tuuUXd3d2aO3euOjo6dNFFF2nVqlUqKChIdZy0KCqLFxRfLCBjjCzLsjkRAAAji2WMMXaHGKpgMCifz6dAIGDL9Sh9e99WwQOnK2Sc6r/tPZV4XBnPAABArhnKz2+exXMcCkrj16C4rYja27lQFgCAVKOgHI98j3oU/zqqc/97NocBAGDkoaAcp+DA83i6eR4PAAApR0E5Tt3OUklSKMDzeAAASDUKynEKDUx3H+nca3MSAABGHgrKcYoUxKe7VxcPDAQAINUoKMfJDEx37+jhDAoAAKlGQTlOTm/8uUHuPh4YCABAqlFQjpN7TJUkqSjMPCgAAKQaBeU4FZfHC4ov2q4cnIwXAICsRkE5Tr6KaknSOHWoqy9scxoAAEYWCspx8gx8xVNohbSP6e4BAEgpCsrxchWpWx5JUmDvOzaHAQBgZKGgDEMgLz5ZW/f+d21OAgDAyEJBGYZuV7kkKcTzeAAASCkKyjCECiokSbEgTzQGACCVKCjDECuKzyZrdTPdPQAAqURBGQarJD6brKuXggIAQCpRUIbBVTpwq3E/090DAJBKFJRhKCyLF5SSSLvNSQAAGFkoKMPgHTdBklRuDqg/ErM5DQAAIwcFZRi8Y0+QJJVbndoX6LI5DQAAIwcFZRiswnJFlCdJOrCXydoAAEgVCspwOBzqcJRKkjqZ7h4AgJShoAxTV/5YSVJvO2dQAABIFQrKMPUNzCYb7thtcxIAAEYOCsowRYrHS5KsIAUFAIBUoaAMk8MXv5PH1cMDAwEASBUKyjC5yqolScX9bTYnAQBg5KCgDFPJuBpJ0pjIPhljbE4DAMDIQEEZptLKiZIkv/Yr0NNvcxoAAEYGCsowucfEp7svskJq28tTjQEASAUKynC5ChW0SiRJHW27bA4DAMDIQEFJgYBznCSpZy8FBQCAVKCgpEBPgV+SFD7QYnMSAABGBgpKCoSLKuO/YLI2AABSgoKSAtbAZG35TNYGAEBKUFBSwHXwTp4+JmsDACAVKCgpUDQwWVtpZK/NSQAAGBkoKClw6GRtnX1hm9MAAJD7KCgpUFgeP4Pis3r03t79NqcBACD3UVBSocCrHnkkSe2737Q5DAAAuY+CkiLtrvitxl1tf7M3CAAAIwAFJUV6PFWSpHD72zYnAQAg91FQUiRSUi1JcgSY7h4AgOGioKRIXln8Th5P97s2JwEAIPelvaDcddddsixLCxYsSKzr6+vTvHnzVF5eruLiYs2aNUttbbk9yVlhxYckSaX9zCYLAMBwpbWgrF+/Xj//+c91+umnJ62/+eab9fTTT+vxxx/X2rVrtXv3bn36059OZ5S0G1MVLyh+s0c9/RGb0wAAkNvSVlC6uro0e/Zs/eIXv9CYMWMS6wOBgH75y1/q3nvv1Sc+8Qmdc845evjhh/Xcc8/p+eefT1ectCv2nyRJ8lsd2r33gM1pAADIbWkrKPPmzdOVV16p+vr6pPUbN25UOBxOWj958mTV1NSoqalp0GOFQiEFg8GkJesUlqlXBZKkfe++ZXMYAAByW1oKyvLly7Vp0yY1NDS8b1tra6tcLpdKS0uT1vv9frW2Dn79RkNDg3w+X2Kprq5OR+zhsSwdyPdLkrraKCgAAAxHygtKS0uLbrrpJi1btkwFBQUpOeaiRYsUCAQSS0tLS0qOm2pdA3Oh9O//m71BAADIcSkvKBs3btSePXt09tlny+l0yul0au3atVqyZImcTqf8fr/6+/vV0dGR9Lq2tjZVVlYOeky32y2v15u0ZKPEXCjB7CxQAADkCmeqD3jppZfqlVdeSVp37bXXavLkybr11ltVXV2t/Px8NTY2atasWZKkbdu2adeuXaqrq0t1nIxylE2U3mUuFAAAhivlBaWkpERTpkxJWldUVKTy8vLE+uuvv14LFy5UWVmZvF6vbrzxRtXV1en8889PdZyMKhx3oiTJF2IuFAAAhiPlBeVY/OQnP5HD4dCsWbMUCoU0Y8YM/exnP7MjSkqVTZgkSao0bersC6ukIN/mRAAA5CbLGGPsDjFUwWBQPp9PgUAgu65H6d4v/Tg+Ydur127XaRP9NgcCACB7DOXnN8/iSaXCMnVbRZKkfS3bbA4DAEDuoqCkkmWp3T1BktT93nabwwAAkLsoKCnWU1wjSYrtf9PmJAAA5C4KSqqVxa9BcQXftjkIAAC5i4KSYh5//E4eXy+TtQEAcLwoKCk2pvpkSVJV9D319EdsTgMAQG6ioKRYyfiBgmLt0669HfaGAQAgR1FQUq24Qr0qUJ5ltGfXDrvTAACQkygoqWZZ2u8+QZLUxa3GAAAcFwpKGvQUT5QkRfdxqzEAAMeDgpIGZkytJMkZ2GlzEgAAchMFJQ0KKwduNe7hVmMAAI4HBSUNyk+cIkmqjr6jQG/Y5jQAAOQeCkoaFI4/VZJ0grVPb+7ea3MaAAByDwUlHYrKFXT45LCM9u3cYncaAAByDgUlTQ4UnihJ6tn9qr1BAADIQRSUNOkvjV8o69jPZG0AAAwVBSVN8ivjU957u96yOQkAALmHgpImYyZOlSRVhXepLxy1OQ0AALmFgpIm3gnxO3lOtFr1ZluHvWEAAMgxFJQ0sXzV6pNbLiuq1p1b7Y4DAEBOoaCki8OhfQXxZ/J0vcOdPAAADAUFJY36Sj8sSYrted3mJAAA5BYKShrlV8avQykOcqsxAABDQUFJo7IPnSVJqgnvVGcfz+QBAOBYUVDSqGRivKCcZO3W9nf22ZwGAIDcQUFJJ2+VuhwlcloxvffWS3anAQAgZ1BQ0smytL84PuV9XwsFBQCAY0VBSbPouNMkSa59r9mcBACA3EFBSbOi6jMkSRU9bygWMzanAQAgN1BQ0qz8pLMlSR/R37Rrf7fNaQAAyA0UlDRzVp6qqBwqs7r01s437I4DAEBOoKCkW75H+93VkqT9b26yOQwAALmBgpIBPWXxGWXNe5vtDQIAQI6goGSAe+K5kqSxwVdlDBfKAgBwNBSUDBh38nRJ0qnmTf1tf4/NaQAAyH4UlAxwnnCWonKo0jqg7Tu22x0HAICsR0HJBFeR9hWcKEk68EazvVkAAMgBFJQM6R0Xn7DN2brZ3iAAAOQACkqGFNbGL5T1d72mSDRmcxoAALIbBSVDxn6kTpI0RW9qe2unzWkAAMhuFJQMcVROUVj5GmN1advrL9sdBwCArEZByRSnS/tKTpEkde34q81hAADIbhSUDIrVnC9JKtmzgQnbAAA4AgpKBo099RJJ0mmR1/RuR6+9YQAAyGIUlAxy18YvlJ3keFcvb3vL5jQAAGSvlBeUhoYGnXvuuSopKVFFRYVmzpypbdu2Je3T19enefPmqby8XMXFxZo1a5ba2tpSHSX7FJZpb0GtJGn/63+xOQwAANkr5QVl7dq1mjdvnp5//nmtXr1a4XBYn/zkJ9Xd3Z3Y5+abb9bTTz+txx9/XGvXrtXu3bv16U9/OtVRslJf1XmSpILdzCgLAMAHsUyar9bcu3evKioqtHbtWl188cUKBAIaN26cHn30UX3mM5+RJL3++us65ZRT1NTUpPPPP/+oxwwGg/L5fAoEAvJ6vemMn3Kdzf+pkj/M16bYh1Vzy3MaW+y2OxIAABkxlJ/fab8GJRAISJLKysokSRs3blQ4HFZ9fX1in8mTJ6umpkZNTU2DHiMUCikYDCYtuarkIx+VJE21dqp52y6b0wAAkJ3SWlBisZgWLFigCy+8UFOmTJEktba2yuVyqbS0NGlfv9+v1tbWQY/T0NAgn8+XWKqrq9MZO73GnKgD7irlW1G1vbzG7jQAAGSltBaUefPmacuWLVq+fPmwjrNo0SIFAoHE0tLSkqKE9uitvliSVPTOX5gPBQCAQaStoMyfP18rV67UM888owkTJiTWV1ZWqr+/Xx0dHUn7t7W1qbKyctBjud1ueb3epCWXlU/9pCTpjPBm/W1/j81pAADIPikvKMYYzZ8/X08++aTWrFmj2trapO3nnHOO8vPz1djYmFi3bds27dq1S3V1damOk5XcH75EMVma7GjRhldftzsOAABZx5nqA86bN0+PPvqofve736mkpCRxXYnP55PH45HP59P111+vhQsXqqysTF6vVzfeeKPq6uqO6Q6eEaGoXPuKT1ZF1+vqfPV/pY+dY3ciAACySsoLyoMPPihJuuSSS5LWP/zww/rKV74iSfrJT34ih8OhWbNmKRQKacaMGfrZz36W6ijZ7UMfl15+XeVtzykUicrtzLM7EQAAWSPt86CkQy7Pg3JQ7M0/y/GfV2uv8Wrr/7tBF5/stzsSAABplVXzoGBwjokXqM9RpHFWUK9vfMbuOAAAZBUKil2cLgVO+JgkqeDN/+V2YwAADkFBsdGYs2dKks4Lv6Ct73XaGwYAgCxCQbGRa/InFZVDkx0temHTRrvjAACQNSgodvKM0f7y+C3GfVtW2hwGAIDsQUGxWckZV0uSzulZp22tfM0DAIBEQbGd58xZisnSuY7tWrt+k91xAADIChQUu3mr1F4+TZIUffl/uJsHAABRULJCybnXSJIuCq3VK+8GbE4DAID9KChZwD31HxRVnqY6/qY///U5u+MAAGA7Cko2KCpXR9VHJUmerb9RXzhqcyAAAOxFQckSYy64VpL0KfOM/vjyOzanAQDAXhSULOGYfIV68sfIb3Vo+1//x+44AADYioKSLZwuxU6PXyx75t6n9ebeLpsDAQBgHwpKFimuu16S9AnHi3rimedtTgMAgH0oKNlk7CQF/OcrzzIq3fKIDnT3250IAABbUFCyjPfjN0mSPm816vHnttqcBgAAe1BQsoz1kcvUVTRRXqtHwaZHuOUYADAqUVCyjcOhgotvlCR9PvK0lj//ps2BAADIPApKFnKeNVt9rjGqduzVrj8/zFkUAMCoQ0HJRq5COS9eKEm6LvIbPfrcGzYHAgAgsygoWcp53j+q1z1WE6x9eu/P/1eB3rDdkQAAyBgKSrZyFcp1ybclSf8Y+42W/u9me/MAAJBBFJQslnfudeotrpHf6pB3w/3MLgsAGDUoKNnM6Zbn/9wtSbrO8f/p/t+uljHG5lAAAKQfBSXbnXy5eqsvltuKaObun+i/X9hldyIAANKOgpLtLEueq+9VxOHSJXkvacvvl+q9QK/dqQAASCsKSi4YO0mOj39XknSLHtEPlv1JkWjM5lAAAKQPBSVHOC64UaGKM+W1enRt6526v/F1uyMBAJA2FJRckeeU+3O/VMRZqPMdW5W37h796bU2u1MBAJAWFJRcMvbDcl59vyRpft4K/W75z7Xl3YDNoQAASD0KSq6Z+hlFp/2jHJbRj60luvfhZdrdwUWzAICRhYKSg/Iuv1vhk/4fFVhh3RNu0Ld//gQlBQAwolBQclGeU/mfe0T946ZqrBXUvT3f03eW/o/epaQAAEYICkquchfLNecJhctPVqV1QPf1flff+uljXJMCABgRKCi5rLhC+df9XuFxp2mcFdBD4e/q35c+qFVb3rM7GQAAw0JByXVFY5V/7UpFJpwvr9WrpY67tOGxH+oHK15WXzhqdzoAAI4LBWUkKCyT8ytPK3bml5RnGX0/f5k+sfEb+sqS32nj2wfsTgcAwJBRUEYKp0uOq++Xrvw3RfPcujjvFS0Nztf/PPRDLfrti9rbGbI7IQAAx8wyxhi7QwxVMBiUz+dTIBCQ1+u1O0722btNkd/+o5xtL0uSXo7V6idmtj5Sd6W++rEPq6zIZXNAAMBoNJSf3xSUkSoakdb/X0Ua75Az3ClJWh/7iH5uZqnirCv05QtO1ORKxg4AkDkUFPxd1x6Zdf+q2IaHlRfrlyS9GRuvR6Of0Bvj/48+duYpuvL08fJ7C2wOCgAY6SgoeL/gezJ/vU+xjf+pvEi3JCliHHo+dor+EJuu1vGXasrJH9FHJ43VGdWlys/j8iQAQGpRUPDBQp3SK79VeP3Dym97KWnTG7EqNcVO1SbHFEXHn63xNZM0tbpUp59QqgljPHI4LJtCAwBGAgoKjk37TmnrU+p/+Unlt22WpeQ/CgdMsV6LTdRrZqJ2OU5Qf8lE5ZV/SKXjT1RNeYn8vgL5Swrk97o1ptBFgQEAHBEFBUPX0y69/ZzMznXqe/NZudu3yWEig+4aMk7tNuXaq1LtNT7tNaVqt0oVcpcr5i6V5S6Rw+NTXqFXrsJSuYvHyO0pUoHLKU9+ngryHSrIz1NBft7A7+PrnHkOOR3WwOJQXl7813kD6yyLAgQAuSxnCsoDDzygH//4x2ptbdUZZ5yh+++/X+edd95RX0dByYBISNqzVWp9WbH3XlFf2w7pwE65u95R3gcUlyOJGkshuRRSvvrlVL/JH/h1fmJdyOQrJoeicigm632/jskhWfF/GitPxoqvlwaKy0CBObTGHNpprEP/acXPF1kDKywNv/wcfgZKUtKaI73DYK/9oO2H72kd4T/hox938GMONdNQDSfzEY87rMw59//VgLSKnnixrvjyLSk95lB+fjtT+s5D8N///d9auHChli5dqunTp+u+++7TjBkztG3bNlVUVNgVCwc53VLVmVLVmXJIKjy4PhaVgu9KHS1S9x6pa4+iwVb1dbQqEmyV6Q3I6u9UXn+nnJEuuSLdciimPMuoUCEVamDCuHScDBns5ws/cwDguDQHymx9f9vOoEyfPl3nnnuufvrTn0qSYrGYqqurdeONN+q222474ms5g5JDjJH6u+MX50ZD8TMzB5doSIr0SZH++D+j/VIsKmOiikWjisViikYjMtGoYrFDlmhUJhqRMTHJRCWT3EPMwP+Ygd8d+ifcHPI/B//om0M3HrU4HW2noxzgCF9TmUFem7z78b2vsYZzfuiwV74v/xGOPJyv5I762uG8b5oyAyNM/vgpGnfGjJQeM+vPoPT392vjxo1atGhRYp3D4VB9fb2ampret38oFFIo9Pep2oPBYEZyIgUsS3IXx5djfYmkvIElP125AABZzZbJLvbt26doNCq/35+03u/3q7W19X37NzQ0yOfzJZbq6upMRQUAADbIidm4Fi1apEAgkFhaWlrsjgQAANLIlq94xo4dq7y8PLW1tSWtb2trU2Vl5fv2d7vdcrvdmYoHAABsZssZFJfLpXPOOUeNjY2JdbFYTI2Njaqrq7MjEgAAyCK23Wa8cOFCzZkzR9OmTdN5552n++67T93d3br22mvtigQAALKEbQXl85//vPbu3avbb79dra2tOvPMM7Vq1ar3XTgLAABGH6a6BwAAGTGUn985cRcPAAAYXSgoAAAg61BQAABA1qGgAACArENBAQAAWYeCAgAAso5t86AMx8E7o3mqMQAAuePgz+1jmeEkJwtKZ2enJPFUYwAAclBnZ6d8Pt8R98nJidpisZh2796tkpISWZaV0mMHg0FVV1erpaWFSeDSiHHODMY5MxjnzGGsMyNd42yMUWdnp6qqquRwHPkqk5w8g+JwODRhwoS0vofX6+UPfwYwzpnBOGcG45w5jHVmpGOcj3bm5CAukgUAAFmHggIAALIOBeUwbrdb//RP/yS32213lBGNcc4MxjkzGOfMYawzIxvGOScvkgUAACMbZ1AAAEDWoaAAAICsQ0EBAABZh4ICAACyDgXlEA888IBOPPFEFRQUaPr06XrhhRfsjpRTGhoadO6556qkpEQVFRWaOXOmtm3blrRPX1+f5s2bp/LychUXF2vWrFlqa2tL2mfXrl268sorVVhYqIqKCn3nO99RJBLJ5EfJKXfddZcsy9KCBQsS6xjn1Hj33Xf1xS9+UeXl5fJ4PJo6dao2bNiQ2G6M0e23367x48fL4/Govr5eO3bsSDpGe3u7Zs+eLa/Xq9LSUl1//fXq6urK9EfJatFoVIsXL1Ztba08Ho9OOukk/fCHP0x6XgtjPXTr1q3TVVddpaqqKlmWpRUrViRtT9WYvvzyy/roRz+qgoICVVdX65577knNBzAwxhizfPly43K5zH/8x3+YV1991dxwww2mtLTUtLW12R0tZ8yYMcM8/PDDZsuWLWbz5s3miiuuMDU1Naarqyuxz9e+9jVTXV1tGhsbzYYNG8z5559vLrjggsT2SCRipkyZYurr682LL75ofv/735uxY8eaRYsW2fGRst4LL7xgTjzxRHP66aebm266KbGecR6+9vZ2M3HiRPOVr3zFNDc3m7feesv88Y9/NG+88UZin7vuusv4fD6zYsUK89JLL5lPfepTpra21vT29ib2ueyyy8wZZ5xhnn/+efOXv/zFfPjDHzZf+MIX7PhIWevOO+805eXlZuXKlWbnzp3m8ccfN8XFxebf//3fE/sw1kP3+9//3nzve98zTzzxhJFknnzyyaTtqRjTQCBg/H6/mT17ttmyZYt57LHHjMfjMT//+c+HnZ+CMuC8884z8+bNS/w+Go2aqqoq09DQYGOq3LZnzx4jyaxdu9YYY0xHR4fJz883jz/+eGKfrVu3GkmmqanJGBP/D8rhcJjW1tbEPg8++KDxer0mFApl9gNkuc7OTjNp0iSzevVq87GPfSxRUBjn1Lj11lvNRRdd9IHbY7GYqaysND/+8Y8T6zo6Oozb7TaPPfaYMcaY1157zUgy69evT+zzhz/8wViWZd599930hc8xV155pbnuuuuS1n360582s2fPNsYw1qlweEFJ1Zj+7Gc/M2PGjEn6e+PWW281J5988rAz8xWPpP7+fm3cuFH19fWJdQ6HQ/X19WpqarIxWW4LBAKSpLKyMknSxo0bFQ6Hk8Z58uTJqqmpSYxzU1OTpk6dKr/fn9hnxowZCgaDevXVVzOYPvvNmzdPV155ZdJ4Soxzqjz11FOaNm2aPvvZz6qiokJnnXWWfvGLXyS279y5U62trUnj7PP5NH369KRxLi0t1bRp0xL71NfXy+FwqLm5OXMfJstdcMEFamxs1Pbt2yVJL730kp599lldfvnlkhjrdEjVmDY1Neniiy+Wy+VK7DNjxgxt27ZNBw4cGFbGnHxYYKrt27dP0Wg06S9rSfL7/Xr99ddtSpXbYrGYFixYoAsvvFBTpkyRJLW2tsrlcqm0tDRpX7/fr9bW1sQ+g/17OLgNccuXL9emTZu0fv36921jnFPjrbfe0oMPPqiFCxfqu9/9rtavX69vfvObcrlcmjNnTmKcBhvHQ8e5oqIiabvT6VRZWRnjfIjbbrtNwWBQkydPVl5enqLRqO68807Nnj1bkhjrNEjVmLa2tqq2tvZ9xzi4bcyYMcedkYKCtJg3b562bNmiZ5991u4oI05LS4tuuukmrV69WgUFBXbHGbFisZimTZumH/3oR5Kks846S1u2bNHSpUs1Z84cm9ONLL/5zW+0bNkyPfroozrttNO0efNmLViwQFVVVYz1KMZXPJLGjh2rvLy8993l0NbWpsrKSptS5a758+dr5cqVeuaZZzRhwoTE+srKSvX396ujoyNp/0PHubKyctB/Dwe3If4Vzp49e3T22WfL6XTK6XRq7dq1WrJkiZxOp/x+P+OcAuPHj9epp56atO6UU07Rrl27JP19nI7090ZlZaX27NmTtD0Siai9vZ1xPsR3vvMd3Xbbbbrmmms0depUfelLX9LNN9+shoYGSYx1OqRqTNP5dwkFRZLL5dI555yjxsbGxLpYLKbGxkbV1dXZmCy3GGM0f/58Pfnkk1qzZs37Tvudc845ys/PTxrnbdu2adeuXYlxrqur0yuvvJL0H8Xq1avl9Xrf98NitLr00kv1yiuvaPPmzYll2rRpmj17duLXjPPwXXjhhe+7TX779u2aOHGiJKm2tlaVlZVJ4xwMBtXc3Jw0zh0dHdq4cWNinzVr1igWi2n69OkZ+BS5oaenRw5H8o+jvLw8xWIxSYx1OqRqTOvq6rRu3TqFw+HEPqtXr9bJJ588rK93JHGb8UHLly83brfbPPLII+a1114zc+fONaWlpUl3OeDIvv71rxufz2f+/Oc/m/feey+x9PT0JPb52te+ZmpqasyaNWvMhg0bTF1dnamrq0tsP3j76yc/+UmzefNms2rVKjNu3Dhufz2KQ+/iMYZxToUXXnjBOJ1Oc+edd5odO3aYZcuWmcLCQvNf//VfiX3uuusuU1paan73u9+Zl19+2Vx99dWD3qZ51llnmebmZvPss8+aSZMmjepbXwczZ84cc8IJJyRuM37iiSfM2LFjzS233JLYh7Eeus7OTvPiiy+aF1980Ugy9957r3nxxRfN22+/bYxJzZh2dHQYv99vvvSlL5ktW7aY5cuXm8LCQm4zTrX777/f1NTUGJfLZc477zzz/PPP2x0pp0gadHn44YcT+/T29ppvfOMbZsyYMaawsND8wz/8g3nvvfeSjvO3v/3NXH755cbj8ZixY8eab33rWyYcDmf40+SWwwsK45waTz/9tJkyZYpxu91m8uTJ5qGHHkraHovFzOLFi43f7zdut9tceumlZtu2bUn77N+/33zhC18wxcXFxuv1mmuvvdZ0dnZm8mNkvWAwaG666SZTU1NjCgoKzIc+9CHzve99L+nWVcZ66J555plB/06eM2eOMSZ1Y/rSSy+Ziy66yLjdbnPCCSeYu+66KyX5LWMOmaoPAAAgC3ANCgAAyDoUFAAAkHUoKAAAIOtQUAAAQNahoAAAgKxDQQEAAFmHggIAALIOBQUAAGQdCgoAAMg6FBQAAJB1KCgAACDrUFAAAEDW+f8BvQZfrLmKgR8AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(slr.loss)\n",
    "plt.plot(slr.val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8de6dcb8-eb49-4a04-a064-cf95600697b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(y_pred, y):\n",
    "    \"\"\"\n",
    "    平均二乗誤差の計算\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_pred : 次の形のndarray, shape (n_samples,)\n",
    "      推定した値\n",
    "    y : 次の形のndarray, shape (n_samples,)\n",
    "      正解値\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    mse : numpy.float\n",
    "      平均二乗誤差\n",
    "    \"\"\"\n",
    "    mse = np.mean((y-y_pred)**2)\n",
    "    \n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbc68293-b312-41ca-8b28-dcc9c6264edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = slr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0cf06cb-a7e1-4c36-a3d3-4c1a9419d6b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154447559296.56726"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE(np.exp(pred_test), np.exp(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c31e2b-e636-4093-b85a-83864da3609c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
